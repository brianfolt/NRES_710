<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 710" />


<title>Basic Concepts of Prob. and Stats.</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 710</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 710</a>
    </li>
    <li>
      <a href="LECTURE1.html">Basic Concepts</a>
    </li>
    <li>
      <a href="LECTURE2.html">Analysis- t test</a>
    </li>
    <li>
      <a href="LECTURE3.html">Analysis- chi2 test</a>
    </li>
    <li>
      <a href="LECTURE4.html">Analysis- linear regression</a>
    </li>
    <li>
      <a href="LECTURE5.html">Analysis- ANOVA</a>
    </li>
    <li>
      <a href="LECTURE6.html">Analysis- Multivariate</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    More Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Links.html">Links</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final Projects</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Basic Concepts of Prob. and Stats.</h1>
<h4 class="author">NRES 710</h4>
<h4 class="date">Fall 2020</h4>

</div>


<div id="download-the-r-code-for-this-lecture" class="section level2">
<h2>Download the R code for this lecture!</h2>
<p>To follow along with the R-based lessons and demos, <a href="LECTURE1.R">right (or command) click on this link and save the script to your working directory</a></p>
</div>
<div id="overview-of-basic-concepts-of-probability-and-statistics" class="section level2">
<h2>Overview of basic concepts of probability and statistics</h2>
<p>Now that we’ve explored data types, we are going to talk about making sense of data! Specifically, through the lens of (frequentist) probability and (classical) statistical tests!</p>
<p>Probability can be tricky. Humans are not very intuitive about probability!</p>
<p>Yet it’s not just laypeople who are confused by concepts of probability and uncertainty. Statisticians themselves argue about probabilities. For example, the <a href="hothand.pdf">hot hand in basketball</a>.</p>
<p>Why does this matter to us? Because we have to use probability to make inferences from our data (which is what statistics is all about)!</p>
</div>
<div id="classical-statistics-part-1" class="section level2">
<h2>Classical Statistics Part 1</h2>
<p>For example, say I asked you: are people with last names beginning A-M taller than those with last names beginning N-Z?<br />
As a scientist, what would we do?</p>
<p>We would want to identify, and then sample, the population of interest!</p>
<p><img src="statistics1.png" /> Unseen population of interest:<br />
<strong>Q</strong>: what is the population in the above example?<br />
<strong>Q</strong> what is the parameter in the above example?</p>
<p>Observed sample from which we can make inference:<br />
<strong>Q</strong> what is the sample in the above example?<br />
<strong>Q</strong> what is the ‘statistic’ in the above example?</p>
<p><em>Population</em>: ???<br />
<em>Parameter</em>: ???<br />
<em>Sample</em>: ???<br />
<em>Statistic</em>: ???</p>
<p>When we go from the population to the sample, we implicitly make several assumptions about the sample (i.e., that the sample is representative of the population).</p>
<p><em>Statistics</em> is about making <em>inference</em> about a population-level property (the <em>parameter</em>) from a sample.</p>
<p>Classical statistics involves computing a <em>summary statistic</em> from the data, and then using a (usually theoretical) ‘sampling distribution’ to determine whether or not that summary statistic contains a meaningful signal or is likely to be a result of random noise.</p>
</div>
<div id="frequentist-probability-part-1" class="section level2">
<h2>Frequentist Probability Part 1</h2>
<p>Where does probability come in? Well, samples are random- and by random we mean that every time we sample from a population can result in a different outcome. Therefore, all <em>statistics</em> derived from samples are random quantities, and will differ with each sample. When something is random, we use probability to quantify how commonplace or unexpected an outcome is. Probability helps us to quantify things that we can’t know with certainty.</p>
<p>If all samples (and the statistics derived from them) differ from one another, how can we be sure that our statistic (derived from a random sample) is actually telling us something about the population of interest (i.e., a true signal)? How can we be convinced that our result is meaningful and not simply an <em>artifact of random sampling</em>?</p>
<div id="aside-probability-measures-lack-of-surprise" class="section level3">
<h3>Aside: probability measures lack of surprise!</h3>
<p><strong>Q</strong>: what is the probability of getting four heads out of 4 coin flips?</p>
<p>We assume that heads or tails are equally likely outcomes of a single coin flip and that the four flips are completely independent.</p>
<p>How many possible outcomes are there?</p>
<p>TTTT<br />
TTTH<br />
TTHT<br />
TTHH<br />
THTT<br />
THTH<br />
THHT<br />
THHH<br />
HTTT<br />
HTTH<br />
HTHT<br />
HTHH<br />
HHTT<br />
HHTH<br />
HHHT<br />
HHHH</p>
<p>Are each of these outcomes/sequences equally likely?</p>
<p>How many of the possible outcomes meet our criterion (all heads)? (How many ways are there of getting 4 heads?)</p>
<p>What is the probability of getting all heads?</p>
<p>If you get four heads in a row, are you <em>surprised</em> enough by that outcome to reject the assumption that the coin is ‘fair’? What about if you got 100 heads in a row?</p>
<p>Probability of a specific outcome = (# ways of getting a specific outcome)/(total number of equally plausible outcomes)</p>
<p>Probability measures your lack of surprise at observing a particular outcome from a sampling process. If there is only one way of getting a particular outcome out of hundreds of different, equally possible sampled outcomes, you might be pretty surprised to see that one specific outcome!</p>
<p><strong>Q</strong> What is the probability of getting at least three of the same face (heads or tails) out of 4 coin flips?</p>
<p>How surprised would you be to flip 4 coins and observe at least three flips with the same face??</p>
</div>
</div>
<div id="classical-statistics-part-2-the-null" class="section level2">
<h2>Classical Statistics Part 2 (the null!)</h2>
<p>The null hypothesis is simply the notion that the effect you are testing for is entirely absent in the population of interest.</p>
<p>The null universe is the universe where the null hypothesis is true. It is this universe that we must dwell in when we practice statistics, at least in classical/frequentist statistics! Therefore in this class we will tend to dwell in the null universe quite a lot!</p>
<p><strong>Q</strong> what is the null hypothesis for the height example above?</p>
<p>The <em>p value</em> is a key concept of frequentist statistics, but one that is often misinterpreted and misunderstood.</p>
<p>The p value is the probability of obtaining a result at least as extreme as your observed result, <em>given you are dwelling in the null universe</em> (the effect you are testing for is entirely absent in the population of interest). The p value answers the question: “what’s the probability that my result (or one even more extreme) could have happened by chance alone, given the null hypothesis is true?”</p>
<p>Another way of stating the p-value:</p>
<p>“Given the null hypothesis is true, how <em>unsurprised</em> would you be to observe a <em>signal</em> at least as strong as the one you observed?”. The lower the p-value, the more surprised you will be that your result could simply be a result of random chance in the null universe AND the more likely you might be to reject the null hypothesis (we just can’t be living in the null universe- the result is too unlikely!)</p>
<div id="aside-extremeness" class="section level3">
<h3>Aside: extremeness</h3>
<p>When we use the phrase ‘at least as extreme as your observed result’ we mean a result that would lend even more support for rejecting the null hypothesis than your observed result (a result with even more <em>signal</em> than the observed data). For example, let’s imagine the following scenario for the height/last name question:</p>
<p>Sample size: 20, 10 with names between A and M and 10 with names between N and Z.<br />
Observed summary statistic: mean height for A-M is 2 cm greater than mean height for N-Z</p>
<p><strong>Q</strong> What are some results that would lend at least as much support for supporting the notion that the null hypothesis is false??</p>
<p><strong>Q</strong> Would you be comfortable rejecting the null hypothesis if random sampling in the null universe yielded mean height differences of 2 cm or greater 20% of the time (p = 0.2)?</p>
</div>
<div id="rejecting-the-null" class="section level3">
<h3>Rejecting the null</h3>
<p>We reject the null hypothesis if the chance of the result occurring by random chance is sufficiently small. For historical reasons, 0.05 is used as the threshold (known as alpha)- if a p value is less than 0.05, we reject the null!</p>
<p>Fisher proposed alpha=0.05 as a nice balance between the probability of mistakenly rejecting a true null hypothesis (Type 1 error), and the probability of failing to reject a false null hypothesis (a Type 2 error). This cutoff value was NEVER intended to be a fixed value to be applied unthinkingly!!! In addition, it made computation easier in the days of the slide rule!</p>
<p>Increasingly, scientists are rejecting the threshold of 0.05 and many are rejecting p-values in general.</p>
</div>
<div id="example-fishers-cups-of-tea" class="section level3">
<h3>Example: Fisher’s cups of tea:</h3>
<p>A Woman claimed she could tell if milk was added to a cup of tea first or last.</p>
<p>Fisher suggested we give her 8 cups of tea at once, 4 with milk first and 4 with tea first. The woman is asked to identify the four cups that were poured with milk first.</p>
<p>There are 70 possible ways of choosing four cups out of 8. The probability of getting all of them right by random chance (in the null universe) would be 1/70 = 0.014 = 1.4%. That would be kinda surprising, right (in the null universe that is)? So if the lady selected all four cups correctly, are we surprised enough to reject the null?</p>
<p><strong>Q</strong>: How would you state the null hypothesis in words?</p>
<p><strong>Q</strong>: If we decide to reject the null, what do we now believe about our tea-taster?</p>
<p><strong>Q</strong>: If we fail to reject the null, what do we now believe about our tea-taster? More specifically, do we now know she is a fraud?</p>
<p>Here are the possibilities (X=correct id, O=false id), which in the null universe are equally likely (since the lady can’t actually tell the difference!). Let’s assume cups 5-8 are milk first and 1-4 are milk last. The number of correct choices is in parentheses…</p>
<p>O1/O2/O3/O4 (0) O1/O3/O4/X5 (1) O1/O4/X7/X8 (2) O2/O4/X5/X6 (2) O3/O4/X7/X8 (2)<br />
O1/O2/O3/X5 (1) O1/O3/O4/X6 (1) O1/X5/X6/X7 (3) O2/O4/X5/X7 (2) O3/X5/X6/X7 (3)<br />
O1/O2/O3/X6 (1) O1/O3/O4/X7 (1) O1/X5/X6/X8 (3) O2/O4/X5/X8 (2) O3/X5/X6/X8 (3)<br />
O1/O2/O3/X7 (1) O1/O3/O4/X8 (1) O1/X5/X7/X8 (3) O2/O4/X6/X7 (2) O3/X5/X7/X8 (3)<br />
O1/O2/O3/X8 (1) O1/O3/X5/X6 (2) O1/X6/X7/X8 (3) O2/O4/X6/X8 (2) O3/X6/X7/X8 (3)<br />
O1/O2/O4/X5 (1) O1/O3/X5/X7 (2) O2/O3/O4/X5 (1) O2/O4/X7/X8 (2) O4/X5/X6/X7 (3)<br />
O1/O2/O4/X6 (1) O1/O3/X5/X8 (2) O2/O3/O4/X6 (1) O2/X5/X6/X7 (3) O4/X5/X6/X8 (3)<br />
O1/O2/O4/X7 (1) O1/O3/X6/X7 (2) O2/O3/O4/X7 (1) O2/X5/X6/X8 (3) O4/X5/X7/X8 (3)<br />
O1/O2/O4/X8 (1) O1/O3/X6/X8 (2) O2/O3/O4/X8 (1) O2/X5/X7/X8 (3) O4/X6/X7/X8 (3)<br />
O1/O2/X5/X6 (2) O1/O3/X7/X8 (2) O2/O3/X5/X6 (2) O2/X6/X7/X8 (3) <em>X5/X6/X7/X8 (4)</em><br />
O1/O2/X5/X7 (2) O1/O4/X5/X6 (2) O2/O3/X5/X7 (2) O3/O4/X5/X6 (2)<br />
O1/O2/X5/X8 (2) O1/O4/X5/X7 (2) O2/O3/X5/X8 (2) O3/O4/X5/X7 (2)<br />
O1/O2/X6/X7 (2) O1/O4/X5/X8 (2) O2/O3/X6/X7 (2) O3/O4/X5/X8 (2)<br />
O1/O2/X6/X8 (2) O1/O4/X6/X7 (2) O2/O3/X6/X8 (2) O3/O4/X6/X7 (2)<br />
O1/O2/X7/X8 (2) O1/O4/X6/X8 (2) O2/O3/X7/X8 (2) O3/O4/X6/X8 (2)</p>
<p><strong>Q</strong> what if the lady selected 3 of 4 correctly? [there are 17 ways of getting three of the four correct out of 70 total permutations]. Would you be surprised enough to reject the null hypothesis. Is it possible that you (and the lady) are living in the null universe? How surprised would you have to be to reject the null universe and admit that she can tell the difference??</p>
<p>ASIDE: the test above is called ‘Fisher’s exact test’ because it literally enumerates all the possible outcomes to compute the exact probability of observing any particular outcome under the null hypothesis. Most tests we will use in this class are (necessarily) approximations that rely on certain assumptions that may or not be met in practice. Fisher’s exact test assumes that all observations are independent, but otherwise makes no additional assumptions.</p>
</div>
<div id="statistics-terminology-type-1-and-type-2-errors" class="section level3">
<h3>Statistics Terminology: Type 1 and Type 2 errors</h3>
<p>A type 1 error is mistakenly rejecting the null hypothesis (probability of false positive).</p>
<p>A <em>p value</em> is the probability that you collect a sample in the null universe with at least as much evidence against the null (at least as much signal) than your observed data. The p-value is not a test in itself- it is only a formal test when paired with a threshold of surprise below which you are sufficiently convinced that the null hypothesis can be rejected. This threshold is called <strong>Alpha</strong>.</p>
<p><strong>Alpha</strong> is the threshold p value below which we are sufficiently convinced (surprised) that we can reject the null. We usually set the alpha level prior to performing an experiment or collecting data so we ensure a low probability of committing a type 1 error. Both an alpha level and a p-value are needed to perform a classical null hypothesis test.</p>
<p>You can reject the null if <em>p</em> &lt; alpha. Conversely you will fail to reject the null hypothesis if <em>p</em> &gt; alpha (you are insufficiently convinced that you are NOT living in the null universe!)</p>
<p>A <em>type 2 error</em> is mistakenly failing to reject a false null hypothesis (you are not living in the null universe but you fail to realize it!)</p>
<p><strong>Beta</strong> is the probability of committing a Type 2 error.</p>
<p><strong>Q</strong> what would it mean to set the Beta level? Why do we set alpha rather than beta?</p>
<p><strong>Q</strong> Related question: how many universes are there where the null hypothesis is true? How about where the null hypothesis is false?</p>
<p>Finally <em>Power</em> is defined as the probability of correctly rejecting an incorrect null hypothesis. Power is simply 1-Beta!</p>
<p>Power is influenced by sample size and effect size (signal strength in the population of interest), among other things. A <em>Power analysis</em> is an attempt to see how much power you have to detect a true signal under a given sampling design.</p>
<p><img src="clt1.jpg" /></p>
</div>
</div>
<div id="quick-review-of-basic-concepts" class="section level2">
<h2>Quick Review of Basic Concepts</h2>
<p>A p value is the probability of obtaining a result at least as extreme (containing at least as much ‘signal’) as the observed result due to random sampling, given you are dwelling in the null universe.</p>
<p>Usually the ‘result’ we are referring to is a single ‘summary statistic’ calculated from our sample and representing the amount of signal in our sample – for example, the sample mean, the difference between two sample means relative to sampling error (as in a 2-sample t test), some other signal-to-noise ratio, or the proportion of a sample that meets some criterion (e.g., proportion red M+Ms).</p>
<p>This implies a multitude of potential results. Your observed result (observed test statistic) is just one of an infinite number of possible results from some sampling/measurement process. This variation across possible samples/results is called ‘sampling variation’ or ‘sampling error’.</p>
<p>Imagine buying a random bag of M&amp;Ms from a random convenience store (one of many possible bags you could buy at many possible convenience stores) and counting the proportion of red ones. There are practically countless possible samples (bags) we could get (each one potentially associated with a different proportion of reds), even though there is only one true proportion of red M+Ms in the world. Each dataset you collect is like one bag of M&amp;Ms out a much larger population of potential bags.</p>
<p><strong>the truth is fixed but unknown and we can only approach the truth indirectly and imperfectly via sampling</strong></p>
<p>Now imagine we want to test the hypothesis that there are a greater proportion of red M&amp;Ms than other five classic colors– that is, that P(red)&gt;(1/6). We start off by pretending we live in the null universe (first level of pretending in classical statistics), and we start collecting imaginary samples (LOTS of potential bags of M&amp;Ms– this is the second level of pretending!) in the null universe where reds are exactly 1/6 of the population. We then determine the probability that these simulated bags have a greater proportion of reds than our observed results (proportion of reds in our bag). This is our p-value. We use our p-value, along with our pre-determined type I error rate (threshold level of surprise we need to reject the null universe).</p>
</div>
<div id="statistics-inference-from-a-sample" class="section level2">
<h2>Statistics: inference from a sample</h2>
<p>Consider the following example:</p>
<p>Yellow-Legged frog example:<br />
<em>Population</em>: all yellow-legged frogs in ponds in the central Sierra Nevada<br />
<em>Parameter</em>: mean body size (SVL) of adult yellow-legged frogs in all ponds in the central Sierra Nevada<br />
<em>Sample</em>: As many frogs are captured and measured as possible in 10 ponds randomly sampled from the central Sierra Nevada<br />
<em>Statistic</em>: Sample mean</p>
<p>Even more generally, the goal of statistics (even when not performing a test) is to infer something about a population from a sample. In the yellow-legged frog example below, there is a “true” mean body size of frogs in ponds in the central Sierra Nevada. We just don’t know what it is! After collecting the data, statistics might help use to say something about the mean body size in the population – both about what we know AND what we don’t know!</p>
<p>Really, all of statistics is about dealing with uncertainty. We don’t need statistics if we have complete certainty!</p>
<p>However, we assume that our sample mean (and many other summary statistics derived from a sample) is representative of the population. How? Why? Because of the <em>Central Limit Theorem</em>.</p>
</div>
<div id="the-central-limit-theorem" class="section level2">
<h2>The Central Limit Theorem</h2>
<p>The Central Limit Theorem (CLT) says that if you have a sample with a reasonably large number of observations, and each observation is randomly sampled, then the sample mean will be similar to the actual population mean: the mean of ALL yellow-legged frogs in ponds in the central Sierra Nevada (how similar? that’s where the t-distribution comes in- we’ll get there). And as the sample size gets bigger, the sample mean will become more representative of the true mean (it will converge on the true mean as sample size approaches infinity).</p>
<p>The concept of <strong>regression to the mean</strong> is a natural consequence of the Central Limit Theorem!</p>
<p>[In-class R demo: regression to the mean]</p>
<p>This is useful, but that’s not all.</p>
<p>The CLT is the magic wand of statistics. It does enormous amounts of work for us. Why?</p>
<p>The CLT also implies that the distribution of sample means collected from repeated sampling is approximately normally distributed even if the underlying data themselves are not normally distributed.</p>
<p>Did you ever wonder why the normal (Gaussian) distribution is so common in statistics? It’s because of the CLT- many summary statistics derived from a sample are expected to have a sampling distribution that is <em>approximately</em> normally distributed (based on the CLT)!</p>
<p><img src="clt1.png" /></p>
<div id="example" class="section level3">
<h3>Example</h3>
<p>How does this work? Let’s use the yellow-legged frog example.</p>
<p>Let’s say that we could measure ALL the frogs in ALL the ponds in CA. What would that look like?</p>
<p>Let’s simulate it using a log-normal distribution that is strongly right skewed (positively skewed), suggesting that there are a lot of frogs out there that are relatively small-bodied, and a few that are giants! NOTE: this is not necessarily biologically realistic, but it makes a point.</p>
<p>First, let’s set the population/parameter (the truth about which we hope to make inference but can never know in reality)</p>
<p><strong>Q</strong> if we could measure the entire population of interest, do we even need statistics???</p>
<pre class="r"><code>#### ALL FROGS IN CA

allfrogs.bodysize &lt;- rlnorm(10000,1.5,0.4)        # statistical &#39;population&#39;
hist(allfrogs.bodysize,main=&quot;&quot;,xlab=&quot;SVL (mm)&quot;)   # plot out histogram</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>truemean_SVL &lt;- mean(allfrogs.bodysize)           # the &#39;parameter&#39;
truemean_SVL </code></pre>
<pre><code>## [1] 4.847143</code></pre>
<p>Now let’s take a sample!</p>
<pre class="r"><code>mysample &lt;- sample(allfrogs.bodysize,10)    # take sample of size 10 (10 frogs measured)
mean(mysample)   # compute the sample mean</code></pre>
<pre><code>## [1] 4.417766</code></pre>
<p>And another, this time with n=20</p>
<pre class="r"><code>mysample &lt;- sample(allfrogs.bodysize,20)    # take sample of size 20 (20 frogs measured)
mean(mysample)   # compute the sample mean</code></pre>
<pre><code>## [1] 4.444531</code></pre>
<p>Since sampling is random, sampling will produce a different result every time.</p>
<p>To get a better picture of the sampling variance, let’s sample many times!</p>
<pre class="r"><code>lotsofsamples &lt;- list()

for(s in 1:5000){
  lotsofsamples[[paste0(&quot;sample&quot;,s)]] &lt;- sample(allfrogs.bodysize,30)    # take sample of size 30 (20 frogs measured)
}

lotsofsamples$sample1</code></pre>
<pre><code>##  [1]  5.086661  3.952056  4.972603  3.535725  7.646382  5.160313  4.288609
##  [8]  5.145795  7.583052  7.127851  5.220969  4.783101  6.618937  1.551185
## [15]  3.732985  2.913098  3.165419  3.428988  6.229032  1.454695  6.051893
## [22]  8.754534  3.171811 16.009213  4.888946  2.710481  3.693597  4.124861
## [29]  3.877815  2.404494</code></pre>
<pre class="r"><code>lotsofsamples$sample99</code></pre>
<pre><code>##  [1] 3.557915 4.546439 6.218008 5.309469 8.149210 6.703916 3.474645
##  [8] 2.651852 3.721907 2.926279 4.418269 8.410390 5.881519 3.887558
## [15] 7.503658 8.534694 3.922078 3.562243 3.782464 2.695983 7.416709
## [22] 7.119845 6.865548 3.139050 4.525872 3.111711 3.306676 4.431164
## [29] 4.753881 4.090956</code></pre>
<pre class="r"><code>lotsofsamples$sample732</code></pre>
<pre><code>##  [1]  2.409691  2.321132  4.016501  4.260324  3.926378  2.703979  3.583664
##  [8] 12.386349  4.487709  2.273845  5.337164  6.032742  2.784819  3.061488
## [15]  3.056266  4.017696  8.899836  4.722898  2.897465  5.314592  4.436262
## [22]  5.241118  3.528245  3.759238  8.068158  6.050058  4.627544  4.456882
## [29]  7.053938  2.698007</code></pre>
<p>Now we can compute the sample means and the sampling variance for the summary statistic (mean body size)</p>
<pre class="r"><code>samplemeans &lt;- sapply(lotsofsamples,mean)

hist(samplemeans,xlab=&quot;mean body size (n=30)&quot;)</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Interesting- does this look skewed to you? Doesn’t it look like a normal distribution??</p>
<p>It’s the CLT at work!!</p>
<p>One way to think about this is just that there are more ways of getting a sample mean near the real mean than to get one far away from the sample mean. It doesn’t matter what the raw distribution is. Of all the samples you could get, there are very few that are all at one end of the distribution. There are a lot more possible random samples that span the full distribution of values, from low to high. Take the average af all those values, low and high, and you get something in the middle. The normal distribution is humped right in the middle, because of the tendency for low and high observations to ‘average out’ within a sample.</p>
<p>Craps example (rolling a pair of dice). There are many more ways to get 7 than to get 2!</p>
<p>This works with coin flips too!</p>
<p>Here’s the sampling distribution for the number of heads out of a single coin flip:</p>
<pre class="r"><code>hist(rbinom(10000,1,.5),xlab=&quot;N heads out of 1&quot;)</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Now let’s build up sample size and see how the sampling distribution changes.</p>
<pre class="r"><code>par(mfrow=c(3,2))
for(i in seq(2,12,2)){
   hist(rbinom(10000,i,.5),main=paste0(&quot;sample size = &quot;,i),xlab=sprintf(&quot;N heads out of %s&quot;,i)) 
}</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>And with really big sample size:</p>
<pre class="r"><code>hist(rbinom(10000,1000,.5),xlab=&quot;N heads out of 1&quot;)</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
</div>
<div id="replication-and-pseudoreplication" class="section level2">
<h2>Replication and pseudoreplication</h2>
<p>Let’s explore replication, and what it truly means.</p>
<p>Make sure you read this <a href="hurlburt.pdf">highly influential monograph on ‘Pseudoreplication’ by Stuart Hurlburt</a>.</p>
<p>And here’s <a href="daviesandgray.pdf">one more paper for good measure, by Davies and Gray (2015)</a>: this is a counter to the Hurlburt article.</p>
<p>Clearly one of the main take-aways is: science is messy!!!</p>
<p>Why does science require replication?</p>
<p>In general, the scientific project is to discover generalizable truths.</p>
<p>Q: How do we know if a result is true, and not a result of just random noise?<br />
A: We use a large enough sample size so that we can convince ourselves that random noise (sampling variation) could not cause the result!</p>
<p>Q: How do we know if a result is general, and not a result of just some localized or specific phenomenon?<br />
A: We draw our sample randomly from the entire population so that our sample is truly representative!</p>
<div id="find-the-pseudoreplication" class="section level3">
<h3>Find the pseudoreplication!</h3>
<p><em>Population</em>: All little brown bats across the USA<br />
<em>Parameter</em>: Infection rate of white nose syndrome infection in cave hibernacula<br />
<em>Sample</em>: Two cave hibernacula in New York state.<br />
<em>Statistic</em>: Infection rate among sampled bats</p>
<p><em>Population</em>: All humans<br />
<em>Parameter</em>: Effectiveness of a coronavirus vaccine<br />
<em>Sample</em>: 10,000 humans sampled in Sweden<br />
<em>Statistic</em>: Infection rate in Uppsala (control) vs infection rate in Helsingborg (treatment)</p>
<p><em>Population</em>: all yellow-legged frogs in ponds in the central Sierra Nevada<br />
<em>Parameter</em>: mean body size (SVL) of adult yellow-legged frogs in all ponds in the central Sierra Nevada<br />
<em>Sample</em>: 3,000 frogs sampled from a single pond in the central Sierra Nevada<br />
<em>Statistic</em>: Sample mean</p>
<p>The takeaway: you can only convince yourself of the generality of a result if the sample is representative of the population. In experimental design, one way to try to ensure generality is to sample randomly from the population of interest.</p>
<p>One part of the scientific endeavor is to poke at other people’s research to see if it stands up to scrutiny. Even more so, we poke at in our own research. If we can convince ourselves of the truth of our results and conclusions, only then can we feel comfortable sharing the results with the scientific community. That’s not out of meanness or masochism, it’s out of a search for truth and generality!</p>
<p>But the search for the truth is messy. In environmental science, we pseudoreplicate all the time - by necessity. For practical reasons our observations are not always completely independent from one another.</p>
<p>Are we ever truly replicating perfectly and sampling sufficiently from our generalized target of inference? In many cases we are not. But that shouldn’t stop us from trying to find truth- we just need to proceed with caution!</p>
</div>
</div>
<div id="assumption-all-data-points-are-independent" class="section level2">
<h2>Assumption: all data points are independent!</h2>
<p>Nearly all of the classical statistical analyses and tests we will go over in this class make a very important assumption – that all data points are independent samples drawn from the population of interest. Unfortunately, truly independent data points are far from the norm in ecology and environmental science! When data points are not independent, the information content of the sample (relative to the population of interest) is reduced. Does this make sense? When we treat non-independent data points as independent we are committing pseudoreplication!</p>
<p>Pseudoreplication, when left ‘untreated’ (i.e., subjected to statistical analyses that account for sources of non-independence), can result in using statistical methods inappropriately – mistakenly assuming that you have more information than you actually have.</p>
<div id="demo-non-independence-and-sampling-distributions" class="section level3">
<h3>Demo: non-independence and sampling distributions</h3>
<p>Let’s see what happens to sampling distributions with and without pseudoreplication. We are interested in testing whether mountain yellow-legged frogs in the cascade range tend to be heavier than frogs in the sierra nevada (which we know to be 1.5 g from previous studies). We take a sample of 100 individuals: 50 from each of 2 ponds.</p>
<p>We assume that frogs from the two mountain ranges are the same size on average (null hypothesis is true) and that the distribution of sizes is Gaussian. However, individuals from the same pond tend to be more similar to one another than individuals sampled from other ponds. Here is the scenario:</p>
<pre class="r"><code># pseudoreplication demonstration

meansize.allfrogs &lt;- 1.5    # population mean 
sdsize.allfrogs &lt;- 0.5     # population sd
sdsize.amongpond &lt;- 0.44   # standard deviation among ponds 

nponds &lt;- 5000   # total number of ponds in the population
nfrogs.perpond &lt;- 1000    # 1000 frogs in each pond</code></pre>
<p>So we can now generate the mean sizes for all ponds in the population and all frogs within each pond</p>
<pre class="r"><code>pondmeans &lt;- rnorm(nponds,meansize.allfrogs,sdsize.amongpond)
 # hist(pondmeans)
allfrogs &lt;- sapply(pondmeans, function(t) rnorm(nfrogs.perpond,t,sqrt(sdsize.allfrogs^2-sdsize.amongpond^2)) )
rownames(allfrogs) &lt;- paste0(&quot;frog&quot;,1:(nfrogs.perpond))
colnames(allfrogs) &lt;- paste0(&quot;pond&quot;,1:nponds)

  # confirm that population mean and standard deviation are as specified
sd(allfrogs)</code></pre>
<pre><code>## [1] 0.4986642</code></pre>
<pre class="r"><code>mean(allfrogs)</code></pre>
<pre><code>## [1] 1.511712</code></pre>
<p>Okay, now we have a population of frogs. Now we need to sample from this population!! Remember that for practical reasons we are only able to sample from 2 ponds.</p>
<pre class="r"><code>nponds.sampled &lt;- 2
nsamp.perpond &lt;- 50 

ponds.sampled &lt;- sample(1:nponds,nponds.sampled)
frogs.sampled &lt;- replicate(nponds.sampled,sample(1:nfrogs.perpond,nsamp.perpond))

thissamp &lt;- sapply(1:nponds.sampled,function(t) allfrogs[frogs.sampled[,t],ponds.sampled[t]])
rownames(thissamp) &lt;- paste0(&quot;frog&quot;,1:(nsamp.perpond))
colnames(thissamp) &lt;- paste0(&quot;pond&quot;,1:nponds.sampled)

head(thissamp)</code></pre>
<pre><code>##          pond1    pond2
## frog1 2.178336 1.594025
## frog2 2.146089 1.912239
## frog3 2.168829 1.617863
## frog4 2.563914 1.636544
## frog5 2.249525 2.043594
## frog6 1.839806 2.153330</code></pre>
<p>Okay, now that we have collected our sample, let’s run a t-test to see if the mean in our sample is different from 1.5</p>
<pre class="r"><code>test &lt;- t.test(as.vector(thissamp),mu=1.5,alternative=&quot;greater&quot;)
test</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  as.vector(thissamp)
## t = 13.31, df = 99, p-value &lt; 2.2e-16
## alternative hypothesis: true mean is greater than 1.5
## 95 percent confidence interval:
##  1.966942      Inf
## sample estimates:
## mean of x 
##  2.033495</code></pre>
<p>Okay, we have now collected one sample from this hypothetical scenario. To generate a sampling distribution we need to generate many more samples. Let’s do that now! Here we will run both a sampling scheme where we only sample 2 ponds and another sampling scheme where we sample 100 frogs randomly from the entire population.</p>
<pre class="r"><code>means &lt;- numeric(1000)
means.ind &lt;- numeric(1000)

ttest &lt;- numeric(1000)
ttest.ind &lt;- numeric(1000)

for(scenario in 1:1000){
  ponds.sampled &lt;- sample(1:nponds,nponds.sampled)
  frogs.sampled &lt;- replicate(nponds.sampled,sample(1:nfrogs.perpond,nsamp.perpond))
  
  thissamp &lt;- sapply(1:nponds.sampled,function(t) allfrogs[frogs.sampled[,t],ponds.sampled[t]])
  thissamp.ind &lt;- matrix(sample(allfrogs,nsamp.perpond*nponds.sampled),ncol=nponds.sampled)
  
  means[scenario] &lt;- mean(thissamp)
  means.ind[scenario] &lt;- mean(thissamp.ind)
  
  test &lt;- t.test(as.vector(thissamp),mu=1.5,alternative=&quot;greater&quot;)
  test.ind &lt;- t.test(as.vector(thissamp.ind),mu=1.5,alternative=&quot;greater&quot;)
  
  ttest[scenario] &lt;- test$p.value
  ttest.ind[scenario] &lt;- test.ind$p.value
  
}</code></pre>
<p>Let’s look at the sampling distribution of the mean difference between male and female body mass in the null universe. Here we compare the sampling distribution for the pseudoreplicated design vs the sampling distribution for the case with 100 true independent replicates.</p>
<pre class="r"><code>layout(matrix(1:2,nrow=1))
hist(means,xlim=c(0,3))
hist(means.ind,xlim=c(0,3))</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>length(which(ttest&lt;0.05))/1000</code></pre>
<pre><code>## [1] 0.423</code></pre>
<pre class="r"><code>length(which(ttest.ind&lt;0.05))/1000</code></pre>
<pre><code>## [1] 0.091</code></pre>
<p>In the pseudoreplicated design, we have a ca. 40% chance of incorrectly rejecting the null hypothesis with nominal alpha = 0.05! With independent samples the rate is around 0.05 as it should be.</p>
<p>Try running this script with less egregious pseudoreplication- e.g., with 5 ponds with 20 frogs each. How much does this improve the result??</p>
<p>NOTE: in the above example, the only reason pseudoreplication is a problem is that we are incorrectly assuming that the sampling distribution of our test statistic resembles the histogram on the right (independent samples). If we used the sampling distribution on the left (which correctly accounts for pseudoreplication) there would be no issue – except that we would need to see much more extreme results in order to reject our null hypothesis!</p>
</div>
</div>
<div id="summary-statistics-calculated-from-sample-data" class="section level2">
<h2>Summary statistics (calculated from sample data)</h2>
<p>We often summarize our samples by their centers (e.g., average) and possibly their spread (dispersion)</p>
<div id="center-statistics-means-medians-geometric-mean" class="section level3">
<h3>“Center” statistics: means, medians, geometric mean</h3>
<p>Sample Mean (arithmetic mean) = sum of all sampled values divided by the sample size<br />
Sample Median (midway point) = 50% quantile. Order the values and select the value at the center.<br />
Sample Geometric mean: product of numbers taken to the nth root. For two numbers, 3 and 4, you’d have the sq root of 3*4 = 3.46</p>
</div>
<div id="data-spread-or-dispersion" class="section level3">
<h3>Data spread, or dispersion:</h3>
<p>Sample Standard Deviation – sigma (<span class="math inline">\(\sigma\)</span>) for population standard deviation, <em>s</em> for the sample standard deviation.</p>
<p>Standard deviations and variances are calculated differently depending on whether we are computing these quantities for an entire population of interest vs a sample drawn from a larger population!<br />
The variance of a sample (<span class="math inline">\(s^2\)</span>) or the variance of the population parameter (<span class="math inline">\(\sigma^2\)</span>) can be computed as the square of the standard deviation.</p>
<p>The variance represents the average squared difference from the mean. The standard deviation represents the square root of the average squared difference from the mean.</p>
<p>Standard deviation is much more commonly reported than variance because it is in the same units/scale as the original measurements.</p>
<p>Coefficient of variation (cv) is the standard deviation represented as a fraction of the mean. The cv only has meaning for <em>ratio data</em>.</p>
<div id="std-deviation-calculation-example" class="section level4">
<h4>Std deviation calculation example</h4>
<p>For a population: <span class="math inline">\(\sigma = \sqrt{\sum_{n=1}^{i}{\frac{(x_i-\mu)^2}{N}}}\)</span><br />
For a sample (estimating population variance from a sample): <span class="math inline">\(s = \sqrt{\sum_{n=1}^{i}{\frac{(x_i-\bar{x})^2}{(N-1)}}}\)</span></p>
<p>For example: compute the variance of 5 numbers: 4, 3, 5, 5, 2</p>
<p><span class="math inline">\(\mu = (4+3+5+5+2)/5 = 3.8\)</span></p>
<p>(4-3.8)^2 = 0.2^2 = 0.04<br />
(3-3.8)^2 = 0.64<br />
(5-3.8)^2 = 1.44<br />
(5-3.8)^2 = 1.44<br />
(2-3.8)^2 = 3.24</p>
<p>Sum these = 6.8<br />
Divide by 5 = <span class="math inline">\(\sigma\)</span> = 6.8/5 = 1.36</p>
<p>For sample sd (summary statistic for population variance from a sample):</p>
<p><span class="math inline">\(\bar{x} = (4+3+5+5+2)/5 = 3.8\)</span></p>
<p>(4-3.8)^2 = 0.2^2 = 0.04<br />
(3-3.8)^2 = 0.64<br />
(5-3.8)^2 = 1.44<br />
(5-3.8)^2 = 1.44<br />
(2-3.8)^2 = 3.24</p>
<p>Sum these = 6.8<br />
Divide by 4 = <span class="math inline">\(s\)</span> = 6.8/4 = 1.7</p>
<p>So the population sd is 1.36 whereas the sample sd is 1.7.</p>
</div>
<div id="aside-degrees-of-freedom" class="section level4">
<h4>Aside: degrees of freedom</h4>
<p>OK, so why the different estimate of dispersion for population vs. sample?</p>
<p>Which is larger? Which are we less confident in?</p>
<p>This has to do with a concept called <em>degrees of freedom</em>.</p>
<p>Sigma is known. Since you have the entire population measured, you can compute a measure of dispersion for the population and that measure is perfect. It is not an estimate, it is a perfect point value that is known with certainty and that represents the square root of the average squared difference from the mean. No bias, perfect precision.</p>
<p>The sample standard deviation <span class="math inline">\(s\)</span>, on the other hand, is an imperfect estimate of dispersion for a much larger population!</p>
<p><strong>Q</strong> would you expect the sample standard deviation using the first formula (pop stdev) to be biased? Why or why not?? In what direction would it be biased?</p>
<p>NOTE: the formula for sample stdev <em>uses the sample mean</em>. Therefore, the sample data have already been used once to compute the standard deviation. If the true population mean was different than the sample mean, what would happen to the estimate of stdev if we replaced the sample mean with the true mean? Is it possible that the sample mean is not equal to the true mean? (of course it is!!)</p>
<p>NOTE ALSO: if you know that four of the five sampled values are 4, 3, 5, and 2 AND we know that the sample mean is 3.8, we know that the final sampled data point MUST be 5. There is no <em>freedom</em> there- the data point has to be 5. Therefore, even though we have 5 data points, we have only 4 <em>degrees of freedom</em> since the sample mean is included in the formula for the estimate for sample stdev. That is, we have four independent pieces of information that we can use for computing standard deviation (we ‘spent’ one degree of freedom already to compute the sample mean!).</p>
<p>By dividing the sum of squared deviations from the sample mean by 4 instead of 5, we are <em>unbiasing</em> the estimate of dispersion so that it is a better estimate of the population standard deviation.</p>
</div>
</div>
</div>
<div id="sampling-variance-variance-among-hypothetical-sample-summary-statistics" class="section level2">
<h2>Sampling variance (variance among hypothetical sample summary statistics)</h2>
<p>NOTE: sampling variance is NOT the same thing as the variance of a sample!</p>
<p>Review: we collect a random sample from a population. We want to know something about the population, but any summary statistic we compute from the sample (e.g., mean, median, variance, stdev, whatever!) will be an imperfect estimate of the true population parameter. So what do we do? How can we truly make inference about the population??</p>
<p>First, we need to know something about the sampling variance. We know the summary statistic will be different for every sample we collect, but <em>how different, really???</em>. How surprised should be be to get a sample statistic as or more extreme than a particular observation under the null hypothesis? What does the sample really tell us about the population of interest??</p>
<p>Statisticians have used probability calculus to work out the sampling distributions (often called sampling variance) for some common summary statistics. For example, the sample mean (super common summary statistic) has a theoretical sampling distribution that follows the t-distribution (N-1 degrees of N-1 for a single sample) and is centered on your null hypothesis value (e.g., zero) with dispersion described by the standard error of the mean. Let’s dissect this one!</p>
<div id="standard-error-of-the-mean" class="section level3">
<h3>Standard error of the mean</h3>
<p>Standard error of the mean = sample std deviation divided by the square root of the sample size</p>
<p><span class="math inline">\(se = \frac{s}{\sqrt{N}}\)</span></p>
<p>The standard error of the mean is used to help us describe the sampling distribution for the sample mean (the expected dispersion of sample means if you collected thousands of new samples and computed the mean).</p>
<pre class="r"><code>#######
# Sampling distribution: the sample mean

mysample &lt;- c(4.1,3.5,3.7,6.6,8.0,5.4,7.3,4.4)
mysample</code></pre>
<pre><code>## [1] 4.1 3.5 3.7 6.6 8.0 5.4 7.3 4.4</code></pre>
<pre class="r"><code>n &lt;- length(mysample)    # sample size
sample.mean &lt;- mean(mysample)  # sample mean
sample.stdev &lt;- sd(mysample)   # sample standard deviation (r uses denominator of n-1 by default!)
std.error &lt;- sample.stdev/sqrt(n) 

std.error </code></pre>
<pre><code>## [1] 0.6122995</code></pre>
<p>Now we have all the information we need to compute the sampling distribution for the sample mean.</p>
<p>Our sample mean is 5.375. But if we collected different samples of size n=8, we would get different values - even if the true population mean was 5.375. What does this distribution of values look like?</p>
<pre class="r"><code>sampdist &lt;- function(x){dt((x-sample.mean)/std.error,n-1)}
curve(sampdist,0,11,ylab=&quot;probability density&quot;,xlab=&quot;value&quot;,main=&quot;sampling distribution for the sample mean!&quot;)
abline(v=sample.mean,col=&quot;green&quot;,lwd=3)
confint &lt;- c(sample.mean+std.error*qt(0.025,n-1),sample.mean+std.error*qt(0.975,n-1))
abline(v=confint,col=&quot;blue&quot;,lty=2)</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>The vertical blue lines indicate the <em>confidence interval</em> around the mean with the <em>confidence level</em> set at 95%. We will talk about confidence intervals a lot more, but just know that about 95% of such confidence intervals generated from random samples should include the true mean. The confidence interval helps us visualize what might happen if we repeated our sampling over and over and over- how might our summary statistic change?</p>
<p>Note the use of the <em>t distribution</em> in the above code block. The t distribution represents the sampling variation that you would expect to observe around the sample mean (or an arbitrary hypothesized value) if you collected many many samples of the same size as yours (see more details below). The t distribution is expressed in units of standard error.</p>
<p>Just to cement this concept, let’s compare the distribution above (based on a t distribution) with a brute-force simulation method!</p>
<pre class="r"><code>#######
# Sampling distribution: the sample mean #2 (brute force simulation version)

mysample &lt;- c(4.1,1.5,3.7,6.6,8.0,4.5,5.3,4.4)
mysample</code></pre>
<pre><code>## [1] 4.1 1.5 3.7 6.6 8.0 4.5 5.3 4.4</code></pre>
<pre class="r"><code>n &lt;- length(mysample)    # sample size
sample.mean &lt;- mean(mysample)  # sample mean
sample.stdev &lt;- sd(mysample)   # sample standard deviation (r uses denominator of n-1 by default!)

simulated.samples &lt;- list()
for(s in 1:10000){
  sd1 &lt;- sqrt(sum((sample(mysample,length(mysample)-1,replace = T)-sample.mean)^2)/(length(mysample)-2))  # account for unknown standard deviation
  simulated.samples[[paste0(&quot;sample &quot;,s)]] &lt;- rnorm(n,sample.mean,sd1)
}
sampling.distribution &lt;- sapply(simulated.samples,mean)

plot(density(sampling.distribution),xlim=c(0,11),ylab=&quot;probability density&quot;,xlab=&quot;value&quot;,main=&quot;sampling distribution for the sample mean!&quot;,lwd=2)    # plot the brute-force sampling distribution
hist(sampling.distribution,add=T,freq=F)
par(new=T)
curve(sampdist,0,11,xlim=c(0,11),xaxt=&quot;n&quot;,yaxt=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,col=&quot;red&quot;,lwd=2)  # official sampling distribution
abline(v=sample.mean,col=&quot;green&quot;,lwd=3)</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Not a bad match right? Obviously it’s easier and faster to use the t distribution (and more accurate) to approximate the sampling distribution, but I hope this helps to cement the concept!!</p>
<p>NOTE: the t distribution accounts for uncertainty about the true population variance as well as the true population mean, which is why I did not just use the sample variance in the code block above (if I had, you would see that the t distribution had ‘heavier tails’ than the brute force distribution)</p>
</div>
</div>
<div id="the-t-statistic-signal-to-noise-ratio-for-comparing-1-or-2-means" class="section level2">
<h2>The t statistic: signal to noise ratio for comparing 1 or 2 means</h2>
<p>The <em>t statistic</em> (the t distribution is the theoretical sampling distribution for the t-statistic) is just another summary statistic that we can calculate from a sample. In this way it is no different from the sample mean or the standard deviation. But, since it has a simple, well described sampling variance, it helps us perform rigorous hypothesis tests and is therefore a very useful summary statistic!</p>
<p>Mathematically, <em>t</em> is the ratio of the departure of the sample mean from its hypothesized value to its standard error:</p>
<p><span class="math inline">\(t = \frac{(\bar{x}-\mu_0)}{s.e.}\)</span></p>
<p>Let’s assume that we are living in a null universe, and that our null hypothesis is that the sample mean is equal to zero.</p>
<p>This is almost easier to conceptualize if we imagine a <em>paired</em> design in which, say, a set of individuals are monitored at two points in time: before a treatment and after. Let’s imagine we are researching the effectiveness of a weight loss drug. We sample 25 overweight patients and weigh them before they start taking the drug and then weigh all of them 1 month after they start taking the drug. Never mind that this isn’t the best drug trial design!</p>
<p>So for each patient we compute the change in weight between the first and second measurements. In the null universe, the drug is not effective and we would expect the change to be no greater than expected by random chance. Our task is to convince ourselves that the results (mean weight loss across all patients) are meaningful, allowing us to break free of the null universe.</p>
<p>Remember: - the t statistic is the difference between the sample mean and some hypothesized value, standardized in units of standard error. - in classical null hypothesis testing, the t statistic represents the difference in standard errors between your sample mean and the hypothesized true mean. - the t distribution is the sampling distribution for the t statistic.</p>
<p>Here is a worked example, in R:</p>
<pre class="r"><code>## Paired t-test example:

weightloss.data &lt;- c(-10.4,-11.6,3.9,1.5,-0.3,-3.5 -10.0,-6.7,-6.1,-2.4,-6.0,2.3,0.1,-4.1,-3.2, -11.3,-3.2,-9.3,-7.5,-5.7,-0.1,0.0,-9.8,1.0,-11.9)
hist(weightloss.data,breaks=7)</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>mean.weightloss &lt;- mean(weightloss.data)
null.weightloss &lt;- 0
stdev.weightloss &lt;- sd(weightloss.data)
sample.size &lt;- length(weightloss.data)
std.error &lt;- stdev.weightloss/sqrt(sample.size)

t.statistic &lt;- (mean.weightloss-null.weightloss)/std.error
t.statistic</code></pre>
<pre><code>## [1] -4.544623</code></pre>
<pre class="r"><code>curve(dt(x,sample.size-1),-5.5,2)
abline(v=t.statistic,col=&quot;green&quot;,lwd=3)</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-19-2.png" width="672" /></p>
<pre class="r"><code>p=pt(t.statistic,sample.size-1)
p    # this is the p value</code></pre>
<pre><code>## [1] 7.241049e-05</code></pre>
<pre class="r"><code>####### Alternative: use R&#39;s built in t test

t.test(weightloss.data,alternative = &quot;less&quot;)   # should get the same p=value!</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  weightloss.data
## t = -4.5446, df = 23, p-value = 7.241e-05
## alternative hypothesis: true mean is less than 0
## 95 percent confidence interval:
##       -Inf -2.966463
## sample estimates:
## mean of x 
##   -4.7625</code></pre>
<p><strong>Q</strong> Does a significant p value here mean that everyone who takes the drug will lose weight? If not, what does it mean?</p>
<p>NOTE: as the df gets large, the t distribution approximates a <em>standard normal</em> distribution.</p>
<p>Fun fact: William Sealy Gosset – a student of Pearson, worked at Guinness brewery. He was hired to work out a way to determine the quality of stout. He published anonymously as ‘Student’ because Guinness prevented its employees from publishing. That’s why we call it the ‘Student’s t-test’!</p>
</div>
<div id="confidence-intervals" class="section level2">
<h2>Confidence intervals</h2>
<p>Confidence intervals, like p-values are super commonly used and are misinterpreted almost as often as they are used!</p>
<p>Remember that in classical, frequentist statistics, the population parameter of interest is fixed- there is no uncertainty associated with the population parameter itself – it’s just that we can only collect and assess a small sample from the much larger population. So it does not make sense to say something like “the parameter has a 90% chance of falling within the confidence interval”. The true parameter is either in the interval (100% probability) or it is not in the interval (0% probability). All we can say is something more like “90% of confidence intervals generated from different random samples would contain the true parameter”.</p>
</div>
<div id="other-statistics-and-sampling-distributions" class="section level2">
<h2>Other statistics and sampling distributions</h2>
<p>Pretty much all of classical statistics works this way. We compute summary statistics from data that have well-described sampling distributions. We climb into the null universe and, using the known sampling distribution, compute if our summary statistic (computed from our data) could have been a result of random sampling error. If not (if <span class="math inline">\(p\le\alpha\)</span>) then we remain unconvinced that the null universe is indeed false.</p>
<div id="goodness-of-fit-chi-square-test." class="section level3">
<h3>Goodness of fit/ Chi-square test.</h3>
<p>This test asks the question “do the data sort into categories as you hypothesized?”</p>
<p>Karl Pearson – Founded the first stats dept, was a socialist and social Darwinist… and became a eugenicist.</p>
<p><em>Example</em>: Are grad students more likely to be born in a given season?</p>
<div id="assumptions" class="section level4">
<h4>Assumptions</h4>
<ul>
<li>Data must be <em>categorical</em></li>
<li>Samples must be independent</li>
<li>Data must be randomly sampled from the population</li>
<li>There must be a sufficient sample size such that the expected number of observations in each categories is at least 5!</li>
</ul>
<p>The Chi-squared statistic is computed from the data. The sampling distribution for the statistic is called the <em>Chi-squared distribution</em>.</p>
</div>
<div id="the-chi-squared-statistic" class="section level4">
<h4>The Chi-squared statistic:</h4>
<p><span class="math inline">\(\chi^2 = \sum_{i=1}^k\frac{(x_i-exp_i)^2}{exp_i}\)</span></p>
<p>Here, <span class="math inline">\(exp_i\)</span> is the expected number of observations in category i under the null hypothesis. <span class="math inline">\(x_i\)</span> is the observed number in category i. So the Chi-squared statistic basically summarizes the degree to which the number of observations disagree with the expected number of observations across all categories.</p>
<p>Why is the numerator squared? First, this makes all deviations positive- negatives and positives can’t cancel each other out. Second, it allows us to use the well-described Chi-squared distribution!</p>
</div>
<div id="the-chi-squared-distribution" class="section level4">
<h4>The Chi-squared distribution</h4>
<p>The Chi-squared distribution is described (like the <em>t</em> distribution) by a certain degrees of freedom.</p>
<p>The degrees of freedom for this test is one less than the number of categories.</p>
</div>
<div id="example-in-r" class="section level4">
<h4>Example in R</h4>
<pre class="r"><code>## Chi squared example

birthdays.bymonth &lt;- c(40,23,33,39,28,29,45,31,22,34,44,20)
months &lt;- c(&quot;Jan&quot;,&quot;Feb&quot;,&quot;Mar&quot;,&quot;Apr&quot;,&quot;May&quot;,&quot;Jun&quot;,&quot;Jul&quot;,&quot;Aug&quot;,&quot;Sep&quot;,&quot;Oct&quot;,&quot;Nov&quot;,&quot;Dec&quot;)
names(birthdays.bymonth) &lt;- months

sample.size &lt;- sum(birthdays.bymonth)
k = length(birthdays.bymonth)   # number of categories
exp.birthdays.bymonth &lt;- sample.size*rep(1/k,times=k)

Chisq.stat &lt;- sum((birthdays.bymonth-exp.birthdays.bymonth)^2/exp.birthdays.bymonth)
Chisq.stat</code></pre>
<pre><code>## [1] 24.14433</code></pre>
<pre class="r"><code>## View the summary statistic along with its sampling distribution under the null hypothesis

curve(dchisq(x,k-1),0,75)
abline(v=Chisq.stat,col=&quot;green&quot;,lwd=3)</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>p &lt;- 1-pchisq(Chisq.stat,k-1)
p</code></pre>
<pre><code>## [1] 0.01213825</code></pre>
<pre class="r"><code>### use R&#39;s built in chi squared function

chisq.test(birthdays.bymonth)     # should get the same p value!</code></pre>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  birthdays.bymonth
## X-squared = 24.144, df = 11, p-value = 0.01214</code></pre>
</div>
</div>
<div id="z-tests" class="section level3">
<h3>Z-tests</h3>
<p>The Z-test is a simpler version of the t test for when sample size is large enough (n&gt;50) or the population dispersion is known.</p>
<p>The Z statistic is:</p>
<p><span class="math inline">\(Z = \frac{(\bar{X}-\mu_0)}{s.e.}\)</span></p>
<p>As you can see, this looks very much like the t statistic! Actually it is exactly the same. The difference is that we are now assuming that the sampling variation for this statistic is normally distributed!</p>
<div id="example-z-test-golden-state-warriors-height" class="section level4">
<h4>Example z-test: Golden State Warriors height</h4>
<p>Tall basketball players are often successful. The Golden State Warriors are often successful. How tall are the Golden State Warriors relative to other basketball players in the NBA?</p>
<p>The GSW are 15 players with an average height of 6’8” (80”). A large survey of NBA players suggests they are, on average, 6’7” tall (79") with a standard deviation of 4 inches.</p>
<p>NOTE: the z-test is only appropriate here because we know the population standard deviation.</p>
<p>NOTE: B. Sullivan obtained those values from research into the height of NBA players.</p>
<p>Here, the mean (mu) and the standard deviation (q) are coming from the population (all NBA players) instead of the sample. There are situations where you can use the standard deviation of the sample, but those should be only when the sample size is very large.</p>
<pre class="r"><code>## Z test  (Ben Sullivan example)

df &lt;- read.csv(&quot;GSW_height.csv&quot;)
GSWheight &lt;- df$Height
GSWheight</code></pre>
<pre><code>##  [1] 75 75 81 79 78 84 79 81 81 79 83 79 83 81 77</code></pre>
<pre class="r"><code>mean.gsw &lt;- mean(GSWheight)
sd.gsw &lt;- sd(GSWheight) 
sd.pop &lt;- 4
n &lt;- length(GSWheight)
s.e. &lt;- sd.pop/sqrt(n)



null.height &lt;- 79

z.statistic &lt;- (mean.gsw-null.height)/s.e.
z.statistic</code></pre>
<pre><code>## [1] 0.6454972</code></pre>
<pre class="r"><code>curve(dnorm(x),-3,3)
abline(v=z.statistic,col=&quot;green&quot;,lwd=3)</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r"><code>p &lt;- 1-pnorm(z.statistic)    # is the p value enough evidence to tell you that GSW players are taller than the NBA average??
p       </code></pre>
<pre><code>## [1] 0.2593025</code></pre>
<pre class="r"><code>pnorm(z.statistic)</code></pre>
<pre><code>## [1] 0.7406975</code></pre>
<p><strong>Q</strong> That means that 74% of NBA players are shorter than the GSW average height, right?</p>
<p>Wrong- that means that 74% of samples of 15 players drawn randomly from the population of NBA players would have a mean height less than or equal to the mean height of the GSW players. Conversely 26% of samples would have a mean height greater than that of the GSW team (that’s the p value).</p>
<p><strong>Q</strong> So can you reject the null hypothesis and say that the GSW team is taller than expected by random chance?</p>
</div>
</div>
<div id="f-tests-anova-regression" class="section level3">
<h3>F tests (ANOVA, regression)</h3>
<p>Fill this in</p>
</div>
</div>
<div id="the-p-value-good-or-evil" class="section level2">
<h2>The p-value: good or evil?</h2>
<p>There has been a lot of hating on p-values in recent decades. Largely this is due to frequent misinterpretation of what they mean, unthinking use of the alpha=0.05 threshold, and failure to report other important pieces of information like effect sizes and confidence intervals (reporting only p-values is called the ‘naked p-value’ phenomenon!).</p>
<p>The American Statistical Association (ASA) released a statement about p-values and their misuse:</p>
<p><a href="pval_asa1.pdf">Summary of ASA statement</a><br />
<a href="pval_asa2.pdf">ASA statement</a></p>
<p>P-values remain very important and widely used and reported. But we need to keep in mind the common pitfalls and try to avoid them!</p>
</div>
<div id="probability-distributions--the-basics-and-how-to-use-them-in-r" class="section level2">
<h2>Probability distributions- the basics (and how to use them in R)</h2>
<div id="discrete-vs.-continuous" class="section level3">
<h3>Discrete vs. continuous</h3>
<p>In <em>discrete distributions</em>, each outcome has a specific probability (like the probability of flipping a coin 10 times and getting 4 heads). For example, let’s consider the Poisson distribution</p>
<pre class="r"><code>#################
# Probability distributions

mean &lt;- 5
rpois(10,mean)    # the random numbers have no decimal component</code></pre>
<pre><code>##  [1] 4 3 4 9 5 5 7 8 5 4</code></pre>
<pre class="r"><code>             # plot a discrete distribution!
xvals &lt;- seq(0,15,1)
probs &lt;- dpois(xvals,lambda=mean)
names(probs) &lt;- xvals
               
barplot(probs,ylab=&quot;Probability&quot;,main=&quot;Poisson distribution (discrete)&quot;)</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>barplot(cumsum(probs),ylab=&quot;Cumulative Probability&quot;,main=&quot;Poisson distribution (discrete)&quot;)   # cumulative distribution</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-22-2.png" width="672" /></p>
<pre class="r"><code>sum(probs)   # just to make sure it sums to 1!  Does it??? </code></pre>
<pre><code>## [1] 0.999931</code></pre>
<p>In <em>continuous distributions</em>, the height of the curve corresponds to <em>probability density</em>, <span class="math inline">\(f(x)\)</span>, not probability <span class="math inline">\(Prob(x)\)</span>. This is because the probability of getting exactly one value in a continuous distribution is effectively zero. This arises from the problem of precision. The sum of the probability distribution must be 1 (there is only 100% of probability to go around). In a continuous distribution, there are an infinite number of possible values of x. So any individual probability is always divided by infinity, which makes it zero. Therefore we have to talk about probability density, unless we want to specify a particular range of values – we can’t calculate <span class="math inline">\(Prob(x = 5)\)</span>, but we can calculate <span class="math inline">\(Prob(4 &lt; x &lt; 6)\)</span> or <span class="math inline">\(Prob(x &gt; 5)\)</span>. Let’s consider the beta distribution:</p>
<pre class="r"><code>#########
# continuous distributions

shape1 = 0.5
shape2 = 0.5

rbeta(10,shape1,shape2)</code></pre>
<pre><code>##  [1] 0.086404688 0.893367684 0.064836894 0.007518622 0.117761028 0.854379153
##  [7] 0.041965947 0.897675434 0.834074868 0.125783456</code></pre>
<pre class="r"><code>curve(dbeta(x,shape1,shape2))   # probability density</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre class="r"><code>curve(pbeta(x,shape1,shape2))   # cumulative distribution</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
<pre class="r"><code>integrate(f=dbeta,lower=0,upper=1,shape1=shape1,shape2=shape2)    # just to make sure it integrates to 1!!</code></pre>
<pre><code>## 1 with absolute error &lt; 3e-06</code></pre>
<div id="some-other-probability-distribution-terms" class="section level4">
<h4>Some other probability distribution terms:</h4>
<p><strong>Moments</strong> – descriptions of the distribution. For a bounded probability distribution, the collection of all the moments (of all orders, from 0 to infinity) uniquely determines the shape of the distribution.</p>
<ul>
<li><p>The zeroth central moment (<span class="math inline">\(\int \left ( x-\mu \right )^{0}Prob(x)\partial x\)</span>) is the total probability (i.e. one),<br />
</p></li>
<li><p>The first central moment (<span class="math inline">\(\int \left ( x-\mu \right )^{1}Prob(x)\partial x\)</span>) is <span class="math inline">\(\mu - \mu = 0\)</span>.<br />
</p></li>
<li><p>The second central moment (<span class="math inline">\(\int \left ( x-\mu \right )^{2}Prob(x)\partial x\)</span>) is the variance.<br />
</p></li>
<li><p>The third central moment (<span class="math inline">\(\int \left ( \left (x-\mu \right )/\sigma \right )^{3}Prob(x)\partial x\)</span>) is the skewness.</p></li>
<li><p>The fourth central moment is the kurtosis.</p></li>
</ul>
<p><strong>Parameters</strong> – the values in the probability distribution function, describing the exact shape and location of the distribution. <em>Parametric statistics</em> require assuming certain things about distributions &amp; parameters, while <em>nonparametric stats</em> do not require these assumptions.</p>
<p><strong>PDF</strong> -</p>
<p><strong>CDF</strong> -</p>
<p><strong>Quantiles</strong> -</p>
</div>
</div>
<div id="some-probability-distributions" class="section level3">
<h3>Some probability distributions</h3>
<p>The Bolker book goes through the main distributions we will be using in this course. Pay particular attention to the type of <em>process</em> described by each distribution. The key to using these distributions to represent random variables is to figure out which statistical process best matches the ecological process you’re studying, then use that distribution. e.g., am I counting independent, random events occurring in a fixed window of time or space (like sampling barnacles in quadrats on an intertidal bench)? Then the distribution of their occurrence is likely to follow a Poisson or Negative Binomial distribution.</p>
<div id="binomial" class="section level4">
<h4>Binomial</h4>
<pre class="r"><code>##########
# Binomial

size &lt;- 10
prob &lt;- 0.3
rbinom(10,size,prob)</code></pre>
<pre><code>##  [1] 4 6 3 3 1 2 1 5 2 3</code></pre>
<pre class="r"><code>xvals &lt;- seq(0,size,1)
probs &lt;- dbinom(xvals,size,prob)
names(probs) &lt;- xvals
               
barplot(probs,ylab=&quot;Probability&quot;,main=&quot;Binomial distribution&quot;)</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code>barplot(cumsum(probs),ylab=&quot;Cumulative Probability&quot;,main=&quot;Binomial distribution&quot;)   # cumulative distribution</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-24-2.png" width="672" /></p>
<pre class="r"><code>sum(probs)   # just to make sure it sums to 1!  Does it???</code></pre>
<pre><code>## [1] 1</code></pre>
</div>
<div id="normal" class="section level4">
<h4>Normal</h4>
<pre class="r"><code>#########
# Gaussian

mean = 7.1
stdev = 1.9

rnorm(10,mean,stdev)</code></pre>
<pre><code>##  [1] 3.475569 6.482166 7.311010 3.691324 7.177475 9.002097 7.605930 4.110654
##  [9] 6.769958 8.333890</code></pre>
<pre class="r"><code>curve(dnorm(x,mean,stdev),0,15)   # probability density</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<pre class="r"><code>curve(pnorm(x,mean,stdev),0,15)   # cumulative distribution</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-25-2.png" width="672" /></p>
<pre class="r"><code>integrate(f=dnorm,lower=-Inf,upper=Inf,mean=mean,sd=stdev)    # just to make sure it integrates to 1!!</code></pre>
<pre><code>## 1 with absolute error &lt; 1.1e-05</code></pre>
</div>
<div id="t-distribution" class="section level4">
<h4>t distribution</h4>
<pre class="r"><code>#########
# t distribution

df = 6

rt(10,df)     # random numbers from the t distribution (not sure why you would ever want this!)</code></pre>
<pre><code>##  [1]  2.08129208  0.77581980 -0.56303887  1.34677265 -0.40333110  0.74470069
##  [7] -0.01191238 -1.85427918  0.41542367 -1.23090331</code></pre>
<pre class="r"><code>curve(dt(x,df),-4,4)   # probability density</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre class="r"><code>curve(pt(x,df),-4,4)   # cumulative distribution</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-26-2.png" width="672" /></p>
<pre class="r"><code>integrate(f=dt,lower=-Inf,upper=Inf,df=df)    # just to make sure it integrates to 1!!</code></pre>
<pre><code>## 1 with absolute error &lt; 1.9e-05</code></pre>
</div>
<div id="chi-squared-distribution" class="section level4">
<h4>Chi-squared distribution</h4>
<pre class="r"><code>#########
# Chi-squared distribution

df = 6

rchisq(10,df)     # random numbers from the t distribution (not sure why you would ever want this!)</code></pre>
<pre><code>##  [1]  0.8128037  5.1009255  3.0094739 14.1825489  7.2478778  5.5541595
##  [7]  5.2636408  6.4238954  6.5538909  2.4683689</code></pre>
<pre class="r"><code>curve(dchisq(x,df),0,15)   # probability density</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre class="r"><code>curve(pchisq(x,df),0,15)   # cumulative distribution</code></pre>
<p><img src="LECTURE1_files/figure-html/unnamed-chunk-27-2.png" width="672" /></p>
<pre class="r"><code>integrate(f=dchisq,lower=0,upper=Inf,df=df)    # just to make sure it integrates to 1!!</code></pre>
<pre><code>## 1 with absolute error &lt; 2.3e-05</code></pre>
</div>
<div id="exercise" class="section level4">
<h4>Exercise:</h4>
<p>Visualize (in R) the following distributions as above: Gamma, Exponential, Lognormal, Negative Binomial.</p>
<p><a href="LECTURE2.html">–go to next lecture–</a></p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
