---
title: "Random Effect Models"
author: "NRES 710"
date: "Last compiled: `r Sys.Date()`"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
    css: styles.css
---

<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
	)
```

```{r echo=FALSE}

#  NRES 710, Random Effect Models
#     University of Nevada, Reno
#     Multi-variable models with random effects

```

Review quiz for interactions.

## Review of recent material

We have been learning about **multi-variable models** using a generic model:

$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 +... + \beta_nX_n + \epsilon \sim N(0, \sigma)$

In theory, you could have unlimited number of X-variables... but in practice, you might get 8 or 10 before it gets nonsensically complicated.

We can have **interactions** (e.g., $\beta_m * X_1 * X_2$).

- Note: Sometimes other fields refer to this in other ways (e.g., 'effect modification'; where one effect modifies another effect).

We have talked about the **advantages** of multi-variable models: eliminates swamping, eliminates bias associated with collinearity (but inflates variance), and allows us to look for interactions.

When we build models with interactions, we should generally include the 'main effects' as well (i.e., the individual effects of $X_1$ or $X_2$).

## Random-effects Models

My main goal today is to introduce the concept of 'random-effects models'. 

Before we do that, let's take a step back and temporarily remove interactions from our brain. Interactions are confusing! Instead, let's again consider more simple models without interactions -- and focus on our linear model without interactions.

But now let's make things a little more complicated again, in a different way. Let's start with an example.

### Example: Fish Density

Let's say that we have three variables: 

**Y -- fish density**   
**$X_1$ -- flow**   
**$X_2$ -- year**   

We have gone into a particular stream and measured the abundance of fish at ~eight different stretches in the stream. We have measured flow rate, because we are interested in understanding how flow rates influence abundance of fish.

But then we repeated the study over a multiple-year period, and we went back measured fish density and flow rate over a 14-year period!

**2010--2024**

**Q:** does anybody have any idea whether fish populations tend to vary from year to year? Yes, they do: fish density can be quite sensitive to rainfall, flowrate, stream disturbance, whatever. We might want to test whether fish density varies from year-to-year.

How would we include 'year' in our model...?

$FishDens = \beta_0 + \beta_12011 + \beta_22012 + ... + \beta_{14}2016 + \epsilon \sim N(0, \sigma)$ -- lots of terms here

Alternatively, we could have written the model like this:

$FishDens = \beta_0 + \beta_1Year + \epsilon \sim N(0, \sigma)$

**Q:** What do you think about this second model -- is it a good model?

No, because it's a categorical variable...?

But it's a number, why can't we treat this as continuous?

The problem with the second model is we are assuming that there is a linear relationship between year and fish density, where fish density will always be going up or always going down through time. This *might* be an okay assumption... if we have a specific hypothesis suggesting that there might be a linear effect of time on fish abundance.

Graphically, our study design and data might look like:

```{r echo=FALSE, fig.height=4, fig.width=4}
# Specify plot margins & set random seed
par(mar = c(4,4,0,0))
set.seed(123)

# Simulate data
years <- 2010:2024
n_years <- length(years)

# Number of stretches
n_stretches <- 8
stretch <- 1:8

# Mean density within each year (assumed values)
density_mu <- rnorm(n_years, 8, 2)

# Expand grid to simulate combinations of years and stretches
datum <- expand.grid(year = years, stretch = stretch)

# Sort the dataframe by year (low to high)
datum <- datum[order(datum$year), ]

# Add a fish density column
datum$FishDensity <- rep(NA, n_stretches * n_years)

# Use the density_mu to simulate fish density for each year-stretch combination
for (i in 1:n_years){
  for (j in 1:n_stretches){
    datum$FishDensity[(i - 1) * n_stretches + j] <- rnorm(1, mean = density_mu[i], sd = 0.5)
  }
}

# Plot Fish Density over Years
plot(datum$year, datum$FishDensity, xlab = "Year", ylab = "Fish Density")

```

In this example, the fish abundance bounces all over the place. It's not going to be a linearly increasing or decreasing relationship. Most of the variation is explained by year in a categorical sense.

**Q:** What do you think of the first model?

It's LONG and UGLY. It's not very elegant. And do we really care if... 2013 is different from 2019? Or 2017 is different from 2024?

My point is: there would be a ton of comparisons you would have to do! How many pairwise comparisons would a post-hoc test have to do?

A post-hoc test would make `13 + 12 + 11 + 10 + 9 + 8 + 7 + 6 + 5 + 4 + 3 + 2 + 1` interannual comparisons, which equals `r sum(13:1)` pairwise comparisons!

What we really want to do by including **Year** in the model is to account for  variation that occurs from year to year -- and we can do that in a different way than the way described here with the long model.

Here's how we do that. Instead, we would run this model (I'm not going to include **Flow** just yet)...

$FishDensity = \beta_0 + \epsilon_{year} \sim N(0, \sigma_{year}) + \epsilon_{residual} \sim N(0,  \sigma_{residual}))$

So now we have an equation with two error terms: annual yearly error and residual error.

Let's think about what this means...

#### Annual error

The variation in fish density from year to year can be described by some normal probability distribution with some standard deviation, \sigma. We can think about this as: the <u>**between-year variation**</u>. 

Graphically, this might be:

```{r echo=FALSE, fig.height=4, fig.width=8}
# Specify plot margins & set random seed
par(mfrow = c(1, 2), mar = c(4, 4, 2, 2))
set.seed(123)

# Simulate data
years <- 2010:2024
n_years <- length(years)

# Number of stretches
n_stretches <- 8
stretch <- 1:8

# Mean density within each year (assumed values)
beta_0 <- 8
density_mu <- rnorm(n_years, beta_0, 2)

# Expand grid to simulate combinations of years and stretches
datum <- expand.grid(year = years, stretch = stretch)

# Sort the dataframe by year (low to high)
datum <- datum[order(datum$year), ]

# Add a fish density column
datum$FishDensity <- rep(NA, n_stretches * n_years)

# Use the density_mu to simulate fish density for each year-stretch combination
for (i in 1:n_years){
  for (j in 1:n_stretches){
    datum$FishDensity[(i - 1) * n_stretches + j] <- rnorm(1, mean = density_mu[i], sd = 0.5)
  }
}

# First plot: Fish Density over Years
plot(datum$year, datum$FishDensity, xlab = "Year", ylab = "Fish Density")

# Add solid filled circles for each year's mean density
points(years, density_mu, pch = 16, col = "blue", cex = 1.5)  # pch = 16 for filled circles, col for color, cex for size

# Add horizontal line for average fish density
abline(h = beta_0, lty = 2, lwd = 4, col = "blue")
# text(x = max(y_vals) * 0.2, y = beta_0 - 0.9, labels = expression(beta[0]), cex = 1.2)

# Second plot: Normal distribution curve on its side
x_vals <- seq(0, 16, length = 100)
y_vals <- dnorm(x_vals, mean = beta_0, sd = 2)

# Rotate the curve by flipping axes
plot(y_vals, x_vals, type = "l", xlab = "Density", ylab = "", ylim = c(0, 16), xlim = c(0, max(y_vals)), axes = FALSE, lwd = 3)
abline(h = beta_0, lty = 2, lwd = 4, col = "blue")
text(x = max(y_vals) * 0.2, y = beta_0 - 0.9, labels = expression(beta[0]), cex = 1.2, col = "blue")
```

The average fish density among all years is $\beta_0$ (**horizontal blue line**). Within each year, there is also an average fish density (**solid blue dots**), and this varies from year to year. The **between-year variation** is normally distributed, which we can visualize with a bell curve that is centered on a mean of $\beta_0$.

#### Residual error

The residual variation is the **within-year variation**. *We can visualize this by drawing little bell curves on the data within each year*. It's the amount of variation we have from stretch-to-stretch within any given year. We assume that it's the same among years. 

#### Total model

So now we have a model for how average fish density varies both among years ($\epsilon_{year}$) and within years ($\epsilon_{residual}$). We have an overall average fish density that can vary each year and also vary within years.

What are the advantages of the model with between-year variation compared to the model with many annual effects?

- The model with year as X-variables has a ton of variables that have to be estimated by the model. 14 betas plus residual error. 
- The model with between-year variation only has 3 parameters! Average fish density, the standard deviation in variation due to year (i.e., between-year variation), and the standard deviation due to error (i.e., residual standard deviation).

### Nomenclature

These two models have names! 

The model with many X-variables and one residual error term is called a '**fixed-effects models**'. Up to this point in class, we have been dealing exclusively with fixed-effects models. This is generally an implicit assumption of any model that someone is talking about (regression, t-test, ANOVA, ANCOVA, whatever) -- when they say they did one of those analyses, they typically mean they built a fixed-effects version of that. The model with the many effects of years is a fixed-effect model; <u>year is treated as a fixed-effect</u>.

The model with the two error terms is called a '**random-effects model**'. Year as an error term that is treated as <u>a random effect</u>. Since the second model does not have any fixed effects, it is called a random-effects model. Random effects models have no X-variables -- they just have random error terms.

If we added in a fixed effect (e.g., flow), this would become a '**mixed-effects model** -- it has both fixed and random effects -- both X-variables and random error terms.

### Fixed or random effects?

How do you determine whether variables should be considered as fixed or random effects? No hard-and-fast rules about this. Up to you!

But, depending on your variable, there are advantages to treating them as random or fixed. Here are some guidelines: 

```{r echo=FALSE}
# Load necessary libraries
library(knitr)

# Create the data frame
data <- t(data.frame(
  `1` = c("Mean of Y", "Variation in Y"),
  `2` = c("Understand effects", "Account for variation"),
  `3` = c("Groups or values are chosen", "Values are a sample of what's possible"),
  `4` = c("Limited groups", "Many, many groups")
))

# Create the table
kable(data, col.names = c("Fixed Effects", "Random Effects"))

```

Fixed effects influence the *mean* of the Y-value within groups, whereas random effects influence the *variation* in Y. This has important implications in your decision.

- If you are interested in understanding what an effect is, you should use a fixed-effects, to understand the effects of the X-variable.
- If you just want to account for variation, you are better off treating a variable as a random effect.

If you chose the treatment levels or treatment groups (i.e., ran an experiment), then better to treat the variable as a fixed-effect. However, if the values are just a sample of what's possible, then maybe treat the variable as a random-effect.

If you have a limited number of groups (e.g., treatment vs. control), then probably treat it as a fixed effect. If you have many, many groups (e.g., years in a long-term study), then better to treat as a random-variable.

- It's hard to estimate a standard deviation among a limited number of groups (e.g., two groups), whereas it's easier to estimate a standard deviation among many group (e.g., 14 years of a study).

But the most important reason is: are you interested in how the *X-variable influences the mean*, or *how it influences variation*.

## Analysis in R

We can briefly look at 'Truth' to see how the data were analyzed.

<br>

## Summary


<br>

## Truth

```{r}
################### 'Truth' #################### 
### Lecture 15: code to simulate data for class

# Set the seed for reproducibility
set.seed(123)

# Simulate data
Years <- rep(1:10, each = 10)

# Between-year error
MeanYear <- rep(rnorm(10, 0, 10), each = 10)

# Within-year error
Error <- rnorm(length(Years), 0, 1)

# Response variable
Abundance <- 50 + Error + MeanYear

# Save the data
datum <- data.frame(Years = Years, Error = Error, MeanYear = MeanYear, Abundance = Abundance)

# Save the data
write.csv(datum, "lecture_15_dataset1.csv", row.names = FALSE)

```

[--go to next lecture--](lecture_16.html)
