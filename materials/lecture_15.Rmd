---
title: "Random Effect Models"
author: "NRES 710"
date: "Last compiled: `r Sys.Date()`"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
    css: styles.css
---

<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
	)
```

```{r echo=FALSE}

#  NRES 710, Random Effect Models
#     University of Nevada, Reno
#     Simple models with random effects

```

## Review of recent material

We have been learning about **multi-variable models** using a generic model:

$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 +... + \beta_nX_n + \epsilon \sim N(0, \sigma)$

In theory, you could have unlimited number of X-variables... but in practice, you might get 8 or 10 before it gets nonsensically complicated.

We can have **interactions** (e.g., $\beta_m * X_1 * X_2$).

- Note: Sometimes other fields refer to this in other ways (e.g., 'effect modification'; where one effect modifies another effect). I mostly see these as 'interactions' in environmental science fields.

We have talked about the **advantages** of multi-variable models: eliminates swamping, eliminates bias associated with collinearity (but inflates variance), and allows us to look for interactions.

When we build models with interactions, we should generally include the 'main effects' as well (i.e., the individual effects of $X_1$ or $X_2$).

## Random-effects Models

My main goal today is to introduce the concept of 'random-effects models'. 

Before we do that, let's take a step back and temporarily remove interactions from our brain. Interactions are confusing! Instead, let's again consider more simple models without interactions -- and focus on our linear model without interactions.

But now let's make things a little more complicated again, in a different way. Let's start with an example.

### Example: Fish Density

Let's say that we have three variables: 

**Y -- fish density**   
**$X_1$ -- flow**   
**$X_2$ -- year**   

We have gone into a particular stream and measured the abundance of fish at ~eight different stretches in the stream. We have measured flow rate, because we are interested in understanding how flow rates influence fish abundance.

But then we repeated the study over a multiple-year period, and we went back measured fish density and flow rate over a 14-year period!

**2010--2024**

**Q:** does anybody have any idea whether fish populations tend to vary from year to year? Yes, they do: fish density can be quite sensitive to rainfall, flow rate, stream disturbance, whatever. We might want to test whether fish density varies from year-to-year.

How would we include 'year' in our model...?

$FishDens = \beta_0 + \beta_12011 + \beta_22012 + ... + \beta_{14}2024 + \epsilon \sim N(0, \sigma)$ -- lots of terms here

Alternatively, we could have written the model like this:

$FishDens = \beta_0 + \beta_1Year + \epsilon \sim N(0, \sigma)$

**Q:** What do you think about this second model -- is it a good model?

No, because it's a categorical variable...?

But it's a number, why can't we treat this as continuous?

The problem with the second model is we are assuming that there is a linear relationship between year and fish density, where fish density will always be going up or always going down through time. This *might* be an okay assumption... if we have a specific hypothesis suggesting that there might be a linear effect of time on fish abundance.

Graphically, our study design and data might look like:

```{r echo=FALSE, fig.height=4, fig.width=6}
# Specify plot margins & set random seed
par(mar = c(4,4,0,0))
set.seed(123)

# Simulate data
years <- 2010:2024
n_years <- length(years)

# Number of stretches
n_stretches <- 8
stretch <- 1:8

# Mean density within each year (assumed values)
density_mu <- rnorm(n_years, 8, 2)

# Expand grid to simulate combinations of years and stretches
datum <- expand.grid(year = years, stretch = stretch)

# Sort the dataframe by year (low to high)
datum <- datum[order(datum$year), ]

# Add a fish density column
datum$FishDensity <- rep(NA, n_stretches * n_years)

# Use the density_mu to simulate fish density for each year-stretch combination
for (i in 1:n_years){
  for (j in 1:n_stretches){
    datum$FishDensity[(i - 1) * n_stretches + j] <- rnorm(1, mean = density_mu[i], sd = 0.5)
  }
}

# Plot Fish Density over Years
plot(datum$year, datum$FishDensity, xlab = "Year", ylab = "Fish Density",
          pch = 4, cex = 1.3, lwd = 2)

```

In this example, the fish abundance bounces all over the place. It's not going to be a linearly increasing or decreasing relationship. Most of the variation is explained by year in a categorical sense.

**Q:** What do you think of the first model?

It's LONG and UGLY. It's not very elegant. And do we really care if... 2013 is different from 2019? Or 2017 is different from 2024?

My point is: **there would be a ton of comparisons we would have to do!** How many pairwise comparisons would a post-hoc test have to do?

A post-hoc test would make `13 + 12 + 11 + 10 + 9 + 8 + 7 + 6 + 5 + 4 + 3 + 2 + 1` interannual comparisons, which equals <u>**`r sum(13:1)` pairwise comparisons**</u>!

What we really want to do by including **Year** in the model is to *account for variation that occurs from year to year* -- and we can do that in a different way than the way described here with the long model.

Here's how we do that. Instead, we would build this model. (I'm not going to include **Flow** just yet)...

$FishDensity = \beta_0 + \epsilon_{year} \sim N(0, \sigma_{year}) + \epsilon_{residual} \sim N(0,  \sigma_{residual}))$

So now we have an equation with **two error terms**: <u>annual yearly error</u> and <u>residual error</u>.

Let's think about what this means...

#### Annual error

The variation in fish density from year to year can be described by some normal probability distribution with some standard deviation, $\sigma_{year}$. We can think about this as: the <u>**between-year variation**</u>.

Graphically, this might be:

```{r echo=FALSE, fig.height=4, fig.width=8}
# Specify plot margins & set random seed
par(mfrow = c(1, 2), mar = c(4, 4, 2, 2))
set.seed(123)

# Simulate data
years <- 2010:2024
n_years <- length(years)

# Number of stretches
n_stretches <- 8
stretch <- 1:8

# Mean density within each year (assumed values)
beta_0 <- 8
density_mu <- rnorm(n_years, beta_0, 2)

# Expand grid to simulate combinations of years and stretches
datum <- expand.grid(year = years, stretch = stretch)

# Sort the dataframe by year (low to high)
datum <- datum[order(datum$year), ]

# Add a fish density column
datum$FishDensity <- rep(NA, n_stretches * n_years)

# Use the density_mu to simulate fish density for each year-stretch combination
for (i in 1:n_years){
  for (j in 1:n_stretches){
    datum$FishDensity[(i - 1) * n_stretches + j] <- rnorm(1, mean = density_mu[i], sd = 0.5)
  }
}

# First plot: Fish Density over Years
plot(datum$year, datum$FishDensity, xlab = "Year", ylab = "Fish Density")

# Add solid filled circles for each year's mean density
points(years, density_mu, pch = 16, col = "blue", cex = 1.5)  # pch = 16 for filled circles, col for color, cex for size

# Add horizontal line for average fish density
abline(h = beta_0, lty = 2, lwd = 4, col = "blue")
# text(x = max(y_vals) * 0.2, y = beta_0 - 0.9, labels = expression(beta[0]), cex = 1.2)

# Second plot: Normal distribution curve on its side
# Define sigma_year based on the standard deviation you used for yearly variation
sigma_year <- 2
x_vals <- seq(0, 16, length = 100)
y_vals <- dnorm(x_vals, mean = beta_0, sd = sigma_year)

# Rotate the curve by flipping axes
plot(y_vals, x_vals, type = "l", xlab = "", ylab = "", ylim = c(0, 16), xlim = c(0, max(y_vals)), axes = FALSE, lwd = 3)
abline(h = beta_0, lty = 2, lwd = 4, col = "blue")
text(x = max(y_vals) * 0.2, y = beta_0 - 0.9, labels = expression(beta[0]), cex = 1.2, col = "blue")

# Add a vertical line starting from the mean and extending up by sigma_year
segments(x0 = 0, y0 = beta_0, x1 = 0, y1 = beta_0 + sigma_year, 
         lty = 2, col = "red", lwd = 3)
text(x = 0, y = beta_0 + sigma_year / 2, labels = expression(sigma[year]), 
     col = "red", pos = 4)

```

The average fish density among all years is $\beta_0$ (**horizontal blue line**). 

*Brian draw long horizontal line to indicate the average among all years*

Within each year, there is also an average fish density (**solid blue dots**), and this varies from year to year.

*Brian draw small horizontal lines to indicate the average density within each year*

The **between-year variation** is normally distributed, which we can visualize with a bell curve that is centered on $\beta_0$, has a mean of 0 (so error is both positive and negative on average), and has a standard deviation of $\sigma_{year}$.

*Brian draw a sideways bell curve indicating the distribution of error in interannual variation*

#### Residual error

The residual variation is the **within-year variation**.

*Brian illustrate this by drawing little bell curves on the data within each year*.

It's the amount of variation we have from stretch-to-stretch within any given year. We assume that it's the same among years.

#### Total model

So now we have a model for how average fish density varies both among years ($\epsilon_{year}$) and within years ($\epsilon_{residual}$). We have an overall average fish density that can vary each year and also vary within years.

What are the <u>**advantages** of the model with between-year variation</u> compared to the model with <u>many annual effects</u>?

- The model with **year as X-variables** has a **ton of variables** that have to be estimated by the model: **14 betas and the residual error**. 
- The model with **between-year variation only has 3 parameters**! Average fish density, the standard deviation in variation due to year (i.e., between-year variation), and the standard deviation due to error (i.e., residual standard deviation). Much more elegant.

### Nomenclature

These two models have names! 

The model with many X-variables and one residual error term is called a '**fixed-effects models**'. Up to this point in class, we have been dealing exclusively with fixed-effects models. This is generally an implicit assumption of any model that someone is talking about (regression, t-test, ANOVA, ANCOVA, whatever) -- when they say they did one of those analyses, they typically mean they built a fixed-effects version of that. The model with the many effects of years is a fixed-effect model; <u>year is treated as a fixed-effect</u>.

The model with the two error terms is called a '**random-effects model**'. Year is an error term that is treated as a <u>random effect</u>. Since the second model does not have any fixed effects, it is called a random-effects model. Random effects models have no X-variables -- they just have random error terms.

If we added in a fixed effect (e.g., flow), this would become a '**mixed-effects model** -- it has both fixed and random effects -- both X-variables and random error terms.

### Fixed or random effects?

How do you determine whether variables should be considered as fixed or random effects? **No hard-and-fast rules about this.** Up to you!

But, depending on your variable, there are advantages to treating them as random or fixed. Here are some guidelines: 

```{r echo=FALSE}
# Load necessary libraries
library(knitr)

# Create the data frame
data <- t(data.frame(
  `One` = c("Mean of Y", "Variation in Y"),
  `Two` = c("Understand effects", "Account for variation"),
  `Three` = c("Groups or values are chosen", "Values are a sample of what's possible"),
  `Four` = c("Limited groups", "Many, many groups")
))

# Create the table
kable(data, col.names = c("Fixed Effects", "Random Effects"))

```

Fixed effects influence the ***mean* of the Y-value** within groups, whereas random effects influence the ***variation* in Y**. This has important implications in your decision.

- If you are interested in understanding what an effect is, you should use a fixed-effects, to understand the effects of the X-variable.
- If you just want to account for variation, you are better off treating a variable as a random effect.

If you chose the treatment levels or treatment groups (i.e., ran an experiment), then it's better to treat the variable as a fixed-effect. However, if the values are just a sample of what's possible, then maybe treat the variable as a random-effect.

If you have a limited number of groups (e.g., treatment vs. control), then probably treat it as a fixed effect. If you have many, many groups (e.g., years in a long-term study), then better to treat as a random-variable.

- It's hard to estimate a standard deviation among a limited number of groups (e.g., two groups), whereas it's easier to estimate a standard deviation among many group (e.g., 14 years of a study).

But the most important consideration is: **are you interested in how the *X-variable influences the mean*, or *how the variable influences variation*?**

## Analysis in R

We can briefly look at 'Truth' to see how the data were analyzed.

### Truth

```{r echo=TRUE}
################### 'Truth' #################### 
### Lecture 15: code to simulate data for class

# Set the seed for reproducibility
set.seed(123)

# Simulate data
Year <- rep(1:10, each = 10)

# Between-year error
MeanYear <- rep(rnorm(10, 0, 10), each = 10)

# Within-year error
Error <- rnorm(length(Year), 0, 1)

# Response variable
Abundance <- 50 + MeanYear + Error

# Save the data
datum <- data.frame(Year = Year, Error = Error, MeanYear = MeanYear, Abundance = Abundance)

# Save the data
write.csv(datum, "lecture_15_dataset1.csv", row.names = FALSE)

```

If we look at truth (the response variable), the average predicted abundance of fish for any given year is 50!

But within any given year, the MeanYear effect has a standard deviation of 10. Since 95% of all data fall within 2 standard deviations, then any given year could be 50 minus 20 or 50 plus 20. So we might expect annual averages to be between 30 and 70.

The term 'Error' is random variation for each observation within each year, which is just usual random error with a standard deviation of 1, so that within each year we might see some random error that is 2 units in either direction.

### Code

Let's look at the data and plot it.

```{r echo=TRUE}
# Read in the data
datum <- read.csv("lecture_15_dataset1.csv")

# Examine it
head(datum, 15)

# Plot it!
plot(Abundance ~ Year, data = datum)

```

When we look at the scatterplot, we can see that the overall average is about 50. For any year, the average varies: it can be increased or decreased by up to 20 fish. And then within the year, the abudnance for any sample can vary by 2-3 in either direction.

We obviously don't want to run a linear model on this. Year does not have a linear effect! Year does have an effect on abundance, but it's not a linear effect.

Let's run a 'lm()' but with Year as a fixed, categorical effect.

```{r echo=TRUE}
# Year as a categorical, fixed effect
results <- lm(Abundance ~ as.factor(Year), data = datum)
summary(results)

```

This code forces Year to be a factor in a fixed-effect model. This is ugly! All it really tells us is whether each year is significantly different than year 1. Not super useful for telling us about relationships between different years...

What if we were to use a post-hoc test? Which will adjust our p-values to 'correct' them...

```{r echo=TRUE}
# Year as a categorical, fixed effect
results2 <- aov(results)
TukeyHSD(results2)

```

These p-values are inflated! Dislike. Buuut... we don't care about this -- because we are actually interested in how year influences *variation*...

I've got some bad news... We can't use 'lm()' anymore!! It's gotten us all the way through two months of class, but we have hit it's limits. It cannot handle random effects! 

We have to move onto a new model called an 'lme()' which is in the 'nlme' package.

```{r echo=TRUE}
# We don't have to install this package - it now comes defaul in R!
library(nlme)

# Examine the help file

```

'nlme' -- non-linear mixed effects

```{r echo=TRUE}
# Examine the help file
help(lme)
```

"A generic function that fits a linear fixed effects model..."

Requires a fixed model -- which we are used to using. 

Data = datum

Random variables! We have to specify what the random variables are now.

Rest of the stuff you can mostly ignore.

```{r echo=TRUE}
# Fit the model
results <- lme(Abundance ~ 1, data = datum, random = ~1|Year)
summary(results)
```

We don't have any fixed effects for our model! No X-variables with fixed effects so far. So we can just specify 1 to tell the model we want to know what the average fish abundance is -- and how it varies in response to the random effect.

What the tilde means is that... we want a random change in the *intercept* due to year. 

Let's discuss the output.

**Linear mixed-effects model fit by REML.** These models are no longer fit by the sum of squared error (SSE). We have now moved into the realm of Maximum Likelihood ('restricted maximum likelihood'; the default). This is ~distinct from SSE models and gives us distinct metrics (AIC, BIC, etc.).

The **coefficients table** that we are used to is the ~same. This model is super simple, so it just gives us the intercept (in this case, fish abundance), including the estimate, SE, t-value, and p-value.

The **random effects** information tells us about our two error terms -- the two sigma terms.

- The intercept (poorly termed) is the **random year effect** ($\epsilon_{year}$). Truth was 10; it estimated 9.53.
- The residual error term is our **residual error** ($\epsilon_{residual}$). Truth was 1; it estimated 0.89.

I wish it labeled the random intercept as "year", but instead it just says (Intercept). Not super useful...

Questions?

## Another Example

### Field experiment

Let's assume we have 15 fields where we are doing a good, old-fashioned, classic agricultural experiment! Within each plot, we have a treatment and a control.

*Brian draw this on the board: field 1, field 2, field 3, ... , field 15: each field with a C and T plot within it that are randomly top or bottom of field*

**Q:** How many samples do we have? **30**: a treatment and a control within each of 15 plots.

We could run this model:

$Biomass = \beta_0 + \beta_1Treatment + \epsilon \sim N(0, \sigma)$

**Q:** What's do normal people call this? **T-test!**

Is there anything we should be worried about for this? What about swamping?

Swamping -- when the effect of one X-variable is masked out by another effect.

Should we be worried about swamping here?

What if things that are different in each field (drainage, sunlight, deer abundance, etc.) influence biomass and thus prevent us from learning about our effect of interest: treatment vs. control. If so, we then we should deal with swamping.

**Q:** How do we normally deal with swamping? We include field in the model! But that requires us to add 14 field effects!! That's impractical and not elegant. Plus we don't care about how fields influence biomass, we mostly want to control for that variation while examining the effect of treatment.

The answer to this problem is to run a **mixed-effects model**:

$Biomass = \beta_0 + \beta_1Treatment + \epsilon_{field} \sim N(0, \sigma_{field}) + \epsilon_{residual} \sim N(0, \sigma_{residual})$

Here, $\epsilon_{field} \sim N(0, \sigma_{field})$ is the *between-field variation*, while $\epsilon_{residual} \sim N(0, \sigma_{residual})$ is the *within-field variation*.

We can partition the treatment effect from random, noise effects among plots. 

The first model has a fancy name: t-test. This second, more complicated model also has a fancy name: <u>**paired t-test**</u>! By including the field effect, we have tied each pair of treatment and control samples within each field together. Without that field effect, we lump all of our samples into giant pools of treatment and control and then compare them. 

This second model is **much more powerful**! We have mathematically eliminated a lot of residual, eliminated swamping, and accounted for residual variation due to field by only adding one parameter: sigma ($\sigma$).

This experimental design also has a fancy term: <u>**randomized block design**</u>. The fields are blocks; we have 'blocked' by field. And within each field, we have randomized treatment assignment. Randomized block designs are very, very powerful!! By eliminating swamping.

We could included field-effect as an X-variable, but it's much better to do it this way!!

Let's look at another dataset.

### Truth

Here is how the data were simulated:

```{r echo=TRUE}
### Dataset 2
# Set the seed for reproducibility
set.seed(123)

# Simulate data
Field <- rep(1:15, each = 2)
Treatment <- rep(0:1, 15)

# Field error
FieldError <- rep(rnorm(15, 0, 5), each = 2)

# Residual error
Error <- rnorm(length(Field), 0, 1)

# Response variable
Biomass <- 25 + 10*Treatment + FieldError + Error

# Save the data
datum <- data.frame(Field = Field, Treatment = Treatment, FieldError = FieldError, Error = Error, Biomass = Biomass)

# Save the data
write.csv(datum, "lecture_15_dataset2.csv", row.names = FALSE)

```

There are 2 samples from 15 fields, Treatment is dummy-coded, Field Error has a standard deviation of 5, residual error has a standard deviation of 1, and Biomass is 25 + 10*treatment + the two error terms.

Truth:

- $\beta_0 = 25$ -- average biomass in control
- $\beta_1 = 10$ -- difference in treatment and control
- $\sigma_{field} = 5$ -- error associated with fields
- $\sigma_{residual} = 1$ -- residual error

### Analysis

Load it into R:

```{r echo=TRUE}
# Load the data
datum <- read.csv("lecture_15_dataset2.csv")

# Examine
head(datum)

# Plot
plot(Biomass ~ Treatment, data = datum)

```

Now remember, our observations within each field are *paired*. Some researchers will connect the dots in each field to indicate this -- or change the pixels to be paired by plot.

Let's run a t-test:

```{r echo=TRUE}
# T-test
results <- lm(Biomass ~ Treatment, data = datum)
summary(results)

```

We are done! Or we could say that. This is sufficient. It is statistically significant. But we are leaving a lot of variation unexplained that we can easily explain with a better model.

Estimate is 9.9, and confidence intervals are about ~3, which overlaps with Truth (10). But that's a pretty big confidence interval. Residual standard error: 6.87. All of the error is being chalked up as residual error. 

There is nothing wrong with this, but our power is low due to the large confidence intervals.

Let's try a LME.

```{r echo=TRUE}
# LME; i.e., paired T-test
results2 <- lme(Biomass ~ Treatment, data = datum, random = ~1|Field)
summary(results2)

```

Examine the field effect between the two analyses. Mostly unchanged!! Zero effect on our effect estimates. But the SE and 95% CI are much reduced now. Confidence intervals went from 3 to 0.6! Big reduction. And our p-values are drastically smaller.

We have now split the error into the two error terms: error due to field, and error due to randomness in observations. Residual error reduced from ~4 to ~1 (Truth), by assigning that error to the field (intercept: 3.8).

No p-value associated with the effect of field! We did not test for this as a fixed-effect, we did not estimate a beta for field, so it doesn't get a statistical test with a p-value. We are mostly interested in effect of treatment, not an effect of field.

If we wanted to test for an effect of field, we could run an F-drop test! We'll do that during the next class.

<br>

## Truth

Again, here is Truth, which we always have at the end of each lecture.

```{r}
################### 'Truth' #################### 
### Lecture 15: code to simulate data for class

# Set the seed for reproducibility
set.seed(123)

# Simulate data
Year <- rep(1:10, each = 10)

# Between-year error
MeanYear <- rep(rnorm(10, 0, 10), each = 10)

# Within-year error
Error <- rnorm(length(Year), 0, 1)

# Response variable
Abundance <- 50 + MeanYear + Error

# Save the data
datum <- data.frame(Year = Year, MeanYear = MeanYear, Error = Error, Abundance = Abundance)

# Save the data
write.csv(datum, "lecture_15_dataset1.csv", row.names = FALSE)


### Dataset 2
# Set the seed for reproducibility
set.seed(123)

# Simulate data
Field <- rep(1:15, each = 2)
Treatment <- rep(0:1, 15)

# Field error
FieldError <- rep(rnorm(15, 0, 5), each = 2)

# Residual error
Error <- rnorm(length(Field), 0, 1)

# Response variable
Biomass <- 25 + 10*Treatment + FieldError + Error

# Save the data
datum <- data.frame(Field = Field, Treatment = Treatment, FieldError = FieldError, Error = Error, Biomass = Biomass)

# Save the data
write.csv(datum, "lecture_15_dataset2.csv", row.names = FALSE)

```

[--go to next lecture--](lecture_16.html)
