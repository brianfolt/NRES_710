---
title: "Random Effect Models"
author: "NRES 710"
date: "Last compiled: `r Sys.Date()`"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
    css: styles.css
---

<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
	)
```

```{r echo=FALSE}

#  NRES 710, Random Effect Models
#     University of Nevada, Reno
#     Multi-variable models with random effects

```

Review quiz for interactions.

## Review of recent material

We have been learning about **multi-variable models** using a generic model:

$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 +... + \beta_nX_n + \epsilon \sim N(0, \sigma)$

<span style="background-color: lightgreen;">$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 +... + \beta_nX_n + \epsilon \sim N(0, \sigma)$</span>

In theory, you could have unlimited number of X-variables... but in practice, you might get 8 or 10 before it gets nonsensically complicated.

We can have **interactions** (e.g., $\beta_m * X_1 * X_2$).

- Note: Sometimes other fields refer to this in other ways (e.g., 'effect modification'; where one effect modifies another effect).

We have talked about the **advantages** of multi-variable models: eliminates swamping, eliminates bias associated with collinearity (but inflates variance), and allows us to look for interactions.

When we build models with interactions, we should generally include the 'main effects' as well (i.e., the individual effects of $X_1$ or $X_2$).

## Moving forward

Let's take a step back and temporarily remove interactions from our brain. Interactions are confusing! Instead, let's again consider more simple models without interactions.


```{r echo=FALSE, fig.height=4, fig.width=4.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Read in the size ~ age + sex data
datum <- read.csv("lecture_12_dataset1.csv")

# Model for females
results <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Female"))

# Model for males
results2 <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Male"))

# Determine the range of the x and y axes
xlim <- range(datum$Age)
ylim <- range(datum$Size)

# Make an empty plot with defined x and y limits
plot(xlim, ylim, type = "n", xlab = "Age", ylab = "Size")

# Add regression lines for females and males
abline(results, col = "red", lwd = 2) # Red line for females
abline(results2, col = "blue", lwd = 2) # Blue line for males

# Optionally, add a legend to distinguish between the two lines
legend("topleft", legend = c("Females", "Males"), col = c("red", "blue"), lwd = 2)

```



## Summary




<br>

## Truth

```{r}
################### 'Truth' #################### 
### Lecture 15: code to simulate data for class

# Set the seed for reproducibility
set.seed(123)

### Dataset 1: age + sex + age*sex
# This is similar to the Age, Sex, and Size data we simulated for in Lecture 12.
# There is no collinearity between Age and Sex, but now there is an interaction
# between Sex and Age.

# First dataset
# X variable
n <- 50
x1 <- c(rep("Female", n), rep("Male", n))
x2 <- runif(n * 2, 1, 10)
dummy <- data.frame(model.matrix(~ x1 - 1))
colnames(dummy) <- c("Female", "Male")

# Simulate error
Error <- rnorm(n * 2, 0, 0.8)

# Predict Y
Response <- 4 + 1.5 * x2 + 2.5 * dummy$Male + 1 * x2 * dummy$Male + Error

# Dataframe
datum <- data.frame(Age = x2, Sex = x1, Male = dummy$Male, Size = Response)

# Save the data
write.csv(datum, "lecture_15_dataset1.csv", row.names = FALSE)

```

[--go to next lecture--](lecture_16.html)
