---
title: "Analysis of Categorical Data"
author: "NRES 710"
date: "Last compiled: `r Sys.Date()`"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
    css: styles.css
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
)
```

```{r echo=FALSE}

#  NRES 710, Analysis of Categorical Data
#     University of Nevada, Reno
#     Testing between two groups

```

## Summary

Up to this point, we have been operating under the idea that we have been examining the relationship between a **continuous Y variable** and a **continuous X variable**.

What if we no longer have a continuous X-variable, but instead have a **categorical X-variable**?

Let's assume that we have our usual continuous Y-variable, but now our X-variable is categorical. And the X-variable is the most simple type of categorical variable: it is binomial (two categories):

- **Continuous Y, Categorical X (binomial)**

**Q:** How would we typically analyze data with a categorical X and a continuous, normally-distributed Y?

**t-Test!** Named after the 'Student t distribution'. 

<br>

## t-Tests

For example, let's say we are interested in testing for body size differences between two sexes of **elephant seal** (*Mirounga leonina*).

![](../pictures/pic_seal.png){width=50%}

Photo: Luke Verburgt

- **$X_1$ = male**
- **$X_2$ = female**
- **$Y$ = size**

Our data might look like this (*Brian plot on board*):

```{r size, echo=FALSE, message=FALSE, warning=FALSE, fig.width=4.5, fig.height=3.5}
### Code for simulating data to be analyzed body size data for two sexes

# Adjust some parameters
par(mar = c(4,4,0,0))

# Set the seed for reproducibility
set.seed(123)

# Simulate the binomial X-variable (sex)
n <- 40
x <- c(rep("Female", n/2), rep("Male", n/2))

# Simulate continuous y-variable data
y <- ifelse(x == "Female",
            rnorm(n/2, mean = 200, sd = 20), #females
            rnorm(n/2, mean = 250, sd = 20)) #males

# Create dataframe
datum <- data.frame(Sex = x, Size = y)

# Plot
stripchart(Size ~ Sex, data = datum, vertical = TRUE, method = "jitter",
           pch = 4, xlab = "Sex", ylab = "Body size (kg)")

# Calculate the means for each sex
means <- tapply(datum$Size, datum$Sex, mean)

# Add horizontal lines at the mean for each sex
for (i in 1:length(means)) {
  segments(i - 0.15, means[i], i + 0.15, means[i], lwd = 2, col = "black")  # Adjust length with i +/- 0.25
}

# Add a vertical line between the two means
x_mid <- mean(1:2)  # Midpoint between group 1 and group 2 on the x-axis
arrows(x_mid, means[1], x_mid, means[2], lwd = 2, col = "black", lty = 1)  # Vertical line connecting the means

```

Both the female and male data are normally distributed. I'm putting a slight 'jitter' on these points, so they can be visualized easier by the naked eye.

We want to know: how much larger are males than females?

*Brian add horizontal lines for means within each point cloud*

Or, what is the difference between the mean of females and males?

*Brian an arrow between the two means*

Our <u>scientific effort</u> might focus on testing the null hypothesis:

**$H_0$: no difference between groups**

- **$\mu_{females} = \mu_{males}$**

From a <u>statistical perspective</u>, we are interested in knowing whether males are significantly different than females in body size, and measuring that effect (if any).

Let's assume that we know truth for this graph:

- **$X_1$ = male = 250 kg**
- **$X_2 = female = 200 kg**
- **$\sigma$ = 20 kg**

The standard deviation for the normal distribution of data for each sex is 20 kg.

The truth is that males are larger than females. Males are about 250 kg, females are about 200 kg, and there is some noise in our data.

Questions?

### Simple t-test in R

Let's make these data in R, and run a t-test:

```{r fig.width=4.5, fig.height=3.5}
### Code for simulating data to be analyzed body size data for two sexes

# Set the seed for reproducibility
set.seed(123)

# Simulate the female data
n <- 20 # sample size
Sex <- rep("Female", n) # female category
Sigma <- 20 # standard deviation of the error (noise) in data
Error <- rnorm(n, mean = 0, sd = Sigma) # simulate the error (noise)
Size <- 200 + Error # make the response variable data
FemaleData <- cbind(Sex, Size)

# Simulate the male data
n <- 20 # sample size
Sex <- rep("Male", n) # female category
Sigma <- 20 # standard deviation of the error (noise) in data
Error <- rnorm(n, mean = 0, sd = Sigma) # simulate the error (noise)
Size <- 250 + Error # make the response variable data
MaleData <- cbind(Sex, Size)

# Bind the data together
datum <- data.frame(rbind(FemaleData, MaleData))

# Make the 'Sex' variable a factor
datum$Sex <- as.factor(datum$Sex)

# Force to 'Size' to numeric and round to 1 digit after the decimal
datum$Size <- round(as.numeric(datum$Size), 1)

# Save the data
write.csv(datum, "lecture_7_seal_data.csv", row.names = FALSE)

```

I am going to show you all how to analyze these data using the 't.test()' function in R!

```{r fig.width=4.5, fig.height=3.5}
### Load in the data
datum <- read.csv("lecture_7_seal_data.csv")

# Examine the data
head(datum)
tail(datum)
str(datum)

# Force 'Sex' to be a factor
datum$Sex <- as.factor(datum$Sex)

# Plot the data to examine it!
plot(Size ~ Sex, data = datum)
# Box and whiskers plot!
# Bold black line = median
# Edges of box: 75% and 25% quartiles
# Bars: 95% limits
# Points: outliers, or 5% of data outside of the 95% intervals

# Analyze this using the 't.test()' function in R:
help(t.test)
results <- t.test(Size ~ Sex, data = datum)

# Examine the summary
summary(results)
# Nothing pops up. Some functions in R don't have summary functions for them because they are so simple!

# Just ask for the object
results

```

All the results come from the object itself!

- t-statistic
- degrees of freedom
- p-value -- testing the null hypothesis that there is no difference in the means between the two groups
- 95% confidence intervals -- on the difference between the two groups
- average size of female group
- average size of male group

**Q:** What is the one thing it doesn't give us that we might want to see??

It does not provide an **estimate** of the size difference between two groups!! This is not helpful (and somewhat silly)!

- It gives us confidence intervals around the estimate of the difference between the two groups, but it does not give us the estimate. Really weird.

We can calculate it by doing some math... subtract the smaller group from the larger group:

```{r fig.width=4.5, fig.height=3.5}
# Difference between groups
248.9 - 202.8

```

This is how much larger the males are than females.

It provides us *confidence limits*, but it does not provide us confidence interval. We can calculate those with:

```{r fig.width=4.5, fig.height=3.5}
# Use confidence limits of difference between groups to calculate confidence intervals
(57.73 - 34.55)/2

# Males are 46.1 kg (+/-11.6; 95% CI) heavier in size than females (p = 1.101e-09).

```

**Q:** Why is the 95% confidence interval that R provided negative?

R calculated this effect as doing females minus males, whereas I calculated it as males minus females. We just need to be savvy to make sure we think about the outputs and ensure that everything makes sense. Use common sense.

**Questions?**

## t-tests are just a regression

One thing that always confused me about learning statistics as an undergraduate and graduate student was that all of the usual analyses (regression, t-test, ANOVA, ANCOVA, etc.) are all taught as 'different tests'. It creates many more boxes and relationships that you have to memorize -- different things to categorize in your brain as different and thus requires more effect to memorize.

Here's a little secret... t-tests are just a **regression**! And can be analyzed as such with 'lm()'!

**Q:** What's our linear model again??

**$Y = \beta_0 + \beta_1 X_1 + \epsilon \sim N(0, \sigma)$**

We can use the **linear model** to analyze the seal size data!

**$Size = \beta_0 + \beta_1 Sex + \epsilon \sim N(0, \sigma)$**

Questions you might ask yourself:

- Why teach this as a whole-new test, when it's the same mathematical formula we have looked at all along?
- And, since sex is not a number, how would this work?

It works through the magical process of **'dummy-coding'**!

**Dummy-coding -- a process to convert categories to 1s and 0s.** For example:

**Sex:**    **Male:**        

Male        1       
Male        1       
Female      0       
Female      0       

In the dummy-coded variable, you can arbitrarily decide which category gets 1s and which category gets 0s. That's up to you. It will only influence whether the effects are estimated as positive or negative, depending on the directionality.

When you create a dummy-coded variable, I recommend making the variable name whatever category was given '1' -- it's the most clear.

Now, we can replace our **sex** variable with **male**:

**$Size = \beta_0 + \beta_1 Male + \epsilon \sim N(0, \sigma)$**

Let's examine how this works mathematically. Anytime a sample is a male, it gives us a 1 here. Anytime a sample is female, it gives us a 0. Using this information, we can simplify this formula and see how it gives us the answer for both sexes.

If we want to know the size of <u>females</u>, we get:

- $Size_{females} = \beta_0 + \epsilon \sim N(0, \sigma)$

If we want to know the size of <u>males</u>, we get:

- $Size_{males} = \beta_0 + \beta_1 + \epsilon \sim N(0, \sigma)$

**Q:** Everyone follow what we just did here?

**Q:** Looking at these equations, what is the meaning of $\beta_0$?

- **Average size of females**
- **Or, more generically, the average Y of the reference group** -- samples that were assigned 0.

**Q:** Looking at these equations, what is the meaning of $\beta_1$?

- **Difference between groups** -- this is what we want to know! The thing that the null hypothesis is testing -- whether this effect is different than zero or not. This is the thing the 't.test()' function didn't even measure for us... (what a shame!)

Let's go back to our original graph.

```{r echo=FALSE, fig.width=4.5, fig.height=3.5}
par(mar = c(4,4,0.5,0))

# Plot
stripchart(Size ~ Sex, data = datum, vertical = TRUE, method = "jitter",
           pch = 4, xlab = "Sex", ylab = "Body size (kg)",
           ylim = c(150, 300))

# Calculate the means for each sex
means <- tapply(datum$Size, datum$Sex, mean)

# Add horizontal lines at the mean for each sex
for (i in 1:length(means)) {
  segments(i - 0.15, means[i], i + 0.15, means[i], lwd = 2, col = "black")  # Adjust length with i +/- 0.25
}

# Add a vertical line between the two means
x_mid <- mean(1:2)  # Midpoint between group 1 and group 2 on the x-axis
arrows(x_mid, means[1], x_mid, means[2], lwd = 2, col = "black", lty = 1)  # Vertical line connecting the means

# Add label for the arrow (beta1)
text(x_mid + 0.1, mean(means), expression(beta[1]), col = "blue", cex = 1.2)

# Add arrow and label for beta0 (mean of the Female group)
beta0_x <- 1  # Position for Female group
arrows(beta0_x, 0, beta0_x, means[1], lwd = 2, col = "red", length = 0.1)  # Arrow from (0, 0) to (1, mean of Female)
text(beta0_x + 0.1, means[1] / 1.2, expression(beta[0]), col = "red", cex = 1.2)  # Adjust label position

```

**Q:** how might we visualize $\beta_0$ and $\beta_1$ on this graph?

If we remember that female is 0, then $\beta_0$ is the y-intercept, or the body size when X = 0.

And then $\beta_1$ is the difference between these two groups.

**Q:** How do we calculate slope again? Rise over run.

Since males are 1, then the 'run' is from 0 to 1 -- which equals 1. The rise is $\beta_1$ divided by 1, which equals $\beta_1$.

<u>So $\beta_1$ is still the slope, but it's also more simply just the difference between the groups</u>.

**Question?**

So: why think about this as a t-test? Perhaps it would be easier to use the linear model that we have studied for three weeks, and gotten to know pretty well -- and instead say that our X-variable is categorical, rather than continuous.

Let's see what this looks like by trying this again in R.

## Analysis of categorical data w/ lm()

```{r fig.width=4.5, fig.height=3.5}
### Code for simulating data to be analyzed body size data for two sexes

# Recall our 'datum' object
head(datum)
tail(datum)

# We need to 'dummy-code' our Sex variable, e.g., as 'Male'
Male <- c(rep(0, n), rep(1, n))

# Add 'Male' to the dataframe
datum <- cbind(datum, Male)

```

Now that we have a dummy-coded variable, we can simulate our data much easier:

```{r fig.width=4.5, fig.height=3.5}
# Aside: we can use dummy-coded sex variable to simulate data much easier
Size <- 200 + 50 * Male + rnorm(n*2, 0, Sigma)
Size
```

Plot the data

```{r fig.width=4.5, fig.height=3.5}
# Plot the data!
plot(Size ~ Male, data = datum)

```

Note: previously R made the X-axis as a categorical (female vs. male), but now it's continuous... R automatically makes graphs depending on it's default interpretation of the data. It reads characters/letters as 'categorical' and numbers and continuous variables. Since we swapped this to be dummy-coded with 1s and 0s, it automatically scaled the X-axis as continuous!

Let's now try to run our t-test using 'lm()':

```{r fig.width=4.5, fig.height=3.5}
# Examine the old t-test results
results
248.9 - 202.8 # effect of being male

# Use lm() to run regression with dummy-coded X-data
results2 <- lm(Size ~ Male, data = datum)
summary(results2)

```

**Q:** What do we see?? 

- Average size of females: 202.8 from t-test, compared to intercept 202.8 from regression
- Effect of being male: 46.1 from t-test, 46.1 from regression

The numbers are the same!

Turns out that when you run 't.test()', it does dummy-coded behind the scenes, and then just runs a 'lm()'. 

```{r fig.width=4.5, fig.height=3.5}
# Confidence intervals
confint(results2)

```

Confidence intervals also ~match up between t-test and lm results.

What if we run a regression but with a categorical X-variable...?

```{r fig.width=4.5, fig.height=3.5}
# Regression with a categorical variable
results3 <- lm(Size ~ Sex, data = datum)
summary(results3)

```

This works also! Because R will automatically turn that categorical X-variable into a dummy-coded continuous variable. In this case, it became 1 for 'SexMale' and 0 for 'SexFemale'. It chose 'Female' as the reference group, and male becomes the group females are being compared to.

**Q:** Why did it choose 'Female' as the reference? Alphabetical order.

So, you don't even have to do the dummy-coding -- R can do that for you! (Although I still like to, to be sure I know what's going on.)

## Conclusions

What this means is: we don't actually have to memorize "Do I do a t-test here, or a regression here, for these data...?" It doesn't matter!

The lightbulb turned on for me with statistics when I realized that we don't have to remember all the different situations for which you would run a t-test, ANOVA, regression, ANCOVA, etc...

Instead, ***every analysis can be done with some form of a linear model***.

For the rest of the semester, we will continue to revisit our tried-and-true formula for the linear model:

**$Y = \beta_0 + \beta_1 X_1 + \epsilon \sim N(0, \sigma)$**

And then we will tweak it just a little bit, week by week, until we use this basic formula for the entire class. It will be a little more complicated each week, but it will be easier to understand because we will build on it slowly, carefully, and hopefully in a logical way!

We will learn how to tweak this formula to analyze our data as the data become more complicated.

**Note:** We can't use our same canned statement for reporting results like we used for linear regression. Next class we will learn how to report our results when X-variable is categorical. And then we will extend the formula to accommodate situations for when we have more than two groups. And then we will keep moving on from there!

[--go to next lecture--](lecture_8.html)
