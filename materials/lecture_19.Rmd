---
title: "Nested Designs (cont.)"
author: "NRES 710"
date: "Last compiled: `r Sys.Date()`"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
    css: styles.css
---

<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
	)
```

```{r echo=FALSE}

#  NRES 710, Nested Designs (cont.)
#     University of Nevada, Reno
#     Split-plot designs

```

So last class we discussed nested designs, and specifically how they can handle pseudoreplication and random effects that are nested within each other.

Today I want to show you the upper limits of what this design can handle.

## Split-plot Design

Let's consider a field experiment with six fields and four variables:

- $Y = biomass$
- $X_1 = bison/no bison$
- $X_2 = fertilizer/no fertilizer$
- $X_3 = water low/medium/high$
- $X_4 = pesticide/no pesticide$

We want this to be a fully-factorial design, where we will cross every single one of these variables with every other variable. 

- E.g., bison, no fertilizer, medium water, no pesticide, etc.
- 2 bison * 2 fertilizer * 3 water * 2 pesticides = 24 plots within each field.
- We need to break up each field into 24 blocks to do all of these combinations.

If we created a tiny block (1/24th of field) and put a bison into it, in two months when we come back to measure the effects, the bison will be dead and all the biomass will be gone.

So an alternative way to approach this would be to split each field in half. Each half will be randomly assigned to have a bison or not.

Now, we'll take every 'bison plot', split those in half, and randomly assign fertilizer or no fertilizer treatment.

Now, we'll take every fertilizer plot, split those in three, and randomly assign water: low, medium, high.

Last, we'll take each water plot, split those in half, and randomly assign pesticide/no pesticide. This is what this would look like:

*Brian draw this on the board: field 1, 2, 3, ... 6.; with design illustrated in different fields with different colored markers*

![](lecture_19_factorial_design.png){width=100%}

How many rows would our Excel file have for this design?

- 6 fields
- 2 bison treatments
- 2 fertilizer treatments
- 3 water treatments
- 2 pesticide treatments
- 6 fields * 2 bison treatments * 2 fertilizer treatments * 3 water treatments * 2 pesticide treatments = <u>**144 biomass measurements**</u>

We will 144 biomass measurements; one each from every pesticide plot.

This is called a 'split-plot design'! Lots of plot splitting happening here.

**Q:** We have a 144 samples to look at the effect of pesticide. Do we have 144 samples to look at the effect of water?

- We have 72 water plots.
- We have 24 fertilizer plots.
- We have 12 bison plots. 

This is important! While we have 144 rows, our sample sizes differ depending the effect we are testing for.

We could simplify the model and just test for an effect of bison, for example. But then we would introduce a lot of pseudoreplication into the design. Therefore, we have to account for these nested random effects to account for that pseudoreplication.

We may want to look at the effect of these all at the same time, because we want to account for swamping.

A scary thing about this design is that the people who do these experiments are interested in interactions. Two-way interactions (e.g., bison, fertilizer; water, fertilizer; fertilizer, pesticides), three-way interactions (e.g., bison, water, pesticide), or four-way interactions...??

But the important part of this design is <u>we need to account for pseudoreplication</u>. We can do that using random effects in our model, that are nested according to the split-plot structure of our experiment.

## Statistical model

$Biomass = \beta_0 + \beta_1Bison + \beta_2Fertilizer + \beta_{3,4}Water + \beta_5Pesticide + epsilon_{field} + epsilon_{field|bison} + epsilon_{field|bison|fertilizer} + epsilon_{field|bison|fertilizer|water}$

- between-field error
- between-bison plot error (*residual error for the bison effect*)
- between-fertilizer plot error (*residual error for the fertilizer effect*)
- between-water plot error (*residual error for the water effect*)
- residual error

**Q:** Why is there no pesticide random effect? Can't include it because each pesticide treatment is only measured once. Maybe if we had a repeated-measures through time for pesticides, then we could have a pesticide effect. But we don't.

**Q:** Are we double-counting fixed effects as random effects? No. The fixed-effect for bison is different from the field|bison random effect. The fixed effect compares all bison plots to all no-bison plots. The random effect is different because *bison is nested within field*, and every field has a different effect, so the bison effects are different within every different field.

Last thing: what if we wanted to test for interactions...?

Let's say we added an interaction term, $\beta_6 * Bison * Fertilizer$. What is the meaning of $\beta_6$?

- When we add $\beta_6$, then $\beta_1$ becomes the effect of bison in unfertilized plots, $\beta_2$ becomes the effect of fertilizer in no-bison plots, and $beta_6$ becomes the difference in the effect of bison between fertilizer and no fertilizer plots.

Let's say we added an interaction term, $\beta_7 * Fertilizer * Pesticide$. What is the meaning of $\beta_7$?

- When we add $\beta_7$, then $\beta_5$ becomes the effect of pesticide in unfertilized plots, and $\beta_5 + \beta_7$ becomes the effect of pesticides in fertilized plots. So $beta_7$ becomes the difference in the effect of pesticide between fertilizer and no fertilizer plots.

Let's say we added another interaction term, $\beta_8 * Fertilizer * Pesticide * Bison$. What is the meaning of $\beta_8$...?!

- We've just changed the meaning of $\beta_6$... before it was the difference between the effect of bison in fertilizer and unfertilized plots. Now that we have the interaction, $\beta_6$ is the <u>difference in the effect of bison between fertilized and unfertilized plots *in no-pesticide plots*</u>.
- Which means, $\beta_8$ is... <u>the difference in the difference in the effect of bison between fertilizer and no fertilizer plots *between pesticide plots and no pesticide plots*</u>. 

Graphically, this might look like...

*Brian draw graphs on the board. Create one panel with bison x fertilizer, with bison positive effect, no fertilizer effect, and a bison x fertilizer interaction. Create another panel with bison x pesticide; bison positive effect, no pesticide effect, but then a negative bison x pesticide effect that is beta8...???*

The take-home message: this gets confusing really quickly... don't do three-way interactions. It can be done, but it's difficult to interpret.

## Split-plot analysis in R

Let's examine how to analyze these data, without interactions, in R. Let's take a look at a dataset:

```{r fig.height=4, fig.width=5}
# Load the data
datum <- read.csv("lecture_19_dataset1.csv")

# Examine it
head(datum, 10)

```

I simulated data with the split-plot design we explored on the board, but I didn't go all the way out to the pesticide treatment because that was ridiculously complex. 

Instead, there are 6 fields, that are split in half (bison, no bison), which are split in half (fertilizer, no fertilizer), which are split into thirds (low, medium, and high water).

We can dive into what the true values are for each effect, but I am not actually super worried about that. Instead, this is a <u>pseudoreplication problem</u>, and we want to make sure that we specify this model properly so that R gets the correct sample size for each effect...!

However, to start, let's assume that we did not recognize the split-plot design of the model. Instead, we fit a simple 'lme()' with only a random-effect of field.

```{r fig.height=4, fig.width=5}
# Linear-mixed effects model w/ random effect of field
library(nlme)
results <- lme(Biomass ~ Bison + Fertilizer + Water, data = datum, random = ~1|Field)
summary(results)
# 72 observations with 6 groups (fields)... does not account for pseudoreplication properly, we need better nesting

```

The summary says we have 72 observations with 6 groups (fields). Take a look at the Degrees of Freedom (DF). We can go into the exact math of degrees of freedom, but it's complicated and I don't pretend to understand it entirely. 

However, a good <u>rule of thumb</u> is that the DF has to be less than the number of samples that should be used. In this case, it is using at least 62 samples when estimating each of these effects. In fact, it's using all 72 ('72 observations' at the bottom).

**Q:** How many samples do we have for 'bison'? In truth, we actually have 12 bison plots: 6 with bison, and 6 without. So the true Degrees of Freedom for bison should be less than 12. But it says 62, which means we are pseudoreplicating with respect to bison, and also fertilizer as well. However, we have not pseudoreplicated with respect to 'water', as all 72 samples are used to test the effect of water.

Part of the definition of a split-plot design is that *you have different sample sizes for different variables*. So we need to remove this pseudoreplication to correctly measure and test these effects.

```{r fig.height=4, fig.width=5}
### Split-plot Design
# Linear-mixed effects model w/ random, nested effects
results2 <- lme(Biomass ~ Bison + Fertilizer + Water, data = datum, random = ~1|Field/Bison/Fertilizer)
summary(results2)
# The random effects are nested with field > bison > fertilizer. Water is not included, because it is our 'ultimate sample' and therefore is the residual error.

```

Let's take a look at the bottom of this. It says we have 72 observations (correct), 24 fertilizer plots (correct), 12 bison plots (correct), and 6 fields (correct)!

If we scroll up and look at the Degrees of Freedom, it's using 46 for water, 11 for fertilizer, and 5 for bison. All of these are less than the number of groups from below, which is good. It is correctly measuring the amount of replication in our model; we are using the correct sample size for each of our particular variables.

We also get random effects for each variable. Reading them from the bottom upward:

- Residual error: error for the water treatment
- Intercept error: variation in biomass due to fertilizer treatment
- Bison %in% field: variation in biomass due to bison effect
- Field: variation in biomass due to field

**Questions?**

You may think this is crazy -- but many of you may have more difficult designs than this!

This design is fully factorial, and balanced, and isn't missing data due to field constraints, lazy technicians, or other weird problems that inevitably arise.

What happens if we add in 'water' as a final, nested random effect?

```{r fig.height=4, fig.width=5}
# Linear-mixed effects model w/ random, nested effects
results3 <- lme(Biomass ~ Bison + Fertilizer + Water, data = datum, random = ~1|Field/Bison/Fertilizer/Water)
summary(results3)
# Water is included when it was not needed to be included.

```

For the most part, it gives us the exact same answer. R is smart enough to recognize that 'water' is the 'ultimate sample' and measures it as such: 72 samples for water in fert. in bison in field.

It doesn't change any of our *t*-values or *p*-values.

It does do something weird, where it tries to partition the water error into error due to water treatment and some additional residual error. I don't know how R tries to do this, but it does. But ultimately it doesn't matter, because it is ultimately using the water-plot to look at the water-effect, with correct sample sizes, which is what we are after.

**Q1:** It is testing whether 'high water' is different from 'low water' and 'medium water'. How do we test if 'low water' is different from 'medium water'? **Re-level and re-run**

**Q2:** What if want to know if water 'as a whole' is significant? **F-drop test!** We run it again without water, and do an F-drop test to compare the two models. And remember, we have to change it to be a 'maximum likelihood model', rather than a REML model.

There is one last thing I want to mention about this model. Again, sometimes you all are going to have models more complicated than this! One of the reasons this model works is that R recognizes that all of the effects are linked to the plots that are nested within each other. But if you start having 'missing data' or other structural anomolies, it can be hard for R to recognize the design of your data and model. There are some other algorithms you can use:

```{r fig.height=4, fig.width=5}
# Help file for lme()
help(lme)
# Notice the optimMethod -- an alternative optimization methods that allows you to change how it recognizes the correct degrees of freedom.

```

Notice the 'optimMethod' -- an alternative optimization methods that helps the model recognize the correct degrees of freedom. Sometimes this can't be fixed... but maybe it doesn't matter. If your sample sizes are large (>100 for treatments), then it shouldn't matter too much because the p-values converge at those larger sample sizes.

## Nested ANCOVA Design

Let's do one more example, using a dataset we have seen before:

- $Y = size$
- $X_1 = age$
- $X_2 = sex$

This is an example that we are pretty comfortable with.

We have previously assumed that every one of our points came from a different individual. But now let's say that every point came from the *same individual*. (We also looked at this with tree sizes, as another example.)

```{r echo=FALSE, fig.height=4, fig.width=5}
# Make a graph to visualize how random intercepts work in mixed-effects models
# with a linear X-variable

# Specify plot margins 
par(mar = c(4,4,1,0))

# Read the data
datum <- read.csv("lecture_19_dataset2.csv")

# Subset to first three rivers for simple example
datum <- subset(datum, datum$Individual %in% c(1,2,3))

# Fit a linear model with a common slope and different intercepts for each river
# Include '0' in the formula to explicitly model different intercepts for each river
fit_common_slope <- lm(Size ~ Age + factor(Individual) - 1, data = datum)

# Set up the plotting area
plot(datum$Age, datum$Size, type = "n", # Set up empty plot
     xlab = "Age",
     ylab = "Body size",
     ylim = c(0, max(datum$Size)))

# Define colors for each individual
colors <- c("blue", "red", "green")

# Loop over each river and add points and the fitted line with a common slope
for (i in 1:3) {
  # Subset data for each river
  ind_data <- subset(datum, Individual == i)
  
  # Plot points for each river
  points(ind_data$Age, ind_data$Size, col = colors[i], pch = 19)
  
  # Calculate the fitted line using the common slope but different intercepts
  intercept <- coef(fit_common_slope)[paste0("factor(Individual)", i)]
  slope <- coef(fit_common_slope)["Age"]
  
  # Add the fitted line to the plot
  abline(a = intercept, b = slope, col = colors[i], lwd = 2)
}

# Add a legend
legend("topleft", legend = paste("Individual", 1:3, c("(F)", "(F)", "(M)")), col = colors, pch = 19, lwd = 2)

```

**Q:** What did we call this particular design, where we measured individuals repeatedly over time. **Repeated measures design**.

Technically, we need to worry about autocorrelation... but there is another thing we need to be worried about that we have not so far.

Our individuals differ in sex: female, female, male, etc.

- **10 individuals**
- **1-10 years old**
- **100 total points**

**Q1:** What do you think our sample size is for looking at the effect of age on sample size?

- A lot of people might say '10', because we only have ten individuals.
- **But it's actually 100 samples for looking at the effect of age on size**.
- The reason is because: for every single point here, **the X-value changed for age**.
- That makes those points <u>*true replicates*</u> for looking at the effect of age on size.

Let's take this to an extreme. Imagine we only had 1 individual, which we measured 10 times. Can we test for the effect of age on size?

- Yes, we can: each observation is a true replicate for looking at the effect of age on size.
- However, this is poor inference, because it's hard to make conclusions beyond that single individual.
- But, it's technically not pseudoreplication. We have true replicates, but we have weak inference.

**Q2:** How many samples do we have to look at the effect of sex? **Ten**. The sex of the individual could not change, which makes each one of those points a *pseudoreplicate* for looking at the effect of sex.

**Q3:** How many samples do we have to look at an interaction -- a sex by age interaction? 10 or 100?

- A little more complicated... the answer is **100 samples**. Each sample is actually a true replicate for looking at the interaction between age and size, *because age did change*.
- **Rule of thumb**: when it comes to interactions, whatever variable has the greatest number of samples -- this becomes the number of samples for your interactions.

Let's learn how to run this analysis in R:

```{r echo=FALSE, fig.height=4, fig.width=5}
### Nested ANCOVA Design
# Read the data
datum <- read.csv("lecture_19_dataset2.csv")

# Examine
head(datum, 15)

```

Briefly looking at our data, we have 10 individuals that we have each measured 10 years, 5 females (0) and 5 males (1). 

```{r echo=FALSE, fig.height=4, fig.width=5}
# Plot it
plot(Size ~ Age, data = datum)

```

You can almost see that there are ten individuals here.

I am not going to worry about autocorrelation, because you know how I feel about it: it ain't worth it!

In terms of nesting structure here, INDIVIDUAL is NOT nested within AGE, because every INDIVIDUAL got every AGE. But individual is nested within SEX, because not every individual got every sex. The design expands from top-to-bottom with: Sex < Individual < Age.

Sex is a fixed effect, so we won't list it as a random effect. All we will do is list individual as random effect, and R will automatically recognize that individual is nested within sex. The key thing here: individual is our *sample* for looking at the effect of sex.

```{r echo=FALSE, fig.height=4, fig.width=5}
# LME with Age and Sex fixed effects and random effect of individual (repeated measures!)
library(nlme)
results <- lme(Size ~ Age + Male, data = datum, random = ~1|Individual)
summary(results)

```

Look at the degrees of freedom: it uses the appropriate less than 10 degrees of freedom for MALE (because we have 10 individuals), and then it uses less than 100 degrees of freedom for the effect of age (because we have 100 measurements at different ages).

Adding in the interaction term:

```{r echo=FALSE, fig.height=4, fig.width=5}
# LME with Age, Sex, and Age*Sex fixed effects and random effect of individual (repeated measures!)
library(nlme)
results2 <- lme(Size ~ Age + Male + Age:Male, data = datum, random = ~1|Individual)
summary(results2)

```

DF for Age:Male = 88 -- so it used all 100 samples to look at the interaction between age and sex.

## Final design

I want to briefly describe one final design that is used by many graduate students. Let's consider these variables:

- $Y = Pain$
- $X_1 = Treatment (drug/placebo)$
- $X_2 = Time$

We have worked with 20 individuals, given them the drug/placebo, and measured their pain through time: 0 hours, 2 hours, 4 hours, 8 hours.

We assume that everyone's pain is the same at the beginning. After 2 hours, the placebo individuals have the same pain, and drugged individuals have less pain. And this continues in this pattern through time... until it ultimately converges together again at 8 hours.

Linear model:

$Pain = \beta_0 + \beta_1Drug + \beta_{2-6}Time + \beta_{7-11}Drug*Time + \epsilon$

We treat 'Time' as categorical, because then we no longer have to assume a linear relationship.

Do we have an interaction? **Absolutely!** We actually don't care about the individual effects of drug or time -- but what we really care about is the interaction effects through time.

**Q:** If we have 10 individuals, what's our sample size for looking at the effect of drug? **10!** -- but again, we don't care about beta1. The interpretation for beta1 is the difference between drug and control at time 0. We already believe this to be ~zero.

Every single point is a true replicate for looking at the effect of time AND every single point is a true replicate for looking at the interaction between drug and time -- which is our focal interest. We want to know how these lines diverge. 

**Q:** If you get a significant interaction, what should you do? **Analyze things separately!** Start to analyze the different time-chunks separately. In each time-chunk, you model becomes:

$Pain = \beta_0 + \beta_1Drug$

In this case, you won't have to include random effects, because you will only have 1 measurement for each individual in each time period, so pseudoreplication will not be an issue. You only need to include the random effect when you use the big, whole model.

So, this model looks scary at first... but if you identify the significant interaction effect, then you can say: "We found a significant interaction between drug and time; therefore, we analyzed the effect of drug separately during each time period."

Many people often jump straight to analyzing the effect of drug separately during each time period, but this is a mistake. Sometimes the noise will not produce significant differences at individual time steps, but the global analysis with all the replicates can identify the significant divergence due to the interaction effect.

<br>

## Truth

Here is the code to create Truth for the two datasets analyzed here today.

```{r}
################### 'Truth' #################### 
### Lecture 19: code to simulate data for class

### Dataset 1
# Simulate split-plot design data w/ three fixed effects: bison, fertilizer, water split across 6 fields
set.seed(123)

# Fields
n_fields <- 6
n_bison <- 2
n_fertilizer <- 2
n_water <- 3
Field <- sort(rep(rep(1:n_fields, n_bison*n_fertilizer*n_water)))

# Bison
Bison <- rep(sort(rep(c(0, 1), n_fertilizer*n_water)), n_fields)

# Fertilizer
Fertilizer <- rep(sort(rep(c(0, 1), n_water)), n_fields)

# Water
Water <- rep(c("Low", "Medium", "High"), n_fields * n_bison * n_fertilizer)
Water <- factor(Water, levels = c("Low", "Medium", "High"))
dummy <- data.frame(model.matrix(~ Water - 1))

# Field error
FieldError <- rep(rnorm(n_fields, 0, 1), each = n_bison * n_fertilizer * n_water)

# Bison error
BisonError <- rep(rnorm(n_bison * n_fields, 0, 2), each = n_fertilizer * n_water)

# Fertilizer error
FertilizerError <- rep(rnorm(n_fertilizer * n_bison * n_fields, 0, 2), each = n_water)

# Residual error
Error <- rnorm(length(Field), 0, 1)

# Response variable -- biomass
Biomass <- 20 + 10*Bison + 10*Fertilizer + 10*dummy$WaterMedium + 10*dummy$WaterHigh + FieldError + BisonError + FertilizerError + Error

# Save the data
datum <- data.frame(Field = Field, Bison = Bison, Fertilizer = Fertilizer, Water = Water, WaterMed = dummy$WaterMedium, WaterHigh = dummy$WaterHigh, FieldError = FieldError, BisonError = BisonError, FertError = FertilizerError, Error = Error, Biomass = Biomass)

# Save the data
write.csv(datum, "lecture_19_dataset1.csv", row.names = FALSE)


### Dataset 2
# Body size as a function of age and sex in multiple individuals
# Set the seed for reproducibility
set.seed(123)

# Individuals
n_individuals <- 10
n_samples <- 10
Individual <- sort(rep(1:n_individuals, n_samples))

# Sex
Male <- c(rep(rep(0, n_samples), n_individuals/2), rep(rep(1, n_samples), n_individuals/2))

# Age
Age <- rep(seq(1, n_samples, 1), n_individuals)

# Error due to individual
IndError <- rep(rnorm(n_individuals, 0, 3), each = n_samples)

# Error within individual (residual error)
# Simulate autocorrelated error - each value using the mean of the previous value
Error <- matrix(NA, length(Age), 1)
Error[1,1] <- rnorm(1, mean = 0, sd = 1)
for (i in 2:length(Age)){
  Error[i,1] <- rnorm(1, mean = Error[i-1, 1], sd = 1)
}

# Response variable - Size
Size <- 10 + 1.5*Age + 5*Male + -1*Male*Age + IndError + Error

# Save the data
datum <- data.frame(Individual = Individual, Age = Age, Male = Male, IndError = IndError, Error = Error, Size = Size)

# Save the data
write.csv(datum, "lecture_19_dataset2.csv", row.names = FALSE)


```

[--go to next lecture--](lecture_20.html)
