---
title: "Multi-Variable Modeling - Interactions"
author: "NRES 710"
date: "Last compiled: `r Sys.Date()`"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
    css: styles.css
---

<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
	)
```

```{r echo=FALSE}

#  NRES 710, Multi-Variable Modeling - Interactions
#     University of Nevada, Reno
#     Modeling interactions between X-variables
#  NOTE: to print as a DOCX, need to comment out the two 'plotly' 3D graphs

```

Any questions before we begin?

**HW:** Please read Odadi et al. 2011 before class on Thursday. The paper got published in Science because they found an interaction! However, they didn't test for an interaction directly... When reading the paper, I want you think: what is their linear model? What are the betas, and how do you interpret them? We'll go over this in class on Thursday, but i want you to try to figure it out first.

Generally when we read scientific articles, we should be able to figure out what their linear model is and interpret what their betas are. If they are reasonably well written, that is! If you can't figure it out, then maybe they did not do a good job describing their stats.

I want you to understand your linear models and other authors linear models because it will make everything easier to understand in the science that you do.

**Review Quiz**

## Interactions

Today we are going to learn about **interactions**!

Last week I suggested that collinearity is critically important to understand, because it is a **nuisance** that we must account for to get unbiased estimates, and so we spent an entire week on the topic. This week, I think interactions are also critically important, so we will again spend a week on this topic. But interactions are actually really **interesting** and can greatly enhance the interest in our research and what we are learning.

What are interactions...? Let's start with an example.

- **Y -- reproductive output**
- **$X_1$ -- Placebo / Pill** -- a binomial variable describing groups in a clinical trials of whether the 'estrogen/progesterone/birth-control pill'.

**Q:** What is the effect of The Pill on reproductive output? ....does it cause reproduction to be ~0? <u>*It depends....*</u>

- **$X_2$ -- Sex**

The Pill reduces reproductive output of females to zero, but it does not influence the reproductive output of males. **This is an interaction!**

**interactions -- the effect (beta) of one X-variable depends on the value of another X-variable**

Sometimes people will implicitly assume there is an interaction, and separate out their data to analyze the effect of the pill on two groups separately: males and females. <u>This is okay</u>! **BUT...**

1. You are implicitly assuming there is an interaction, and,
2. If there is a difference, you cannot say the difference is statistically significant or not, because you did not test that.

Let's say that we find that the effect of the pill on males is some very small number close to ~0 (e.g., beta = 0.001), and then we get a large beta for the effect of the pill on females. How would we know that these effects are statistically different? Well, we can't know that if we analyzed these effects separately.

If you want to know if the effect is different between two groups (sexes, species, etc.), you need to test for an interaction **in your linear model**. 

## Example

Let's again consider an example we have seen before, that I like, because it is easy...

- **Y -- Size**
- **$X_1$ -- Age**
- **$X_2$ -- Sex**

Previously we ran this multi-variable model: $Size = \beta_0 + \beta_1 * Age + \beta_2 * Sex + \epsilon \sim N(0, \sigma)$

This model is implicitly assuming that lines for each group are **parallel** -- they have the same slope:

```{r echo=FALSE, fig.height=4, fig.width=4.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Read in the size ~ age + sex data
datum <- read.csv("lecture_12_dataset1.csv")

# Model for females
results <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Female"))

# Model for males
results2 <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Male"))

# Determine the range of the x and y axes
xlim <- range(datum$Age)
ylim <- range(datum$Size)

# Make an empty plot with defined x and y limits
plot(xlim, ylim, type = "n", xlab = "Age", ylab = "Size")

# Add regression lines for females and males
abline(results, col = "red", lwd = 2) # Red line for females
abline(results2, col = "blue", lwd = 2) # Blue line for males

# Optionally, add a legend to distinguish between the two lines
legend("topleft", legend = c("Females", "Males"), col = c("red", "blue"), lwd = 2)

```

The effect of sex is the same for all ages, and the effect of age is the same for both sexes. <u>**This is an assumption that we have made**.</u> But this is *not* an assumption that we *have* to make...

What if we assume that these effects are different...?

```{r echo=FALSE, fig.height=4, fig.width=4.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Read in the size ~ age + sex + age*sex data
datum <- read.csv("lecture_13_dataset1.csv")

# Model for females
results <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Female"))

# Model for males
results2 <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Male"))

# Determine the range of the x and y axes
xlim <- c(0, max(datum$Age))
ylim <- c(0, max(datum$Size))

# Make an empty plot with defined x and y limits
plot(xlim, ylim, type = "n", xlab = "Age", ylab = "Size")

# Add regression lines for females and males
abline(results, col = "red", lwd = 2) # Red line for females
abline(results2, col = "blue", lwd = 2) # Blue line for males

# Optionally, add a legend to distinguish between the two lines
legend("topleft", legend = c("Females", "Males"), col = c("red", "blue"), lwd = 2)

```

All individuals start out at about the same size, but then <u>males grow faster and reach larger sizes at earlier ages than females</u>.

This is an interaction! <u>**The effect of age depends on sex**</u>. We get a different beta (slope) for the age-size relationship for males then we do for females.

Another interesting thing I want to point out is that for interactions, perspective does not matter:

- The **effect of age** depends on **sex**.
- The **effect of sex**, or the difference in size between sexes, also depends on **age**.

For example:

```{r echo=FALSE, fig.height=4, fig.width=4.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Make an empty plot with defined x and y limits
plot(xlim, ylim, type = "n", xlab = "Age", ylab = "Size")

# Add regression lines for females and males
abline(results, col = "red", lwd = 2) # Red line for females
abline(results2, col = "blue", lwd = 2) # Blue line for males

# Optionally, add a legend to distinguish between the two lines
legend("topleft", legend = c("Females", "Males"), col = c("red", "blue"), lwd = 2)

# Define ages for vertical lines (e.g., young age = 5, old age = 20)
young_age <- 3
old_age <- 8

# Add vertical lines at the chosen ages
segments(young_age, 8.5, young_age, 14, col = "black", lty = 2, lwd = 2)
segments(old_age, 16, old_age, 26.5, col = "black", lty = 2, lwd = 2)

```

<u>At a young age, the difference between sexes is small</u>; <u>at an older age, the difference is larger</u>.

You can take whatever perspective is most valid for you, and it depends on the questions you are asking. A little open to interpretation.

## Interactions in the linear model

How do we add an interaction into the linear model?

1) Like I said before, you could just do this analysis separately: one regression for males, one for females. But you will get two slope estimates and you won't know if they are statistically significant.
2) Add in another effect to our linear model ($\beta_3$) and then multiply the two X-variables by eachother (times eachother).

So now the linear model looks like:

$Size = \beta_0 + \beta_1Size + \beta_2Sex + \beta_3 Age*Sex  + \epsilon \sim N(0, \sigma)$

As we have done before, let's break this down for what the interaction does for both males and females! Let's consider females as the reference group (0) and males as the treatment group (1)

**Female = 0**

**Male = 1**

$Size(female) = \beta_0 + \beta_1Age + \beta_2 * 0 + \beta_3 * 0 + \epsilon \sim N(0, \sigma)$

*Brian cancel out the effects of $\beta2$ and $\beta3$*

The effect of beta2 disappears, and so does the effect of beta3! So the female line is simply: the intercept (beta0) plus the effect of age (beta1)!

```{r echo=FALSE, fig.height=4, fig.width=4.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Make an empty plot with defined x and y limits
plot(xlim, ylim, type = "n", xlab = "Age", ylab = "Size")

# Add regression lines for females and males
abline(results, col = "red", lwd = 2) # Red line for females
abline(results2, col = "blue", lwd = 2) # Blue line for males

# Optionally, add a legend to distinguish between the two lines
legend("topleft", legend = c("Females", "Males"), col = c("red", "blue"), lwd = 2)

# Define ages for vertical lines (e.g., young age = 5, old age = 20)
young_age <- 3
old_age <- 8

# Add vertical lines at the chosen ages
segments(young_age, 8.5, young_age, 14, col = "black", lty = 2, lwd = 2)
segments(old_age, 16, old_age, 26.5, col = "black", lty = 2, lwd = 2)

# Store some effects from the female and male models
# Extract the coefficients from the female model
coeffs <- coef(results)
beta0 <- coeffs[1]
beta1 <- coeffs[2]

# Add vertical line from origin to regression line for beta0
segments(x0 = 0, y0 = 0, x1 = 0, y1 = beta0, col = "Orange", lty = 2, lwd = 3)

# Add labels for Beta0 (intercept) and Beta1 (slope)
text(x = 0, y = beta0/3, labels = expression(beta[0]), pos = 4, col = "Orange", font = 2, cex = 2)
text(x = max(datum$Age)/2, y = beta0 + beta1 * (max(datum$Age)/5), labels = expression(beta[1]), pos = 4, col = "Red", font = 2, cex = 2)

# Define points to illustrate rise over run for slope beta1
x1 <- 2
x2 <- 5
y1 <- beta0 + beta1 * x1
y2 <- beta0 + beta1 * x2

# Add horizontal and vertical lines to represent rise over run
segments(x0 = x1, y0 = y1, x1 = x2, y1 = y1, col = "red", lty = 2, lwd = 3) # Run
segments(x0 = x2, y0 = y1, x1 = x2, y1 = y2, col = "red", lty = 2, lwd = 3) # Rise

```

Now let's examine this for males.

Intercept, plus effect of age, then sex gets a 1, and then sex * age gets a 1...

We can rearrange this to make it less messy:

$Size(male) = \beta_0 + \beta_1Age + \beta_2*1 + \beta_3Age*1 + \epsilon \sim N(0, \sigma)$, or:

$Size(male) = \beta_0 + \beta_1Age + \beta_2*1 + \beta_3Age*1 + \epsilon \sim N(0, \sigma)$

$Size(male) = (\beta_0 + \beta_2) + (\beta_1 + \beta_3)Age + \epsilon \sim N(0, \sigma)$

So now our new intercept for males is $\beta0 + \beta2$, and our our new slope is $\beta1 + \beta3$:

```{r echo=FALSE, fig.height=4, fig.width=4.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Make an empty plot with defined x and y limits
plot(xlim, ylim, type = "n", xlab = "Age", ylab = "Size")

# Add regression lines for females and males
abline(results, col = "red", lwd = 2) # Red line for females
abline(results2, col = "blue", lwd = 2) # Blue line for males

# Optionally, add a legend to distinguish between the two lines
legend("topleft", legend = c("Females", "Males"), col = c("red", "blue"), lwd = 2)

# Define ages for vertical lines (e.g., young age = 5, old age = 20)
young_age <- 3
old_age <- 8

# Add vertical lines at the chosen ages
segments(young_age, 8.5, young_age, 14, col = "black", lty = 2, lwd = 2)
segments(old_age, 16, old_age, 26.5, col = "black", lty = 2, lwd = 2)

# Females: visualize female parameters on graph
# Extract the coefficients from the female model
coeffs <- coef(results)
beta0 <- coeffs[1]
beta1 <- coeffs[2]

# Add vertical line from origin to regression line for beta0
segments(x0 = 0, y0 = 0, x1 = 0, y1 = beta0, col = "Orange", lty = 2, lwd = 3)

# Add labels for Beta0 (intercept) and Beta1 (slope)
text(x = 0, y = beta0/3, labels = expression(beta[0]), pos = 4, col = "Orange", font = 2, cex = 2)
text(x = max(datum$Age)/2, y = beta0 + beta1 * (max(datum$Age)/5), labels = expression(beta[1]), pos = 4, col = "Red", font = 2, cex = 2)

# Define points to illustrate rise over run for slope beta1
x1 <- 2
x2 <- 5
y1 <- beta0 + beta1 * x1
y2 <- beta0 + beta1 * x2

# Add horizontal and vertical lines to represent rise over run
segments(x0 = x1, y0 = y1, x1 = x2, y1 = y1, col = "red", lty = 2, lwd = 3) # Run
segments(x0 = x2, y0 = y1, x1 = x2, y1 = y2, col = "red", lty = 2, lwd = 3) # Rise


# Males: Visualize male parameters on graph
# Extract the coefficients from the female model
coeffs <- coef(results2)
beta0male <- coeffs[1]
beta1 <- coeffs[2]

# Add vertical line from origin to regression line for beta0
segments(x0 = 0, y0 = beta0, x1 = 0, y1 = beta0male, col = "brown", lty = 1, lwd = 3)

# Add labels for Beta0 (intercept) and Beta1 (slope)
text(x = 0, y = beta0male, labels = expression(beta[2]), pos = 4, col = "brown", font = 2, cex = 1.5)
text(x = max(datum$Age)/2.5, y = beta0male + beta1 * (max(datum$Age)/3), labels = expression(beta[1] + beta[3]), pos = 4, col = "green", font = 2, cex = 1.5)

# Define points to illustrate rise over run for slope beta1
x1 <- 4
x2 <- 6
y1 <- beta0male + beta1 * x1
y2 <- beta0male + beta1 * x2

# Add horizontal and vertical lines to represent rise over run
segments(x0 = x1, y0 = y1, x1 = x2, y1 = y1, col = "green", lty = 2, lwd = 3) # Run
segments(x0 = x2, y0 = y1, x1 = x2, y1 = y2, col = "green", lty = 2, lwd = 3) # Rise

```

**Q:** Questions?

Let's break this down and look at the meaning of our individual betas.

Before, $\beta_0$ was the intercept of the female line. This remains unchanged; <u>it is the average Y-value **when all other X-variables are zero**</u>! **The average size of females at the age of zero.**

What is $\beta_2$? <u>**The difference in average size of males and females at age = 0.**</u> Graphically, it's that little distance between males and females at age zero.

**Q:** Review question... <u>If we didn't have the interaction</u>, what would $beta_2$ mean? It would simply be the **difference between males and females at all ages**. With the interaction, it restricts this meaning of this parameter to be at zero for the continuous X-variable, <u>because the effect of being male now changes along the continuous X-variable</u>.

What is $\beta_1$? Before it was the effect of age for all individuals, everybody. Now, it is **the effect of age for females.**

<u>Finally, what is $\beta_3$? This is a little more ***tricky***.</u> It is not the effect of age for males. Instead, <u>it is the *difference* in the effect of age between females and males</u>. The difference in the slope.

<u>So, if we want to know if the slopes are different for males and females, that's what $\beta_3$ tells us!</u>

If $\beta_3$ is zero, what does that mean? ...There is no interaction, there is no differnece in the slopes for females and males, and the slopes are the same.

For this reason, I argue that if you get a non-significant interaction, it would be best to take this parameter out of the model. The main reason is that:

- By leaving the insignificant interaction in, it changes the meaning of the other betas, and makes this more confusing and messy. If it's not significant, you are justified to **take it out** for your results to be more simple and clear.

Alternatively, you might want to leave it in:

- If you have a small sample size and your interaction beta is large, maybe you might leave it in to report it so that maybe will encourage future researchers to collect more data to potentially test for that effect.
- And if you are specifically testing for this interaction, you would **not want to take it out** -- you would want to leave it in to **report it**! <u>"I hypothesized this to be different, but I didn't find support for that -- but here are the results."</u>

The reason why we test for interactions this way, rather than having a separate slope for males, is that:

- If $\beta_3 = 0$, there is no interaction, and the slopes are the same.
- If $\beta_3 \neq 0$, there is an interaaction, the slopes are different, and this equation accompanies that.

**Q:** As I've drawn this here, would $\beta3$ be positive or negative?

Positive! It causes the slope to increase for males relative to females.

Alternatively, <u>$\beta_3$ could be negative</u>. Let's assume we have a positive $\beta_2$ and a negative $\beta_3$:

```{r echo=FALSE, fig.height=4, fig.width=4.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Read in the size ~ age + sex + age*sex data with a negative interaction effect
datum <- read.csv("lecture_13_dataset2.csv")

# Model for females
results <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Female"))

# Model for males
results2 <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Male"))

# Determine the range of the x and y axes
xlim <- c(0, max(datum$Age))
ylim <- c(0, max(datum$Size))

# Make an empty plot with defined x and y limits
plot(xlim, ylim, type = "n", xlab = "Age", ylab = "Size")

# Add regression lines for females and males
abline(results, col = "red", lwd = 2) # Red line for females
abline(results2, col = "blue", lwd = 2) # Blue line for males

# Optionally, add a legend to distinguish between the two lines
legend("topleft", legend = c("Females", "Males"), col = c("red", "blue"), lwd = 2)

```

In this case, the negative $\beta3$ causes the slope of the male line to be flat -- or zero. If we did the male analysis separate from females, there would probably be no statistically significant effect here. However, the full model with the interaction effect **would recover a statistically significant interaction term**! In this case, $\beta3$ would probably be pretty close to $\beta1$, but just negative as such, canceling eachother out.

There is no hard-and-fast rule for (1) whether to test for an interaction or not, or (2) whether to leave a non-significant interaction term in your model or not. It depends on your **needs**. The purpose of statistics is not to tell you the answer. Rather, it is a tool to support or reject hypotheses that you have about the world. Statistics are a way to test hypotheses. You should only test for an interaction if you have a specific hypothesis suggesting that there might be an interaction effect.

Note: this is a simple interaction model, a two-way interaction. What if we had another X-variable that might also interact with age and sex... e.g., Mother Size? That will get very complicated, very fast. And it will be very difficult to explain what those effects mean. So while you could start testing for three- or four-way interactions, it will be impossible to explain what those results mean! Point is: think about what your interactions are before putting them in the model.

## Reporting results

Without interactions, we learned to report results with <u>two sentences</u>:

- For the continuous variable, age: "For each unit increase in age, we observed [beta] increase in size."
- For the categorical variable, sex: "We found that males were [beta] more/less than females."

We now need to modify the sentence for the continuous variable. Our sentences will be:

- Continuous variable: "We found that, <u>*in females*</u>, for each one unit increase in age, we observed a [beta] increase in size."
- Categorical variable: "We found that males were [beta] units larger than males <u>*at age zero*</u>."
- Interaction variable sentence is a bit trickier: "We found a significant interaction, such that for males, the difference in the age-size relationship was [beta] [Y-units per X-units] ([P-value])."

We have to add in these <u>*qualifiers*</u> when we have an interaction, which makes it clunkier.

This third sentence for the interaction ~works, but it is clunky, ugly, and not exactly elegant!

Before I said that I didn't like analyzing males separate from females because you couldn't do a test to demonstrate that the male and female slopes were significantly different. However, describing the interaction effect is one situation where I will support (1) splitting your data, (2) re-running the simple model (e.g., males-only), and then (3) reporting that more simple beta to your readers. You can justify this approach by saying:

- "We found a significant interaction between males and females ($\beta_3$ = [...]; [p-value]). Therefore, we analyzed males and females separately. We found that, for females, for each one unit increase in age, we observed a [beta] unit (95% CI) increase in size (P-value). We also found that for males, for each one unit increase in age, we observed a [beta] increase in size."

This all works when you have a categorical variable in the model. If you have a categorical variable in the model, it is often easiest to figure a way to break them apart.

If you have two continuous variables, it becomes more confusing, and I recommend using a graph -- specifically a three-dimensional one.

*Brian pull up 3D plots on HTML page*

Here, size is the Y-variable, elevation and latitude are the X-variables. There is no interaction between elevation and latitude.

```{r echo=FALSE, fig.height=6, fig.width=8.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Read in size ~ elevation + latitude
datum <- read.csv("lecture_13_dataset3.csv")

# Load necessary library
library(plotly)

# Fit a linear model to predict Size based on Latitude and Elevation
model <- lm(Size ~ Latitude + Elevation, data = datum)

# Create a grid of points for plotting the plane
grid <- expand.grid(
  Latitude = seq(min(datum$Latitude), max(datum$Latitude), length.out = 30),
  Elevation = seq(min(datum$Elevation), max(datum$Elevation), length.out = 30)
)

# Predict Size values on the grid
grid$Size <- predict(model, newdata = grid)

# Create a 3D plot with plotly
plot_ly(
  x = ~grid$Latitude,
  y = ~grid$Elevation,
  z = ~grid$Size,
  colorscale = "Viridis",
  type = "mesh3d",
  intensity = ~grid$Size,  # Color the plane based on Size values
  showscale = FALSE,        # Show the color scale
  opacity = 0.8            # Set plane opacity
) %>%
  layout(
    scene = list(
      camera = list(eye = list(x = -1.6, y = -1.6, z = 0.3)),
      xaxis = list(title = 'Latitude', range = c(0, max(datum$Latitude))),
      yaxis = list(title = 'Elevation', range = c(0, max(datum$Elevation))),
      zaxis = list(title = 'Size')
    ),
    width = 666,   # Set the desired width in pixels
    height = 500   # You can adjust the height as well if needed
  )

```

**Q:** How do we know there is no interaction between the X-variables?

- It's a flat plane
- The lines are parallel!

An interaction between latitude and elevation would look like this:

```{r echo=FALSE, fig.height=5, fig.width=8.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Read in size ~ elevation + latitude
datum <- read.csv("lecture_13_dataset4.csv")

# Load necessary library
library(plotly)

# Fit a linear model to predict Size based on Latitude and Elevation
model <- lm(Size ~ Latitude + Elevation + Latitude*Elevation, data = datum)

# Create a grid of points for plotting the plane
grid <- expand.grid(
  Latitude = seq(min(datum$Latitude), max(datum$Latitude), length.out = 30),
  Elevation = seq(min(datum$Elevation), max(datum$Elevation), length.out = 30)
)

# Predict Size values on the grid
grid$Size <- predict(model, newdata = grid)

# Create a 3D plot with plotly
plot_ly(
  x = ~grid$Latitude,
  y = ~grid$Elevation,
  z = ~grid$Size,
  colorscale = "Viridis",
  type = "mesh3d",
  intensity = ~grid$Size,  # Color the plane based on Size values
  showscale = FALSE,       # Show the color scale
  opacity = 0.8            # Set plane opacity
) %>%
  layout(
    scene = list(
      camera = list(eye = list(x = -1.6, y = -1.6, z = 0.3)),
      aspectmode = "manual",
      aspectratio = list(x=1, y=1, z=1),
      xaxis = list(title = 'Latitude', range = c(min(datum$Latitude), max(datum$Latitude))),
      yaxis = list(title = 'Elevation', range = c(min(datum$Elevation), max(datum$Elevation))),
      zaxis = list(title = 'Size', range = c(min(datum$Size), max(datum$Size)))
    ),
    selectdirection = "v",
    width = 666,  # Set the desired width in pixels
    height = 500   # You can adjust the height as well if needed
  )

```

The plane is not flat! The effect of latitude is *steeper* at low elevations than it is at high elevations. Conversely, the effect of elevation is *steeper* at low latitudes than it is at high latitudes. You might describe these features qualitatively to your readers.

## Analysis in R

Let's analyze some data that we simulated in R. Here is truth:

```{r}
### Dataset 1: age + sex + age*sex
# This is similar to the Age, Sex, and Size data we simulated for in Lecture 12.
# There is no collinearity between Age and Sex, but now there is an interaction
# between Sex and Age.

# First dataset
# X variable
n <- 50
x1 <- c(rep("Female", n), rep("Male", n))
x2 <- runif(n * 2, 1, 10)
dummy <- data.frame(model.matrix(~ x1 - 1))
colnames(dummy) <- c("Female", "Male")

# Simulate error
Error <- rnorm(n * 2, 0, 0.8)

# Predict Y
Response <- 1 + 2 * x2 + 4 * dummy$Male + 1 * x2 * dummy$Male + Error

# Dataframe
datum <- data.frame(Age = x2, Sex = x1, Male = dummy$Male, Size = Response)

# These data were saved as 'lecture_13_dataset.csv'

```

What is **Truth** for these data -- and what do the effects mean?

- **beta0 = 1** -- average size of females at age 0
- **beta1 = 2** -- effect of age in females
- **beta2 = 4** -- difference between males and females at age 0
- **beta3 = 1** -- difference in slope between males and females

**Q:** Given these numbers, what is the slope of the age-size relationship for males?

3! $\beta_1 + \beta_3$ = 2 + 1 = 3

Let's do the analysis in R.

```{r fig.width = 5}
# Load the data
datum <- read.csv("lecture_13_dataset.csv")
head(datum)

# Plot the data
plot(Size ~ Age, data = datum)

```

If your data look like this, you likely have an interaction!!

```{r fig.width = 5}
# Fit a lm() without the interaction
results <- lm(Size ~ Age + Sex, data = datum)
summary(results)
```

Is there any indication that we missed an interaction? Nope. There are symptoms of collinearity, but there aren't symptoms of interactions.

The <u>effect of age is ~2.5</u>, which is basically the average of the male and female slopes. Which, isn't too bad.

The difference between males and females is ~9.5. However, for our data, the difference between males and females at young ages is smaller, and at old ages is larger. So this is again an average. It's not bad... But we are missing an opportunity to learn about interesting information in our data.

```{r fig.width = 5}
# Fit a lm() with the interaction
results <- lm(Size ~ Age + Sex + Age:Sex, data = datum)
summary(results)
```

The age-size relationship for females is 2! The effect of being male is 3.79, and truth (4) is contained within the confidence interval. The difference in slope between male line and female line is 1.03 -- very close to truth.

You could report your results using these results. "We found that for each one year increase in age, females had a 2.00 cm increase in size." -- with p-values and confidence intervals. Same sentence as before, but we have the qualifiers *in females*. "We found that males were 3.79 [CI] cm larger at age 0 [P]." Again, need the qualifier of male. "We found that the slope of the age-size relationship was 1.03 [CI] cm per year larger in males than females [P]."

Or, you could state that you found a significant interaction between the X-variables: "We found a significant age-sex relationship (P < 2e-16); therefore, we analyzed males and females separately."

How do you do that?

```{r fig.width = 5}
# Fit a lm() for males only
resultsMale <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Male"))
summary(resultsMale)
```

When we run that, we get a slope of 3.03 -- which is equal to the sum of the age effect and the age:sex effect from our interaction model. Just adding them together and getting the same result -- but now we have a confidence interval for our simple slope also:

```{r fig.width = 5}
# Confint for males
confint(resultsMale)
```

And we can do this for females also:

```{r fig.width = 5}
# Fit a lm() for females only
resultsFemale <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Female"))
summary(resultsFemale)
confint(resultsFemale)
```

This will slightly influence your confidence intervals, but that's okay.

## Using the * to build the model

Here's some other stuff to know about:

```{r fig.width = 5}
# Fit a lm() with interactions using a *
results <- lm(Size ~ Age*Sex, data = datum)
summary(results)
```

The * will fit all possible combinations of effects between the variables included in the term. This can be a useful shortcut. BUT, it might be a little sloppy and might not clearly convey what is going on. I prefer to be <u>explicit</u> and spell out the terms individually.

## Summary

Interactions are not that complicated! An interaction means that your betas for a given variable change depending on another variable. If you have groups, the beta depends on the group; different betas for each group. If you have interactions between continuous variables, the effect of each variable depends on the other variable, and you can change perspective depending on either variable (e.g., the second 3-D plot above).

I encourage you to always tests for interactions, rather than splitting your data and running separate tests for your groups. That way you can get an explicit test that your data are different, according to that interaction term! However, once you find that significant interaction term, I then *do* encourage you to break those groups apart, so that the explanation is much more simple.

<br>

## Truth

```{r}
################### 'Truth' ####################
### Lecture 13: code to simulate data for class

# Set the seed for reproducibility
set.seed(123)

### Dataset 1: age + sex + age*sex
# This is similar to the Age, Sex, and Size data we simulated for in Lecture 12.
# There is no collinearity between Age and Sex, but now there is an interaction
# between Sex and Age.

# First dataset
# X variable
n <- 50
x1 <- c(rep("Female", n), rep("Male", n))
x2 <- runif(n * 2, 1, 10)
dummy <- data.frame(model.matrix(~ x1 - 1))
colnames(dummy) <- c("Female", "Male")

# Simulate error
Error <- rnorm(n * 2, 0, 0.8)

# Predict Y
Response <- 4 + 1.5 * x2 + 2.5 * dummy$Male + 1 * x2 * dummy$Male + Error

# Dataframe
datum <- data.frame(Age = x2, Sex = x1, Male = dummy$Male, Size = Response)

# Save the data
write.csv(datum, "lecture_13_dataset1.csv", row.names = FALSE)


### Dataset 2: age + sex + age*sex, with a negative interaction effect
# First dataset
# X variable
n <- 50
x1 <- c(rep("Female", n), rep("Male", n))
x2 <- runif(n * 2, 1, 10)
dummy <- data.frame(model.matrix(~ x1 - 1))
colnames(dummy) <- c("Female", "Male")

# Simulate error
Error <- rnorm(n * 2, 0, 0.8)

# Predict Y
Response <- 4 + 1.5 * x2 + 5 * dummy$Male + -1.5 * x2 * dummy$Male + Error

# Dataframe
datum <- data.frame(Age = x2, Sex = x1, Male = dummy$Male, Size = Response)

# Save the data
write.csv(datum, "lecture_13_dataset2.csv", row.names = FALSE)


### Dataset 3:
# Set the seed for reproducibility
set.seed(123)

# Sample size
n <- 90

# Simulate X-variables
# Continuous variables: Latitude and Elevation
Latitude <- runif(n, 0, 1) * 30
Elevation <- runif(n, 0, 1) * 30

# Response variable: Size
Size <- 3.33*Latitude + 1.6*Elevation

# Create dataframe
datum <- data.frame(Latitude=Latitude, Elevation=Elevation, Size=Size)

# Save the CSV file
write.csv(datum, "lecture_13_dataset3.csv")


### Dataset 4: size ~ latitude + elevation + latitude*elevation
# Set the seed for reproducibility
set.seed(123)

## Dataset 4
# Sample size
n <- 90

# Simulate X-variables
# Continuous variables: Latitude and Elevation
Latitude <- runif(n, 0, 1) * 30
Elevation <- runif(n, 0, 1) * 30

# Response variable: Size
Size <- 3.33*Latitude + 1.6*Elevation - 0.08 * Latitude * Elevation

# Create dataframe
datum <- data.frame(Latitude=Latitude, Elevation=Elevation, Size=Size)

# Save the CSV file
write.csv(datum, "lecture_13_dataset4.csv", row.names = FALSE)


### Dataset for class exercise: age + sex + age*sex
# This is similar to the Age, Sex, and Size data we simulated for in Lecture 12.
# There is no collinearity between Age and Sex, but now there is an interaction
# between Sex and Age.

# First dataset
# X variable
n <- 50
x1 <- c(rep("Female", n), rep("Male", n))
x2 <- runif(n * 2, 1, 10)
dummy <- data.frame(model.matrix(~ x1 - 1))
colnames(dummy) <- c("Female", "Male")

# Simulate error
Error <- rnorm(n * 2, 0, 0.8)

# Predict Y
Response <- 1 + 2 * x2 + 4 * dummy$Male + 1 * x2 * dummy$Male + Error

# Dataframe
datum <- data.frame(Age = x2, Sex = x1, Male = dummy$Male, Size = Response)

# Save as CSV
write.csv(datum, "lecture_13_dataset.csv")
```

[--go to next lecture--](lecture_14.html)
