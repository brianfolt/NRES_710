---
title: "Linear Regression - presenting results"
author: "NRES 710"
date: "Last compiled: `r Sys.Date()`"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
    css: styles.css
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
)
```

```{r echo=FALSE}

#  NRES 710, Linear Regression 2 
#     University of Nevada, Reno
#     Presenting results of linear regression   

```

Questions from last class? We covered a lot; don't hesitate to reach out if I can help you understand anything better.

Schedule: our first Exercise is next class! You will need a computer, so please bring one.

## Review

Today we will discuss how to present results from linear regression. I am going to describe an approach that uses a few concise sentences to distill down all of the relevant information from the analysis you ran. But first, a brief review from last class.

Most important thing to have learned from last class so far:

**The equation for the general linear model: $Y = \beta_0 + \beta_1X + \epsilon \sim N(0, \sigma)$**

- We will use this throughout the class!
- Y is equal to $\beta_0$ (intercept) plus $\beta_1$ (slope) times X plus error ($\epsilon$), which is normally distributed with a mean of zero and a standard deviation ($\sigma$).
- The error term indicates that we have a *bunch of noise* that is *centered on the line* (mean of zero); the standard deviation determines *how close to the line the points are* and has to be estimated.
- We determine the regression line by ensuring the average error is zero and minimize the sum of squares error (the distance from every point to that line).
- To calculate p-values, we partition the total variation in Y (Total Sum of Squares; TSS) into Sum of Squared Error (SSE) and the Sum of Squares due to Regression (SSR). **TSS = SSE + SSR.**
- Three things that influence p-values in regression: **sample size**, **effect size**, and **error** (noise).

## Presenting results from regression

Today we will discuss how to report results when you run regression. By spending time discussing how to report the results, I am hoping this will help all of the material in this class make more sense.

In class exercise, I will expect you to report your results using this format, which is the format that you would be expected to report results in your thesis, dissertation, and scientific papers that you publish.

This approach has **four elements**:

1) **Slope**
2) **P-value** (And you should report a slope whether or not the p-value is significant or not!)
3) **Confidence interval**
4) **$r^2$**

We have discussed slopes and p-values in the previous few classes, but today we will introduce *confidence intervals* and $r^2$ values at greater detail. And we will start with confidence intervals -- what are confidence intervals...?

## Confidence intervals

**95% CI -- a measure of uncertainty** around our estimate; **ca. 2 * SE**

- We have given our reader an estimate of the p-value, but it is unlikely to be a perfect estimate of truth.
- On average, it is unbiased. If we collect some data 1,000 different times and fit a regression each time, the average slope from among each of those regressions will approximate truth. But any one regression on a single dataset may not be very close to truth... due to the nature of data, noise, process error, and sampling error.
- Thus, we need to give our readers some measure of *how certain* we are of our slope estimate.

Definition of 95% confidence interval: <u>**95% of all such intervals contain truth.**</u>

- It might be tempting to think about CI by saying that: there's a 95% chance that truth is within the interval.
- However, this is **wrong**, because of the idea of the coinflip from the first day of class. Truth is either inside the interval, or it isn't. Just because we don't know that does not mean there is an underlying probability.
- This is a nice way of thinking about confidence intervals, but it's not technically correct.
- Does this make sense? I personally find this topic confusing.

What you really need to know, the technical definition: <u>**95% of such intervals contain truth**</u>.

### An example

Here's how we might visualize this. I simulated 100 random datasets, and each dataset had *n* = 30. I simulated each dataset with a mean $\mu$ = 0, and then I measured the mean and 95% confidence intervals for each of the 100 datasets. I then plotted each individual dataset on the y-axis (1 to 100) and the 95% confidence intervals around the mean on the x-axis.

```{r simulate-ci-plot, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=7}
# Set the seed for reproducibility
set.seed(123)

# Parameters
mu <- 0
n_simulations <- 100
n_samples <- 30
alpha <- 0.05

# Function to calculate 95% CI
calculate_ci <- function(data, alpha) {
  mean_val <- mean(data)
  se <- sd(data) / sqrt(length(data))
  error <- qt(1 - alpha/2, df=length(data) - 1) * se
  ci_lower <- mean_val - error
  ci_upper <- mean_val + error
  return(c(ci_lower, ci_upper))
}

# Simulate data and calculate CIs
ci_list <- matrix(NA, nrow=n_simulations, ncol=2)
for (i in 1:n_simulations) {
  sample_data <- rnorm(n_samples, mean=mu, sd=1)
  ci_list[i, ] <- calculate_ci(sample_data, alpha)
}

# Create a data frame for plotting
ci_df <- data.frame(
  Sample = 1:n_simulations,
  CI_Lower = ci_list[, 1],
  CI_Upper = ci_list[, 2],
  Contains_Mu = (ci_list[, 1] <= mu & ci_list[, 2] >= mu)
)

# Plotting the CIs
library(ggplot2)
ggplot(ci_df, aes(y = Sample, x = CI_Lower, xend = CI_Upper)) +
  geom_segment(aes(yend = Sample, color = Contains_Mu), size = 1.2) +
  scale_color_manual(values = c("TRUE" = "black", "FALSE" = "red")) +
  geom_vline(xintercept = mu, linetype = "dashed") +
  labs(x = "Confidence Interval",
       y = "Sample Number",
       color = "Contains Mean") +
  theme_minimal() +
  theme(legend.position = "top")

```

The red intervals are situations where the 95% confidence intervals do not include the true known mean! Four times the 95% CI did not include the true mean value, 0!

Unfortunately, when we analyze data, we have no idea if our particular confidence interval is one that includes 'truth' for the parameter!

Don't worry if you find this difficult -- I think most people do! And practically speaking, just about everyone interprets a 95% confidence interval as having a 95% probability of including the true parameter -- and it doesn't really matter that much!

### 95% CI and significance testing

Here's another useful way to think about this *graphically*.

Let's create a frequency distribution (y-axis) of our slope (x-axis). We run 1000 regressions, and get 1000 estimates of slope. The mean of all these estimates is 'truth' -- the true slope (orange vertical line). The distribution of estimates would be a normally-distributed bell curve. A noisy system has a wide bell curve and a wide confidence interval; a less noisy system (or a large sample sizes) would have a more narrow confidence interval. See below:

```{r normal-curve-plot-base-r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=7.5, fig.height=4.5}
# Set parameters for the normal distribution
mean_slope <- 0  # Mean labeled as "Slope"
sd_slope <- 1    # Standard deviation

# Create a sequence of x values
x_values <- seq(mean_slope - 4 * sd_slope, mean_slope + 4 * sd_slope, length.out = 1000)

# Calculate the density values and normalize them
density_values <- dnorm(x_values, mean = mean_slope, sd = sd_slope)
density_values_less_noisy <- dnorm(x_values, mean = mean_slope, sd = sd_slope / 2)

# Normalize the density values so that their peaks are the same
density_values <- density_values / max(density_values)
density_values_less_noisy <- density_values_less_noisy / max(density_values_less_noisy)

# Reduce outer margins
par(mar = c(4, 6, 0, 0), mgp = c(2, 0.5, 0))  # bottom, left, top, right

# Plot the normal distribution
plot(x_values, density_values, type = "l", col = "blue", lwd = 4,
     xlab = expression("True slope (or " * beta[1] * ")"), ylab = "Frequency", cex.lab = 1.5,
     ylim = c(0, 1.1), axes = FALSE)

# Add the second normal distribution
lines(x_values, density_values_less_noisy, col = "green", lwd = 4)

# Add vertical line for the mean
abline(v = mean_slope, lty = 2, lwd = 4, col = "orange")

# Add box around the plot
box()

# Add legend
legend("topright", legend = c("Noisy data", "Less noisy data"), 
       col = c("blue", "green"), lwd = 2, bty = "n", cex = 1.2)
```

The confidence interval in our regression takes this distribution and instead of centering it on 'truth', it centers it on our estimated slope ($\beta_1$). If we assume this is the true slope, this bell curve represents the range of $\beta_1$ estimates we would get when we run regression.

95% of the time we will get a slope estimate that is within the central area under the curve that contains 95% of this distribution. This is the 95% confidence interval. We can add two lines to the above graph to indicate this range of values around the true slope:

```{r normal-curve-2, echo=FALSE, message=FALSE, warning=FALSE, fig.width=7.5, fig.height=4.5}
# Set parameters for the normal distribution
mean_slope <- 0  # Mean labeled as "Slope"
sd_slope <- 1    # Standard deviation
ci_level <- 0.95  # Confidence level

# Calculate the 95% CI boundaries
z_score <- qnorm((1 + ci_level) / 2)
ci_lower <- mean_slope - z_score * sd_slope
ci_upper <- mean_slope + z_score * sd_slope

# Create a sequence of x values
x_values <- seq(mean_slope - 4 * sd_slope, mean_slope + 4 * sd_slope, length.out = 1000)

# Calculate the density values and normalize them
density_values <- dnorm(x_values, mean = mean_slope, sd = sd_slope)

# Normalize the density values so that their peaks are the same
density_values <- density_values / max(density_values)

# Reduce outer margins
par(mar = c(4, 6, 0, 0), mgp = c(2, 0.5, 0))  # bottom, left, top, right

# Plot the normal distribution
plot(x_values, density_values, type = "l", col = "blue", lwd = 4,
     xlab = expression("True slope"), ylab = "Frequency", cex.lab = 1.5,
     ylim = c(0, 1.1), axes = FALSE)

# Add vertical line for the mean
abline(v = mean_slope, lty = 2, lwd = 4, col = "orange")

# Add vertical lines for the 95% CI
abline(v = ci_lower, col = "purple", lwd = 2, lty = 2)
abline(v = ci_upper, col = "purple", lwd = 2, lty = 2)

# Add box around the plot
box()

# Add legend
legend("topright", legend = c("Data", "Mean", "95% CI"), 
       col = c("blue", "orange", "purple"), lwd = 4, lty = c(1, 2, 2), bty = "n", cex = 1.2)
```

This same bell curve is also used in the calculation of **p-values**. Instead of centering on slope of mean = 'truth', it centers the distribution on a slope = 0, and it says: if we assume that slope = 0 (the null hypothesis; $H_0$), what is the probability of getting our observed data (or more extreme) that produced $\beta_1$?

```{r normal-curve-3, echo=FALSE, message=FALSE, warning=FALSE, fig.width=7.5, fig.height=4.5}
# Set parameters for the normal distribution
mean_slope <- 0  # Mean labeled as "Slope"
sd_slope <- 1    # Standard deviation
ci_level <- 0.95  # Confidence level

# Calculate the 95% CI boundaries
z_score <- qnorm((1 + ci_level) / 2)
ci_lower <- mean_slope - z_score * sd_slope
ci_upper <- mean_slope + z_score * sd_slope

# Create a sequence of x values
x_values <- seq(mean_slope - 4 * sd_slope, mean_slope + 4 * sd_slope, length.out = 1000)

# Calculate the density values and normalize them
density_values <- dnorm(x_values, mean = mean_slope, sd = sd_slope)

# Normalize the density values so that their peaks are the same
density_values <- density_values / max(density_values)

# Reduce outer margins
par(mar = c(4, 6, 0, 0), mgp = c(2, 0.5, 0))  # bottom, left, top, right

# Plot the normal distribution
plot(x_values, density_values, type = "l", col = "blue", lwd = 4,
     xlab = expression("Slope = 0 (" * H[0] * ")"), ylab = "Frequency", cex.lab = 1.5,
     ylim = c(0, 1.1), axes = FALSE)

# Add vertical line for the mean
abline(v = mean_slope, lty = 2, lwd = 4, col = "orange")

# Add vertical lines for the 95% CI
abline(v = ci_lower, col = "purple", lwd = 2, lty = 2)
abline(v = ci_upper, col = "purple", lwd = 2, lty = 2)

# Add a dot in the bottom right tail and label it as "Beta1"
dot_x <- mean_slope + 2.4 * sd_slope  # Position of the dot
dot_y <- dnorm(dot_x, mean = mean_slope, sd = sd_slope) / max(density_values) * 0.1  # Adjust y position
points(dot_x, dot_y, pch = 19, col = "red", cex = 1.5)
text(dot_x, dot_y, labels = expression(beta[1]), pos = 2, col = "red", cex = 1.2)

# Add box around the plot
box()

# Add legend
legend("topright", legend = c("Data", "Mean", "95% CI"), 
       col = c("blue", "orange", "purple"), lwd = 4, lty = c(1, 2, 2), bty = "n", cex = 1.2)
```

If our $\beta_1$ is out in the tail of this distribution (or, outside of the 95% confidence interval), then we would get a small p-value, and then **reject** $H_0$.

**What this means is that we technically don't need a p-value!!** Our confidence interval can tell us whether our slope estimate is statistically significant or not.

```{r normal-curve-4, echo=FALSE, message=FALSE, warning=FALSE, fig.width=7.5, fig.height=4.5}
# Set parameters for the normal distribution
mean_slope <- 0  # Mean labeled as "Slope"
sd_slope <- 1    # Standard deviation
ci_level <- 0.95  # Confidence level

# Calculate the 95% CI boundaries
z_score <- qnorm((1 + ci_level) / 2)
ci_lower <- mean_slope - z_score * sd_slope
ci_upper <- mean_slope + z_score * sd_slope

# Create a sequence of x values
x_values <- seq(mean_slope - 4 * sd_slope, mean_slope + 4 * sd_slope, length.out = 1000)

# Calculate the density values and normalize them
density_values <- dnorm(x_values, mean = mean_slope, sd = sd_slope)

# Normalize the density values so that their peaks are the same
density_values <- density_values / max(density_values)

# Reduce outer margins
par(mar = c(4, 6, 0, 0), mgp = c(2, 0.5, 0))  # bottom, left, top, right

# Plot the normal distribution
plot(x_values, density_values, type = "l", col = "blue", lwd = 4,
     xlab = expression(beta[1]), ylab = "Frequency", cex.lab = 1.5,
     ylim = c(0, 1.1), axes = FALSE)

# Add vertical line for the mean
abline(v = mean_slope, lty = 2, lwd = 4, col = "orange")

# Add vertical lines for the 95% CI
abline(v = ci_lower, col = "purple", lwd = 2, lty = 2)
abline(v = ci_upper, col = "purple", lwd = 2, lty = 2)

#  Add vertical line to the right of the 95% CI and label it zero
zero_line <- ci_lower - 1.5
abline(v = zero_line, col = "red", lwd = 2, lty = 1)

# Add x-axis with a single tick for zero
axis(1, at = -3.46, labels = "0", las = 1)

# Add box around the plot
box()

# Add legend
legend("topright", legend = c("Data", "Mean", "95% CI", "Zero"), 
       col = c("blue", "orange", "purple", "red"), lwd = 4, lty = c(1, 2, 2, 1), bty = "n", cex = 1.2)

```

All we need to know is whether 0 (zero) is within the 95% confidence interval of $\beta$ or not.

- If 0 is **outside** the 95% CI for $\beta$, then $\beta$ **is** a real, statistically significant effect!
- If 0 is **inside** the 95% CI for $\beta$, then $\beta$ **is not** a real, statistically significant effect.

Confidence interval terms:

- **Confidence limits:** the values for the upper and lower edges of 95% CI.
- **Confidence interval:** the distance between the $\beta$ and each confidence limit.

If we get a p-value = 0.05, that means zero will be at one of our confidence limits (i.e., significant).

If zero is outside of our confidence limits, the p-value will be < 0.05 (i.e., significant).

If zero is within one of our confidence limits, the p-value will be > 0.05 (i.e., not significant).

### Reporting confidence intervals

**$\beta_1$ +/- 95% CI**

Since the CI are symmetric, we can use plus/minus to report the CI.

- Note: why use +/-? Journals made this rule up to save space...

### What you need to know:

1. The technical definition of a 95% confidence interval: 95% of all such intervals contain 'truth'.
    - If 95% CI includes 0, then *p* > 0.05
    - If 95% CI do not include 0, then *p* < 0.05
2. The relationship between confidence intervals and p-values. They are the flip-side of the same coin.

**Easy red flag**: something is "significant", but the 95% CI include zero. Something is wrong here!

## $r^2$

### What is $r^2$

The technical definition of **$r^2$ is the <u>*proportion*</u> of variation in Y that is explained by X.** This definition will be on quizzes, etc. Also known as: **coefficient of determination**.

- Keyword: <u>*proportion*</u>.
- This is **not** the absolute amount of variation in Y explained by X -- which is the Sum of Squares due to Regression (SSR).
- How do you estimate the *proportion*...? Divide by the TOTAL.. the Total Sum of Squares (TSS).
- So by dividing the SSR by the TSS, we get a proportion of variation in Y that is explained by X.
- Also known as: coefficient of determination.

**Thus: $r^2 = \frac{SSR}{TSS}$**

Since it is a proportion, 0 < $r^2$ < 1.

**If $r^2$ = 0, then no variation in Y is explained by X**. Points at random, with no slope. **Slope = 0**.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4.5, fig.height=4}
set.seed(123); par(mar = c(4, 4, 0.5, 0))

# Random data
n <- 25
x <- runif(n, 0, 10)
y <- runif(n, 5, 15)

# Plot
plot(y ~ x)

```

**If $r^2$ = 1, then all variation in Y is explained by X; no variation in Y is due to error.** This means you have no error, all your points are right on the line! You have a perfect relationship.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=4.5, fig.height=4}
set.seed(123); par(mar = c(4, 4, 0.5, 0))

# Random data
n <- 25
x <- runif(n, 0, 10)
y <- 5 + 2*x

# Plot
plot(y ~ x)

```

This doesn't really ever happen in environmental sciences... There is so much noise and other processes driving relationships that $r^2$ values never approach 1! Some have said that an $r^2$ of 0.1 is a good results in ecology (!).

### *r*

Another thing you should know is **r** -- the **correlation coefficient**. It doesn't have a good meaning, but it is the square-root of $r^2$. It can describe correlative relationships that are positive or negative.

- $r = \sqrt{r^2}$
- This can be positive or negative and anywhere from -1 to 1; it can describe negative relationships or positive relationships.

### Why report $r^2$?

I have previously said the main goal of regression was to measure the slope. So why would we need to report the $r^2$? 

Let's assume you graduate and get a job as a habitat manager. Your new supervisor wants you to increase Greater Sage-grouse density in central Nevada, and they want you to do it by manipulating some variable (e.g., native grass density). You go out and collect data to estimate the relationship between grouse and grasses.

**Which of the following two scenarios would you rather have?** There's no wrong answer.

```{r simulate-plot-4, echo=FALSE, message=FALSE, warning=FALSE, fig.width=9, fig.height=4}
# Set the seed for reproducibility
set.seed(123)

# Set up the plotting area to display two plots side by side
par(mfrow = c(1, 2))

# Simulate data for the shallow slope
n <- 40
x1 <- rnorm(n, mean = 20, sd = 10)
y1 <- round(10 + 3 * x1 + rnorm(n, mean = 0, sd = 16), 1)

# Create a data frame for the shallow slope
datum1 <- data.frame(x = x1, y = y1)

# Plot the data with the shallow slope
plot(y ~ x, data = datum1,
     ylim=c(0, 100), xlab = "Grass density", ylab = "Grouse abundance")

# Add a line to the plot for the shallow slope
results1 <- lm(y ~ x, data = datum1)
abline(results1)

# Simulate data for the steep slope
x2 <- rnorm(n, mean = 20, sd = 10)
y2 <- round(10 + 1 * x2 + rnorm(n, mean = 0, sd = 2), 1)

# Create a data frame for the steep slope
datum2 <- data.frame(x = x2, y = y2)

# Plot the data with the steep slope
plot(y ~ x, data = datum2,
     ylim=c(0, 100), xlab = "Grass density", ylab = "Grouse abundance")

# Add a line to the plot for the steep slope
results2 <- lm(y ~ x, data = datum2)
abline(results2)
```

Left graph has a steeper slope, but right graph has less error.

**Q:** Anybody want to make an argument for the left situation being better?

- Left graph: a small change in X makes a larger increase in Y, because the effect size is big.

**Q:** Anybody want to make an argument for the right situation being better?

- Right graph: we may have to make a bigger change in X to make the same change in Y, but we **know** that we will achieve that change in Y. 

- Left graph: There is a lot of uncertainty, and an increase in X could actually cause a decreased outcome in Y. The slope is larger, but there's a lot of noise. And if we make the increase in X that we are interested in, it's possible that the outcome could actually go down! Because of noise and other factors we can't control.

I suppose the answer to the habitat management question... depends on risk tolerance.

What we really want is the slope on the left with the $r^2$ on the right! A small increase will get a big effect with high certainty. 

But from a statistical perspective, when we use regression, we will report the slope and also the $r^2$, because the $r^2$ **describes the *strength of the relationship* and how closely X and Y are related.**

- We could have a strong slope with lots of noise (low $r^2$)
- Or we can have a weak slope with little noise (high $r^2)

The slope and the $r^2$ tell us different things about the relationship. One is the size of the effect, the other is the proportion of variation in the response variable explained by changes in the X-variable.

## Reporting results

Here is my generic sentence for reporting results from linear regression! Anything in <u>brackets</u> gets replaced by actual word or number you would put in there.

**For each 1 [X-units] increase in [X], we observed a [slope / $\beta$] [Y-units] (+/-[95% CI]; +/-95% CI) [increase/decrease] in [Y] (p = [p-value]; [$r^2$]).**

- **If p > 0.05, then add: "; however, our results are not statistically significant."**

We do this to clearly communicate our results to our readers! The 95% CI will also indicate that the result was non-significant -- but again, it's good to be slightly redundant here for clarity's sake.

Let's try this with our results from last class.

```{r}
### Analyze data

# Read in the data from Tuesday: precipitation effects on biomass
datum <- read.csv("lecture_3_dataset1.csv")

# Fit the linear model
results <- lm(biomass ~ precip, data = datum)

# Examine the results
summary(results)

```

**For each 1 cm increase in precipitation, we observed a 2.81 kg/ha ...** -- but where are the confidence intervals??

**Note:** I recommend simply using two significant digits after the decimal. In the above case, you would avoid saying "a 3 unit increase" or "a 2.8 unit increase".

- (Sidenote: technically, usual scientific convention involved reporting significant digits according to precision in the sampling... Maybe you remember this from chemistry class? But we often don't know that in ecology, so we can be more relaxed about this here.)

```{r}
# Print the confidence intervals
confint(results)

```

These are actually the confidence limits! But we want to calculate the confidence intervals (e.g., +/-95% CI). This is annoying.

Note: when you get the confidence limits, your $\beta$ estimate should be smack-dab in the middle!

**Q:** How would we calculate the 95% CI? There are a few ways...

1) Upper limit minus estimate
2) Estimate minus lower limit
3) Upper limit minus lower limit divided by two

Use whichever one makes most sense to you. I frequently use (3), but don't forget to divide by two! Also, be careful with negative values...

```{r}
# Calculating the 95% confidence intervals
(3.071 - 2.556) / 2
3.071 - 2.814

```

Using this info, we can continue to write our sentence:

**For each 1 cm increase in precipitation, we observed a 2.81 kg/ha (+/-0.25; 95% CI) increase in biomass, which was statistically significant (p = 2x10^-16; $r^2$ = 0.95).**

<u>95% of the variation is explained by precipitation, while 5% is driven by noise!</u>

Notes:

- When p < 2x10^-16, the p-value is so small that Program R won't even give you the exact value.
- Report the exact p-value with scientific notation in this class, so I can quickly know whether your results are correct or not. In a journal article, we would never write "2x10^-16" -- we would instead write "p < 0.0001".
- Two $r^2$ values! Generally, we want to always use the <u>**multiple $r^2$**</u> -- ignore the adjusted. $r^2$ values often go up when we add more variables, and the adjusted $r^2$ tries to account for that. We may talk about this later, but just use the 'Multiple r-sqared' value.
- Again: 95% CI and p-values are related. As 95% CI become smaller, p-values become smaller. And when 95% CI no longer overlap with zero, then p-values will become less than 0.05.

### Minor adjustments to the results sentence

**Negative effects:** **Q:** What if the effect (slope) is negative? Don't include the negative in the sentence, just change **increase** to **decrease**.

**Rescaling effects:** Sometimes when the way we measure things makes our $\beta$ values a little... strange. For example, elevational gradients influence species richness in strong ways. However, the unit change for elevation is meters (meter), and a 1 unit increase in meters will only have a tiny effect on species richness -- although highly significant. We can **re-scale** the metrics at ecologically relevant scales to make the effects make more sense. For elevation, we might multiply the X value, the effect, and the 95% CI times 1000, so that everything is expressed in terms of kilometers, rather than meters. You can do this with your raw data or with your results. We will need to do this during Exercises.

**Comparing effect sizes:** When we examine our results, we should consider whether the effects we are observing are <u>biologically significant</u>. 1 cm increase in rainfall causes 2.81 kg/ha increases in biomass... is that biologically significant? I think so. When you are trying to compare two different effects and trying to infer which has a stronger biological effect, you cannot compare one beta to another.

- Which is more important in driving biomass: precipitation or fertilizer?
- We can't make that comparison.
- If you want to compare betas directly, you have to standardize your variables first, before fitting the model.
  - e.g., <u>'Mean transformation':</u> subtract each value by the mean and divide by the standard deviation.
  
**SD vs. SE**: Our results provide an immediate assessment of uncertainty in parameter estimates in the form of **Standard Error (SE)**. This is slightly different from **Standard Deviation (SD)**. SD is variation of data that you have collected; 95% of data are within 2 SD of the mean of the data. SE is the SD of a statistic (the *mean*), such that 95% of mean estimates lie within 2 SE of the estimate.

## Concluding thoughts

I really like this sentence structure for reporting results. This was what I was taught in graduate school, and I have taught this to many peers and students since then. I find that it makes clear sense to readers about cause and effect in ecology. You could read this to your parent and they will understand it. They may not care -- but they would understand it. That's powerful!

**We want to communicate our science clearly to our scientific peers, natural resource managers, and policy makers, and the public.** This sentence structure is useful to do this. It is clear and it emphasizes biological effects, which helps us better understand and communicate cause and effect in nature.

This sentences are what you will report for the results of your exercise on linear regression! That's it -- report these sentences. I will not check your exact grammar, but it should follow this general pattern and be mindful of some of the details I mentioned above.

[--go to next lecture--](lecture_5.html)
