---
title: "Collinearity"
output: beamer_presentation
date: "2024-10-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Symptoms of collinearity

1) Collinearity between independent variables
  - High $r^2$ values between X-variables
  - Statistically-significant relationships between X-variables
2) High variance inflation factors (VIF) of variables in model
3) Variables significant in simple regression, but not in multi-variable regression
4) Individual variables not significant in multi-variable regression model, but the overall multi-variable regression model is significant
5) Large changes in coefficient estimates between full and reduced models
6) Large SE in multi-variable regresion models, despite high power

## Simulation exercise 1

- I simulated 1,000 datasets with varying degrees of collinearity (correlation) between two X-variables. Here is truth:
  - Simulations: $n = 1,000$
  - $y = 10 + 3X_1 + 3X_2 + \epsilon \sim N(0, 2)$ -- both X variables have effects on Y!
  - $X_1 = U[0,10]$
  - $X_2 = X_1 + N(0, z)$
  - For each simulation, I used a different value of *z* from a uniform distribution: $z = U[0.5, 20]$.

```{r simulation_1, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=4.5}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0,0))

# Number of simulations
s <- 1000

# Empty vectors to save results from each simulation
simple_beta1 <- numeric(s)
simple_se1 <- numeric(s)
simple_p1 <- numeric(s)
multi_beta1 <- numeric(s)
multi_se1 <- numeric(s)
multi_p1 <- numeric(s)
r2 <- numeric(s)
vif <- numeric(s)

# x2 = x1 + error(0, z), where
# z can range from 0.5 (highly correlated to x1) to 20 (not correlated at ~all)
z_values <- seq(0.5, 20, length.out = s)

# Loop through each simulation replicate to measure everything
for (i in 1:s){
  
  # Number of datapoints per simulation
  n <- 100

  # x1
  x1 <- runif(n, 0, 10) # Random, uniform variable; only simulated once

  # error for y; only simulated once
  error <- rnorm(n, 0, 2)
  
  # X2 for each simulation
  x2 <- x1 + rnorm(n, 0, z_values[i])
  
  # Y for each
  y <- 10 + 3 * x1 + 3 * x2 + error

  # Simple model
  results1 <- lm(y ~ x1)
  simple_beta1[i] <- summary(results1)$coefficients[2,1]
  simple_se1[i] <- summary(results1)$coefficients[2,2]
  simple_p1[i] <- summary(results1)$coefficients[2,4]

  # Multi-variable model
  results2 <- lm(y ~ x1 + x2)
  multi_beta1[i] <- summary(results2)$coefficients[2,1]
  multi_se1[i] <- summary(results2)$coefficients[2,2]
  multi_p1[i] <- summary(results2)$coefficients[2,4]

  # R^2 between X-variables
  model_x12 <- lm(x2 ~ x1)
  r2[i] <- summary(model_x12)$r.squared
  
  # VIF for multi-variable model
  vif[i] <- car::vif(results2)["x1"]
}

```

## Methods

For each simulation, I did a few things:

- Fit a **simple model** ($Y \sim X_1$) and measured the estimate, SE, and p-value for $\beta1$
- Fit a **multi-variable model** ($Y \sim X_1 + X_2$) that included both of the collinear, confounding variables, and measured the effect, SE, and p-value for $\beta1$.
- Measured how collinearity between $X_1$ and $X_2$ (i.e., $r^2$) influenced the the **Variance Inflation Factor** from the multi-variable model

## Variance Inflation Factor

**Variance Inflation Factor (VIF) -- the amount (in *times*) that the variance ($SE^2$) in the $\beta$ increases due to collinearity**

```{r simulation_2, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0,1))

# Plotting VIF against collinearity values
plot(r2, vif, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = "VIF")

# Add horizontal line for when VIF = 2
abline(h = 2, lty = 2)

```

## Simple model: $Y \sim X_1$

```{r simulation_3, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the simple model
plot(r2, simple_beta1, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("Estimate of " ~ beta[1]))

```

## Simple model: $Y \sim X_1$

```{r simulation_4, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot SE estimate by collinearity values from the simple model
plot(r2, simple_se1,
     xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]),
    ylab = expression("SE of estimate of" ~ X[1]))

```

## Simple model: $Y \sim X_1$

```{r simulation_5, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.5, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot p-values by collinearity values from the simple model
plot(r2, log(simple_p1),
     xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]),
     ylab = expression("P-values of estimate of" ~ X[1] ~ " (log-transformed)"))

```

## Multi-variable model: $Y \sim X_1 + X_2$

```{r simulation_6, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the multi-variable model
plot(r2, multi_beta1, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("Estimate of " ~ beta[1]))

# Add a horizontal line for when r^2 = 0.6
abline(v = 0.6, lty=2)

```

## Multi-variable model: $Y \sim X_1 + X_2$

```{r simulation_7, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the multi-variable model
plot(r2, multi_se1, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("SE of estimate of " ~ beta[1]))

```

## Multi-variable model: $Y \sim X_1 + X_2$

```{r simulation_8, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the multi-variable model
plot(r2, log(multi_p1), xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("P-value of estimate of " ~ beta[1] ~ "(log-transformed)"))

```

## Confounding variables

**Confounding variable -- a variable that will bias results if you leave it out**.

- Correlated with another X-variable
- Has it's own effect on Y

To avoid negative effects of confounding variables, I recommend:

1) **Sample in a manner that eliminates collinearity**.
2) **Use multi-variable regression**.
3) **Include confounding variables, even if they are non-significant**.
4) **Get more data!** This decreases SE and VIF.

## Redundant variables

**Redundant variables -- collinear X-variables that don't have an effect on the Y-variable**.

- Correlated to another X-variable, but
- Do not have an effect on Y-variable

A useful way to think about confounding or redundant variables might be with the $\beta$s.

- If the $\beta \neq 0$, it's a confounding variable.
- If the $\beta = 0$, it's a redundant variable.

## Simulation exercise 2

This simulation exercise is similar as before, but now only $X_1$ has an effect, and $X_2$ is a redundant variable.

- $X_1$ = 3
- $X_2$ = 0

```{r sim_exercise_2_1, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=4.5}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0,0))

# Number of simulations
s <- 1000

# Empty vectors to save results from each simulation
simple_beta1 <- numeric(s)
simple_se1 <- numeric(s)
simple_p1 <- numeric(s)
simple_beta2 <- numeric(s)
multi_beta1 <- numeric(s)
multi_beta2 <- numeric(s)
multi_se1 <- numeric(s)
multi_p1 <- numeric(s)
r2 <- numeric(s)
vif <- numeric(s)

# x2 = x1 + error(0, z), where
# z can range from 0.5 (highly correlated to x1) to 20 (not correlated at ~all)
z_values <- seq(0.5, 20, length.out = s)

# Loop through each simulation replicate to measure everything
for (i in 1:s){
  
  # Number of datapoints per simulation
  n <- 100

  # x1
  x1 <- runif(n, 0, 10) # Random, uniform variable; only simulated once

  # error for y; only simulated once
  error <- rnorm(n, 0, 2)
  
  # X2 for each simulation
  x2 <- x1 + rnorm(n, 0, z_values[i])
  
  # Y for each
  y <- 10 + 3 * x1 + error

  # Simple model of X1
  results1 <- lm(y ~ x1)
  simple_beta1[i] <- summary(results1)$coefficients[2,1]
  simple_se1[i] <- summary(results1)$coefficients[2,2]
  simple_p1[i] <- summary(results1)$coefficients[2,4]
  
  # Simple model of X2
  results2 <- lm(y ~ x2)
  simple_beta2[i] <- summary(results2)$coefficients[2,1]

  # Multi-variable model
  results3 <- lm(y ~ x1 + x2)
  multi_beta1[i] <- summary(results3)$coefficients[2,1]
  multi_beta2[i] <- summary(results3)$coefficients[3,1]
  multi_se1[i] <- summary(results3)$coefficients[2,2]
  multi_p1[i] <- summary(results3)$coefficients[2,4]

  # R^2 between X-variables
  model_x12 <- lm(x2 ~ x1)
  r2[i] <- summary(model_x12)$r.squared
  
  # VIF for multi-variable model
  vif[i] <- car::vif(results3)["x1"]
}

```

## Variance Inflation Factor

```{r sim_exercise_2_2, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plotting VIF against collinearity values
plot(r2, vif, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = "VIF")

# Add horizontal line for when VIF = 2
abline(h = 2, lty = 2)

```

## Simple regression model: $Y \sim X_1$

Now, we see that the existence of a redundant variable does not influence our estimation of $\beta_1$ using a simple linear model! This is good.

```{r sim_exercise_2_3, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the simple model
plot(r2, simple_beta1, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("Estimate of " ~ beta[1]))

```

## Simple regression model: $Y \sim X_1$

```{r sim_exercise_2_4, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot SE estimate by collinearity values from the simple model
plot(r2, simple_se1,
     xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]),
    ylab = expression("SE of estimate of" ~ X[1]))

```

## Simple regression model: $Y \sim X_1$

```{r sim_exercise_2_5, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.5, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot p-values by collinearity values from the simple model
plot(r2, log(simple_p1),
     xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]),
     ylab = expression("P-values of estimate of" ~ X[1] ~ " (log-transformed)"))

```

## Simple regression model w/ redundant variable: $Y \sim X_2$

```{r sim_exercise_2_6, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the simple model
plot(r2, simple_beta2, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("Estimate of " ~ beta[2]))

```

## Multi-variable model: $Y \sim X_1 + X_2$

```{r sim_exercise_2_7, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the simple model
plot(r2, multi_beta1, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("Estimate of " ~ beta[1]))

# Add line to indicate the start of variance inflation
abline(v = 0.6, lty = 2)

```

## Multi-variable model: $Y \sim X_1 + X_2$

```{r sim_exercise_2_8, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the simple model
plot(r2, multi_beta2, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("Estimate of " ~ beta[2]))

# Add line to indicate the start of variance inflation
abline(v = 0.6, lty = 2)

```

## Practical guidance for examining and dealing with collinearity

<u>**Do you have collinearity in your data or system?**</u>

1) Be careful to identify potential confounding variables prior to data collection. Use logic and try to identify all confounding variables and measure these.
2) Calculate collinearity and VIF among independent variables -- before you start your analysis. High collinearity between X-variables tends to imply redundancy.
3) Pay attention to how coefficient estimates and variable significance change as variables are removed or added.

## Practical guidance for examining and dealing with collinearity

<u>**Is a variable redundant or confounding?**</u>

1) Think! Use logic.
2) If there is extreme collinearity, there's likely a **redundant** variable
3) Large changes in coefficient estimates of **both variables** between full and reduced models: variables are likely **confounding**.
4) Large changes in coefficient estimates of **one variable** between the reduced and full model, and the full model estimates a variable to be close to zero: **redundant**
4) Not sure whether it's redundant or confounding? **Assume confounding & include it.** Multi-variable regression also produces unbiased estimates (on average) regardless of the type of collinearity.

## Practical guidance for examining and dealing with collinearity

<u>What to do with **redundant variables**?</u>

1) Determine which variable best explains the response using P-values from regression and changes in coefficient estimates with variable addition and removal
2) Do not include redundant variable in final model (to reduce VIF)

<u>What to do with **confounding variables**?</u>

1) Sample in a manner that eliminates collinearity, which can be due to real collinearity or sampling artifact.
2) Use multi-variable regression; may have large SE if collinearity is strong.
3) Include confounding variables, even if non-significant.
4) Get more data! Decrease SE due to variance inflation.

