---
title: "ANOVA"
author: "NRES 710"
date: "Fall 2020"
output: 
  html_document: 
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

```

```{r echo=FALSE}

############################################################
####                                                    ####  
####  NRES 710, Lecture 5                               ####
####                                                    ####
####  Kevin Shoemaker and Ben Sullivan                  #### 
####  University of Nevada, Reno                        ####
####                                                    #### 
############################################################


############################################################
####  ANOVA                                             ####
############################################################


```


Transition to ANOVAs and F statistic


So if linear regression is based on a t-test, then why is the p value associated with an “F statistic?” and what is an F-statistic anyway? 

The F distribution was developed by George Snedecor (founder of first US stats dept – at Iowa State. The “F” is in honor of Fisher. It’s a probability distribution of the null distribution of a test statistic. 

F=  (n(B ̂-B_0 )^2)/s^2 

F = t2

That’s right – The F distribution is just the t distribution, squared. A t-test shows the areas to the left, right, or center (depending on one- or two-tailed tests), the F distribution (being the square of that) only shows the area to the left of the statistic. In the end, the p values for a t distribution and the F distribution are IDENTICAL. Nifty, huh? 

Show F distribution. Note there are now two degrees of freedom. This is a major difference from the t-test. (but remember how a linear regression had DF  = n-p-1? The -1 can be thought of as the second degree of freedom…)  

What is the F-distribution commonly associated with (besides linear regression)? 
That’s right – ANOVA. Analysis of Variance. 

What is ANOVA? It’s a special case of regression! It was developed by Fisher. That’s why the F statistic associated with it is named after him. 

It’s used when the x-axis is categorical. There is one independent variable and two or more dependent variables that are categorical. 

Draw this out. This results in bins. We want to test if the means of the bins are the same. However, whether the means of the bins are different depends on the variance around each mean. 

Null hypothesis is that the bins are not different – i.e. µ1 = µ2 = µ3 = … = µi 

Assumptions: 
Each sample group is drawn from a normally distributed population. 
All sample groups have the same variance
Samples are independent of each other (though we’ll see a special case where this isn’t true)

Note that, like T-tests, ANOVA is reasonably robust against violations of the normal distribution assumption. 

Also note that if you’re facing a situation with two groups, you can use EITHER a t-test or an ANOVA. Which to use? T-test works with unequal variance. Use it! 

FOLLOW the ANOVA-how to do pdf! 

We deal with a model in which the individual observations (yij) are a function of the average height of the plants (µ) plus the effect of the fertilizer (Ai) plus an error term (Eij)
Our goal here is to test if A1= A2=A3=0

Three types of DF: treatment, total, and residual. DFtreat = groups – 1 . DFtotal = total observations-1. Residuals =DFtot-DFtreat

Use related Anova tables in Excel to work along. 

Then go to F table for a = 0.05, DFn = 2, DFd = 5. Fcrit = 5.79




[--go to next lecture--](LECTURE6.html) 

















