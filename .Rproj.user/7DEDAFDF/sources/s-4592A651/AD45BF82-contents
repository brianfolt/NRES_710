---
title: "Linear Regression"
author: "NRES 710"
date: "Fall 2020"
output: 
  html_document: 
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

```

```{r echo=FALSE}

############################################################
####                                                    ####  
####  NRES 710, Lecture 4                               ####
####                                                    ####
####  Kevin Shoemaker and Ben Sullivan                  #### 
####  University of Nevada, Reno                        ####
####                                                    #### 
############################################################


############################################################
####  Linear Regression                                 ####
############################################################


```


T-tests roll us straight into linear regression. Why? 

What is the hypothesis of a one-sample t-test? µ=0. 

What is the hypothesis of a linear regression? Slope = 0. 

We assume there is some true model out there (for simple linear regression): 
y= β_0+β_1 x

But we can only approximate it: 
y= b_0+b_1 x+ε

ε= y-µ_y
for each point. 

So t is simply: 
t=  |B ̂-B_0 |/〖SE〗_B 
With df = n-2 (actually n-p-1, where p is the number of parameters)

We must assume: 
Linear model – best model will have least square fit to the data. 
Independence
Equal variance (homoscedasticity, or lack of heteroscedasticity)
Normal distribution
x-values are correct – the error is associated with the y value, not x. 

The goal of linear regression is to model the relationship between some response variable (dependent) and an explanatory variable (independent). You can have multiple explanatory variables… hence you can have multiple linear regression. We will focus on simple linear regression here. 

The model is unknown but parameters of the model are estimated from the data, plus error (as seen above). Models are fitted using “least squares” approach. The best fit model minimizes the sum of the squared residuals. DRAW THIS OUT

In order to calculate a regression, we must know the slope and the intercept. 

m=(∑_(i=1)^n▒(x_i-x ̅ )(y_i-y ̅ ) )/(∑_(i=1)^n▒(x_i-x ̅ )^2 )  


b= y ̅-(x ̅m)

However, these terms do not account for variability between the model and the data. We get that from the correlation coefficient (r). 
r=  (∑_(i=1)^n▒(x_i-x ̅ )(y_i-y ̅ ) )/√(∑_(i=1)^n▒(x_i-x ̅ )^2 ×∑_(i=1)^n▒(y_i-y ̅ )^2 )

X	Y 	xi-xbar	Yi-ybar	xi-xbar – yi-ybar	(xi-xbar)^2	(yi-ybar)^2
4.1	2.2	-8.075	-7.85	63.38875	65.205625	61.6225
6.5	4.5	-5.675	-5.55	31.49625	32.205625	30.8025
12.6	10.4	0.425	0.35	0.14875	0.180625	0.1225
25.5	23.1	13.325	13.05	173.89125	177.555625	170.3025
				268.925	275.1475	262.85
12.175	10.05					

So m = 262.85/275.1475 = 0.977385

B = 10.05-(12.175*0.977385) = -1.84966

R = 0.999987



Explore all four possible outcomes of linear regression: non-significant p/ high r, significant p/ low r, and significant p/high r and nonsignificant p/low r/.

Show anscombe’s quartet. Use anscombe’s quartet as jumping off point to explore R output associated with regression. 

Plot your regression object. See what comes out:

> plot(ellem)
Hit <Return> to see next plot: 

First up, residuals vs fitted plot. What is a residual? It’s the yi-ybar above. Remember, this is always on the y. This figure tests the assumption of whether the relationship between your variables is linear - Is there equal variance along the regression line? It’s “good” if you have no real shape to the data, no clear outliers, and symmetrical around the dotted “0” line. 

Next up: Q-Q plot: plots your data relative to the normal distribution. The y-axis is the residuals. The x-axis is the percentile of your data. 0 is the 50th percentile of your data. The 95th percentile would be 1.64. It’s “good” if your data line up along the dotted line, without getting too far away. Your data would not be normally distributed if they got really far off. It’s subjective. There are tests for this, though! 

Next: Scale-Location plot: This also tests for homoscedasticity. Are the residuals spread evenly along the ranges of predictors (the x-axis)? If not, you’ll get a strong trend and generally see a cone-shaped pattern. 

Finally: Residuals vs Leverage. This tests if any data points are exhibiting a strong pull on the data. It includes an estimate of “Cook’s distance”  - an indicator of outlier-ness. NOTE: Not all outliers actually exhibit a pull on your linear regression. It depends where it’s located. In this plot, pattern is not relevant – you need to look for data points that fall outside Cook’s distance. 

These four figures, in sum, should be a guide to interpreting how robust your regression is. You need to be the scientist. Analyze these plots and make decisions about what you’re willing to accept.




[--go to next lecture--](LECTURE5.html) 

















