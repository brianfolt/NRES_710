---
title: "t-tests"
author: "NRES 710"
date: "Fall 2020"
output: 
  html_document: 
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

```

```{r echo=FALSE}

############################################################
####                                                    ####  
####  NRES 710, Lecture 2                               ####
####                                                    ####
####  Kevin Shoemaker and Ben Sullivan                  #### 
####  University of Nevada, Reno                        ####
####                                                    #### 
############################################################


############################################################
####  T-tests                                           ####
############################################################


```

T-tests are extremely flexible: 
They can be paired or unpaired. 
They can be a one-sample test (x1 = V (any value))
They can be a two-sample test (x1=x2)
And they can be one tailed vs two- tailed -  deals with directionality in the data.

In the classic t-test, there are equal sample size and equal variance. 

Here’s how we solve for a one-sample t-test. 

t=  (x ̅-µ_0)/(s⁄√n)
Degrees of freedom equal n-1. 
Here, we assume that 
	Samples are independent of each other
	Data are continuous
	The dependent variable should be approximately normally distributed*

Null hypothesis is that the difference between the population mean and the value for comparison is 0. 

But that’s not all! 

Two sample t-test: 

t=(x ̅_1-x ̅_2)/√((s_1^2)/n_1 +(s_2^2)/n_1 )
Degrees of Freedom  = (n1+ n2) – 2
Assumptions: 
	Data are continuous
	Samples are independent 
	Equal sample sizes
	Equal variance
	The data are approximately normally distributed* 

* T-tests have been shown to be robust against violations of these assumptions. Also, consider the CLT: the CLT doesn’t want the data themselves to be normally distributed, it wants the mean and variance to be representative of the population. The CLT provides the basis for this… but that’s not good enough for most people. 

Tails: if one tailed, 95 % of the distribution falls to the left or the right. 
If two tailed, 95 % of the distribution falls in the middle, with 2.5% of the distribution on each tail. 



Paired t-test: calculate differences. Then get the average, SD, SE. DF = differences – 1. Ho: µ=0

t=   (x_d ) ̅/〖SE〗_(x ̅d) 


Welch’s t-test: Unequal sample sizes, unequal variance. Like a two-sample t-test, but more flexible. Already robust against violations of normality. This is the DEFAULT version of a t-test in R. If you want an actual t-test, you need to tell it to do so! (var.equal = True, sample sizes must also be equal!)

t=  ((x_1 ) ̅-(x_2 ) ̅)/√((s_1^2)/n_1 -(s_2^2)/n_2 )

Degrees of freedom is more complicated, and is an approximation: 

DF=  〖((s_1^2)/n_1 +(s_2^2)/n_1 )〗^2/(〖((s_1^2)⁄n_1 )〗^2/(n_1-1)+〖((s_2^2)⁄n_2 )〗^2/(n_2-1))
It is usually a decimal number. 




```{r}

######
# T test examples


#### 
# T-tests
####

#Ttest
# Are my data greater than zero? 
Group0 <- c(0.5, -0.03, 4, 2.5, 0.89, 2.2, 1.7, 1.125)
hist(Group0)
t.test(Group0,alternative="greater") # This gets at directionality


#Are my data different than zero? 
Group0 <- c(0.5, -0.03, 4, 2.5, 0.89, 2.2, 1.7, 1.125)
hist(Group0)
t.test(Group0) # Okay, that's to zero. What about if it's different than 1? 

# are my data different than 1? 
t.test(Group0, mu=1)

# Now let's test two groups. 
# are the means equal? 
group1 <- c(7,9,6,6,6,11,6,3,8,7)
group2 <- c(11,13,8,6,14,11,13,13,10,11)
t.test(group1, group2, var.equal=T) # Notice how we set equal variance? Look at the output - "Two Sample t-test."
# is this one-tail or two? 

group1 <- c(7,9,6,6,6,11,6,3,8,7)
group2 <- c(11,13,8,6,14,11,13,13,10,11)
t.test(group1, group2) # T value does not change, but DF And p-value do! "Welch Two Sample t-test"
# WELCH IS THE DEFAULT IN R


```






[--go to next lecture--](LECTURE3.html) 

















