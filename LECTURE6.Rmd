---
title: "Linear Regression"
author: "NRES 710"
date: "Fall 2022"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

```

```{r echo=FALSE}

#  NRES 710, Lecture 6   -----------------------------
#   University of Nevada, Reno                        

#   Linear Regression                          


```

## Download the R code for this lecture!

To follow along with the R-based lessons and demos, [right (or command) click on this link and save the script to your working directory](LECTURE6.R)

## Overview: Linear Regression

Classical linear regression involves testing for a relationship between a continuous response variable (dependent variable) and a continuous predictor variable (independent variable).

You can have multiple explanatory variables (multiple linear regression). But we will focus on *simple linear regression* here (i.e., only one predictor variable). 

The null hypothesis is that there is no relationship between the response variable and the predictor variable in your population of interest. That is, values of your predictor variable are useless for predicting your response variable. 

### Simple example

Imagine we are testing for a relationship between the brightness of artificial lighting on coastal beaches (e.g., from hotels and other forms of development) and the total number of hatchling sea turtles per nest that successfully make it to the ocean. 

![](statistics1.png){width=30%}

*Population*: All nests deposited on coastal beaches across the region of interest.        
*Parameter(s)*: The relationship between the mean number of successful hatchlings per nest and the brightness of artificial lighting.   
*Sample*: All monitored nests (hopefully a representative sample of the population)         
*Statistic(s)*: Slope and intercept of the linear relationship between the measured response variable (number of successful ocean-arrivers per nest) and the predictor variable (brightness of artificial lighting)    

## Some more specifics!

### The process model

We assume there is some true model out there describing the expected (mean) value of our response variable *y* as a linear function of our predictor variable *x*: 

$E(y)= \beta_0 + \beta_1\cdot x$ 

To interpret this equation: the true mean of our response variable ($E(y)$) is computed by taking the true intercept ($\beta_0$) and adding the product of the true slope term ($\beta_1$) and our predictor variable. This is just another way of saying that the expected value of the response variable is computed as a linear function of the predictor variable. $\beta_0$ and $\beta_0$ are both *parameters* that we wish to estimate!

### The noise

To complete the thought, we also assume that there is some "noise" in the system. The "noise" term in regression and ANOVA is also known as the *error*. Specifically, we assume that the noise is normally distributed with mean of zero and standard deviation of $\sigma$.  

Mathematically, we are assuming that our data/sample was generated from this process:

$y= \beta_0 + \beta_1\cdot x + \epsilon$    

OR:

$y = E(y) + \epsilon$  

WHERE:

$\epsilon \equiv Normal(0, \sigma)$

This is actually the same assumption we made for a one-sample t-test! 

For a t-test, we assumed that there is a true population mean $\mu$ (equivalent to $E(y)$) and that the true "noise" is normally distributed with standard deviation of $\sigma$. 

As with a t-test, where we can only approximate the true population mean by computing the sample mean, we can only approximate the linear relationship between our response and predictor variables:

$\bar{y} = \hat{B_0} + \hat{B_0}\cdot x$

Just like any other statistical test, we assume that our observed linear relationship (defined by coefficients $\hat{B_0}$ and $\hat{B_1}$) is just one of many such possible relationships that *could have been derived* from random sampling from our population of interest. If we collected a different sample, we would get a different linear relationship. 

NOTE: in linear regression we are generally far more interested in the slope of the linear relationship ($\hat{B_1}$ rather than the intercept). So for now, we assume $\hat{B_1}$ (slope between response and predictor, computed from the sample) is the main test statistic of interest!

So.. what is the sampling distribution for our test statistic $\hat{B_1}$ under the null hypothesis in this case? Well, the answer is that it (when converted to units of standard error) is t-distributed! Let's look into this a bit more.

## Regression and t-tests- the link!

Our discussion of t-tests actually rolls us straight into linear regression. Why? How?   

In a one-sample t-test we are interested in estimating the true population mean, and we assume that our t-statistic (i.e., deviation of the sample mean from the null-hypothesis mean, in units of standard error) is t-distributed.

What is the null hypothesis of a linear regression? (Slope = 0 - that is, the true relationship is zero). 

So already we are seeing a bit of a similarity. 

In a t-test we assume that the population mean is equal to the null mean and that the data are normally distributed. We could write this as (using regression notation):

$y = \beta_0 + \epsilon$

WHERE:

$\epsilon \equiv Normal(0, \sigma)$

In the above equation, $\beta_0$ represents the population mean under the null hypothesis. 

We approximate our population mean using the sample mean $\bar{\beta_0}$ (formerly known as $\bar{x}$) and we use the CLT and other statistical theories to show that the t-statistic:

$t = \frac{\bar{\beta_0}-\beta_0}{StdErr(\beta_0)}$

Is t-distributed with df computed as the sample size minus the number of parameters estimated in the model (there is only one estimated parameter- the sample mean $\bar{\beta_0}$). 

In linear regression we assume that the mean of our response is determined by two parameters- the intercept and the slope (linear relationship with the predictor variable). The null hypothesis (usually) is that the true mean is defined only by the intercept term (overall mean response value) and there is no relationship with the predictor variable (slope term is equal to zero). 

The slope term of the linear regression can be computed as: 

$\hat{\beta_1} = \frac{\sum_{i=1}^{n}{(x_i-\bar{x})(y_i-\bar{y})}}{\sum_{i-1}^{n}{(x_i-\bar{x})^2}}$

And the intercept term can be computed as:

$\hat{\beta_0} = \bar{y} - \beta_1*\bar{x}$

Simple linear regression models (that is, the slope and intercept terms above) are fitted using “least squares”. The best fit model minimizes the sum of the squared residuals. DRAW THIS OUT.

The standard error of the slope term (as opposed to the standard error of the mean) is computed as:

$std.err_{\hat{\beta_1}} = \sqrt{\frac{\frac{1}{n-2}\sum_{i=1}^n{\hat\epsilon_i^2}}{\sum_{i=1}^n{(x_i-\bar{x})^2}}}$

Where the $\hat\epsilon_i$ refers to the residual errors. 

We can then compute a t-statistic for the slope term (difference from the sample slope term and the null slope term in units of standard error):

$t=\frac{\hat{\beta_1}-\beta_{1null}}{std.err_{\hat{\beta_1}}}$

Just like with the t-test, we assume that this t-statistic is t-distributed under the null hypothesis. This time the degrees of freedom is 2 less than the sample size (since computing the residual error requires computing two parameters: mean and slope). 


## Simple linear regression: examples

Okay let's consider the sea turtle example from the beginning of lecture:

Imagine we are testing for a relationship between the brightness of artificial lighting on coastal beaches of the eastern USA (e.g., from hotels and other forms of development) and the total number of hatchling sea turtles per nest that successfully make it to the ocean. 

First we will simulate some data under a known process model (don't worry if you don't totally follow this data simulation code):

```{r}

# Examples --------------------------------------

eggs.per.nest <- 100
n.nests <- 15
light <- rnorm(n.nests,50,10)   # make up some light pollution values (predictor var)

probsucc <- function(light){    # egg success as a function of light pollution
  plogis(1.5-0.01*light)
}

hatchlings.successful <- rbinom(n.nests,eggs.per.nest,probsucc(light))   # determine number of successful eggs (response var)

#curve(probsucc,0,100)

plot(hatchlings.successful~light)  # plot the data


```
Now that we have data, let's run a simple linear regression analysis! This one time we will compute the statistics directly from the data without using R functions.  

```{r}

slope <- sum((light-mean(light))*(hatchlings.successful-mean(hatchlings.successful)))/sum((light-mean(light))^2)
intercept <- mean(hatchlings.successful) - slope*mean(light)

exp.successful <- intercept+slope*light # expected number of eggs for each observation
residuals <- hatchlings.successful-exp.successful

stderr <- sqrt(((1/(n.nests-2))*sum(residuals^2))/(sum((light-mean(light))^2)))    # standard error

t.stat <- (slope-0)/stderr    # t statistic

pval <- 2*pt(t.stat,n.nests-2)    # p value

```

A much easier approach is to use R's `lm()` function: 

```{r}

# use lm() function instead (easy way!)

model <- lm(hatchlings.successful~light)

summary(model)   # get the same t stat and p-value hopefully!

```

And we can plot the regression line on top of a scatterplot:

```{r}
# plot regression line!

plot(hatchlings.successful~light)  # plot the data
abline(intercept,slope,col="blue")

```

Of course, we can't end with a single regression line, since there is sampling error associated with our best-fit regression line. There is error associated with the intercept and there is error associated with the slope term. To do this in r, we can use the 'predict' function:


```{r}

# add confidence interval on the regression line

newdata <- data.frame(    # make a data frame containing the light values we want to make predictions for (spanning the range of light values in our data)
  light = seq(20,80,1)
)

my.predict <- predict(model, newdata = newdata, interval = "confidence")  # 95% conf int by default

plot(hatchlings.successful~light)  # plot the data
abline(intercept,slope,col="blue")
lines(newdata$light,my.predict[,"upr"],col="red",lty=2)   # add upper bound
lines(newdata$light,my.predict[,"lwr"],col="red",lty=2)   # add lower bound

```

This confidence interval loosely represents the range of plausible best-fit lines we could fit if we were to gather multiple alternative datasets. If some of the plausible lines that fit within the upper and lower bounds of the confidence interval on the regression line have a positive slope and others have a negative slope, we might remain unconvinced that the relationship between the response and predictor variable is 'statistically significant'! 


**Q:** take a couple minutes to run a simple linear regression of tree Volume as a function of tree Girth using the built-in `trees` dataset.


```{r echo=FALSE, eval=FALSE}

#  use the lm() function to regress tree volume on tree girth using the 'trees' dataset

mod <- lm(Volume~Girth,data=trees)
summary(mod)

```


## Assumptions of simple linear regression

### Normal error distribution

Simple linear regression assumes that the model **residuals** are normally distributed. 

Let's look at the residuals for our sea turtle example:

```{r}

my.intercept <- model$coefficients["(Intercept)"]
my.slope <- model$coefficients["light"]
expected.vals <- my.intercept+my.slope*light 
my.residuals <- hatchlings.successful-expected.vals
my.residuals

### alternative way of getting residuals (best way!)

my.residuals2 <- model$residuals

### alternative way using predict function

my.residuals3 <- hatchlings.successful-predict(model)

### histogram of residuals

hist(my.residuals)

### test for normality

qqnorm(my.residuals)

shapiro.test(my.residuals)

```


### Linear model

The true relationship between the response and predictor variables is linear!

Which one of the following plots violates this assumption?

```{r}

layout(matrix(1:4,nrow=2,byrow = T))
plot(anscombe$y1~anscombe$x1,ylab="response",xlab="predictor")
plot(anscombe$y2~anscombe$x2,ylab="response",xlab="predictor")
plot(anscombe$y3~anscombe$x3,ylab="response",xlab="predictor")
plot(anscombe$y4~anscombe$x4,ylab="response",xlab="predictor")

```


### Independence of observations

Of course! All classical analyses make this assumption!

### Equal variance (homoscedasticity, or lack of heteroscedasticity)

In simple linear regression, we assume that the model residuals are normally distributed-- and that the spread of that distribution does not change as your predictor variable or response variable goes from small to large. That is, the residuals are **homoskedastic** -- which means they have equal variance across the range of the predictor and response variables.   

Let's look at the sea turtle example:

```{r}

# evaluate assumptions -----------------------------

my.residuals <- model$residuals
my.standardized.residuals <- rstandard(model)    # even better...

plot(my.residuals~predict(model))    # plot residuals against fitted values
# plot(my.standardized.residuals~predict(model))


```

Do you see any evidence for **heteroskedasticity**? Heteroskedasticity means non-homogeneity of variance across the range of the predictor variable. Serious violations of the equal variance assumption may warrant a **transformation** of your data, or you may choose to use an alternative analysis like a **generalized linear model**. 


### Predictor variable is known with certainty
That is, the observed (e.g., measured) values for your predictor variable are correct and known with precision. In standard regression analyses, the randomness in linear regression is associated only with the response variable - and that randomness is normally distributed.


## Diagnostic plots

Simple linear regression analyses are generally accompanied by 'diagnostic plots', which are intended to diagnose potential violations of key assumptions, or other potential pitfalls of regression. 

When you use the 'plot' function in R to evaluate a model generated with the 'lm' function, R returns four diagnostic plots:

```{r}

layout(matrix(1:4,2,byrow = T))
plot(model)

```

The first diagnostic plot (residuals vs fitted) lets you check for possible non-linear patterns. This plot should have no obvious pattern to it- it should look like random noise across the range of fitted values. 

The second diagnostic plot (normal Q-Q) lets you check for normality of residuals. You already know how to do this. 

The third diagnostic plot (scale-location) lets you check for homoskedasticity. This plot should have no obvious pattern to it- it should look like random noise across the range of fitted values.

The fourth diagnostic plot (residuals vs leverage) lets you check to make sure your regression model isn't driven by a few **influential points**. Look for points that fall far to the right of the other points- these are high leverage points. Also look specifically at the upper and lower right hand side of this figure- points in these regions (with large values for "Cook's distance") have the property that if they were removed from the analysis the results would change substantially. 

These four figures, taken together, should be a guide to interpreting how robust your regression is. Analyze these plots and make decisions about what you’re willing to accept.

### Influential points

Which of the following plots has a highly influential point?

```{r}

# which one has high influence point?

layout(matrix(1:4,nrow=2,byrow = T))
plot(anscombe$y1~anscombe$x1,ylab="response",xlab="predictor")
plot(anscombe$y2~anscombe$x2,ylab="response",xlab="predictor")
plot(anscombe$y3~anscombe$x3,ylab="response",xlab="predictor")
plot(anscombe$y4~anscombe$x4,ylab="response",xlab="predictor")

```

Can you try to draw a regression line if this point was removed? 


## Performance evaluation

### R-squared

The coefficient of determination, also known as R-squared, is a statistic that is commonly used to indicate the performance of a regression model. Specifically, R-squared tells you how much of the total variance in your response variable is explained by your predictor variable. The maximum value for R-squared is 1- values close to 1 indicate a very "good" model! An R-squared of zero indicates that the model with the predictor variable is no better than a model that simply predicts the overall mean of the response variable no matter what the predictor variable is. 

R-squared can be computed as:

$R_2 = 1-\frac{SS_{res}}{SS_{tot}}$

where $SS_{tot} = \sum(y_i-\bar{y})^2$ and $SS_{res} = \sum(y_i-y_{pred})^2$.  

In the model summary output for `lm()` objects in R, you will see an R-squared value, and also an "adjusted R-squared" value. This is only valuable for multiple linear regression. The adjusted R-squared value represents the proportion of variance in your response variable that is explained by "significant" predictor variables. If you include a bunch of meaningless predictor variables, the overall R2 will still go up (some of the variation in your data will be "explained" by the meaningless noise). The adjusted R-squared corrects for this effect! 

## Regression outcomes 

Let's explore some possible outcomes of linear regression: 

Try to come up with scenarios (with plots) for each of the following:     

1. Non-significant p, high R2      
2. Significant p, low R2       
3. Significant p, high R2     
4. Non-significant p, low R2      


## Linear regression vs correlation

When you run a simple linear regression you are assuming that one variable is your response variable and the other variable is a predictor variable. You are essentially modeling only one of these variables (the response) as a random variable and the other (the predictor variable) as a set of fixed values (a fixed effect). 


### Assessing correlation in R

In R you can use the `cor` function (part of base R) to assess correlation. To run a correlation test (assess if the correlation is 'significant') you can use the 'cor.test' function. By default, the `cor.test()` function runs a Pearson correlation test, which is a parametric test that assumes both variables are normal, no outliers, etc. If you want you can run a non-parametric version- the Spearman's correlation (or Kendall's tau). 

```{r}

# assess correlation -----------------------

## load the 'mtcars' data set

data(mtcars)

## define which variables to assess correlation for.

myvars <- c("disp","hp","wt")

## grab only the variables of interest

mtcars.reduced <- mtcars[,myvars]

## compute (pearson) correlations for all pairs of variables (correlation matrix)

cor.mat <- cor(mtcars.reduced)

cor.mat

## visualize correlations with the 'pairs' function

pairs(mtcars.reduced)

## run a correlation test- with 95% confidence interval
    # default is Pearson product-moment correlation.

cor.test(mtcars$disp,mtcars$wt)

## now try a non-parametric version

cor.test(mtcars$disp,mtcars$wt, method = "kendall") # or spearman (kendall generally preferred)


```

## Multiple linear regression

Multiple, or multi-variable, linear regression simply refers to the case where you have more than one predictor variable. 
NOTE: avoid using the term 'multivariate' because that often refers to the case where you have multiple **response** variables. 

If we were running a multiple linear regression with the variables "disp","hp","wt" from the `mtcars` dataset as predictor variables (see above), we might be somewhat concerned about multicollinearity. What is that, you ask? 

### Multicollinearity (for multiple linear regression)

If you have multiple predictor variables, you should first check that your predictor variables are not too correlated with one another. If the correlation between all pairs of quantitative covariates is less than around 0.7 or 0.8 you should generally not have a big problem with multicollinearity (although you might still check your variance inflation factors!). If you have a major problem with multicollinearity, you should generally use only one variable out of every pair of highly correlated variables for your final model. 

You might also consider "penalized regression" approaches like 'ridge regression' or 'lasso regression'. This, however, is outside the scope of this class!

#### Variance Inflation Factors

Variance Inflation Factors (VIFs) are a diagnostic tool to help you evaluate if you have a multicollinearity problem. In general VIFs less than 4 are considered okay. On the other hand, VIFs greater than 5 or 10 indicate problems!

To compute variance inflation factors, you can use the 'vif' function in the 'car' package. Let's try this using the trees dataset!

```{r}

# variance inflation factors ---------------------

###
# load the car package

library(car)

###
# load the trees dataset

data(trees)

###
# visualize relationships among predictor vars

pairs(trees[,c("Girth","Height")])


###
# check for correlation in predictor variables

cor(trees[,c("Girth","Height")])   # predictor variables not highly correlated

# run a multiple regression model

my.mod <- lm(Volume~Girth+Height,data=trees)


# check variance inflation factors

car::vif(my.mod)

```
Here it looks like we don't have to worry!

What about if we use the 'mtcars' dataset? It seemed there were stronger correlations among predictor variables in that case!

```{r}

###
## mtcars multicollinearity example

mymod <- lm(mpg~disp+hp+wt,data=mtcars) 

vif(mymod)

```

Now we are in the danger zone, and we might consider reducing our set of variables. First we might find our most highly correlated pair of variables and eliminate one of these!

```{r}

cor(mtcars.reduced)

#  alternative: use "findCorrelation" from caret package

## let's remove the 'disp' variable and keep weight...

mymod <- lm(mpg~hp+wt,data=mtcars) 

vif(mymod)

```

Looks much better now!!!


## What if assumptions are violated?

Let's look at what to do if some of the basic assumptions of linear regression are violated?

### Non-linearity

If we suspect a non-linear functional relationship between our response and predictor variables, we might perform a **non-linear regression** analysis. 

We can perform a non-linear regression using the `nls()` function in R (for non-linear least squares). Let's use the "DNase" dataset in R to explore this.

```{r}

# ?nls

data(DNase)

plot(DNase$density~DNase$conc)   # looks non-linear!

### run linear regression model

model1 <- lm(density~conc,data=DNase)

### run diagnostic plots

par(mfrow=c(2,2))
plot(model1)     # clear non-linearity (and non-normal residuals, and heteroskedasticity!)

par(mfrow=c(1,1))      # plot data with regression line - obvious issues!
plot(DNase$density~DNase$conc)
abline(model1)

### run non-linear regression model - use saturation curve

model2 <- nls(density ~ (max*conc)/(K+conc),data=DNase,start=list(max=2,K=1))
summary(model2)

### run non-linear regression model - use logistic function 

model3 <- nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase,
                 start = list(Asym = 3, xmid = 0, scal = 1))

summary(model3)

plot(DNase$density~DNase$conc)
abline(model1)
concs.toplot <- seq(0.01,14,0.1)
densities.toplot <- predict(model2,newdata=data.frame(conc=concs.toplot))
lines(concs.toplot,densities.toplot,col="blue")
densities.toplot <- predict(model3,newdata=data.frame(conc=concs.toplot))
lines(concs.toplot,densities.toplot,col="red")

legend("topleft",lty=c(1,1,1),col=c("black","blue","red"),legend=c("linear",
                  "saturation","logistic"))


```

You can run residual plots with non-linear regression just like you can with ordinary linear regression:

```{r}

## residual plots

resids <- DNase$density-predict(model3)

residuals(model3)   # alternative method! This works for 'lm' objects as well (and many other model objects)

## check for normality

qqnorm(resids)
shapiro.test(resids)


## check for heteroskedasticity
plot(resids~predict(model3))

```


### Heteroskedasticity

If you have severely heteroskedastic residuals, you can sometimes run a **transformation** of your response variable that improves the situation.

Alternatively, you might run a **generalized linear model (GLM)** that doesn't assume homoskedasticity (e.g., run a model that assumes the residuals are gamma distributed, or Poisson distributed). In a gamma or Poisson distribution, the residuals are expected to get larger as the expected mean value gets larger!

We will talk more about GLM soon. But for now, let's run an example with a transformed response variable:

```{r}
###
# heteroskedasticity

### first, simulate data with heteroskedastic residuals

simulated.x <- runif(100,0.1,5)
simulated.y <- exp(rnorm(100,1.1+0.3*simulated.x,0.7))

plot(simulated.y~simulated.x)

### run linear model

model1 <- lm(simulated.y~simulated.x)
par(mfrow=c(2,2))
plot(model1)  # run diagnostic plots    heteroskedasticity issues


### Take a minute to test for normality of residuals!

### run linear model with log transformation of response variable

model2 <- lm(log(simulated.y)~simulated.x)
par(mfrow=c(2,2))
plot(model2)  # run diagnostic plots - no issues!

# can also run "bptest" in "lmtest" package to test for violation of homoskedasticity

```

### Non-independence of observations

If you have a known source of non-independence in your observations, you can try to thin (rarefy) your data by throwing away non-independent observations (e.g., keeping only one out of every set of known non-independent observerations). 

There are also more advanced regression models that explicitly account for sources of non-independence:

*Regression with phylogenetic correction* -- use this if phylogenetic relatedness among your sampling units is the main source of non-independence

*Autoregression (more generally, ARIMA-type time series modeling)* -- use this if the main source of non-independence is temporal- observations taken closer together in time are more likely to be related

*Spatial autoregression* (more generally, spatial regression models) -- use this if the main source of non-independence is spatial -- observations taken closer together in space are more correlated.  

These models are outside the scope of this class, but it's useful to know they exist!

### Non-normality of residuals

First of all, regression analysis is somewhat robust to non-normality of the residuals- due to the CLT! 

But if your residuals are highly non-normal AND you can't seem to correct this using non-linear regression, transformations, or GLM, you might need to use a non-parametric alternative. 

One such alternative is to run a non-parametric correlation analysis such as a Spearman rank correlation test. 

## Regression and ANOVA

Before we move on to ANOVA, I just wanted to point out the essential similarity between regression and ANOVA. The only difference between regression and ANOVA is that in regression, the response variable is numeric/continuous and in ANOVA your response variable(s) is categorical. 

How similar are regression and ANOVA? Well, we can fit regression and ANOVA models with the `lm()` function, the assumptions are essentially the same, and the diagnostic plots are essentially the same. 

There are differences- we will touch on this in the next lecture- but for now let's focus on the similarities. Let's run an ANOVA in R using the same approaches we have just learned for linear regression!

In this example we will use the famous iris dataset.

```{r}

## ANOVA as regression example

data(iris)    # load the iris dataset

plot(iris$Sepal.Length ~ iris$Species)   # r uses a boxplot by default for categorical predictor

my.mod <- lm(Sepal.Length~Species,data=iris)    # run an ANOVA!
summary(my.mod)    # look at the results

anova(my.mod)     # produce an analysis of variance table

###
# alternative!

my.mod <- aov(Sepal.Length~Species,data=iris)   # same model!!
summary(my.mod)     # but produces an anova table by default


```


[--go to next lecture--](LECTURE7.html) 

















