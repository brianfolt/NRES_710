<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 710" />


<title>Intro to multi-variable modeling</title>

<script src="site_libs/header-attrs-2.27/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 710</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Syllabus</a>
</li>
<li>
  <a href="schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="lecture_1.html">Syllabus and the Purpose of Statistics</a>
    </li>
    <li>
      <a href="lecture_2.html">P-value Discussion and Intro to R</a>
    </li>
    <li>
      <a href="lecture_3.html">Sampling Uncertainty</a>
    </li>
    <li>
      <a href="lecture_4.html">Linear Regression</a>
    </li>
    <li>
      <a href="lecture_5.html">Linear Regression - results</a>
    </li>
    <li>
      <a href="lecture_6.html">Linear Regression - assumptions</a>
    </li>
    <li>
      <a href="lecture_7.html">Linear Regression - predictions</a>
    </li>
    <li>
      <a href="lecture_8.html">Analysis of Categorical Data - 2 groups</a>
    </li>
    <li>
      <a href="lecture_9.html">Analysis of Categorical Data - &gt;2 groups</a>
    </li>
    <li>
      <a href="lecture_10.html">Analysis of Categorical Data - posthoc tests</a>
    </li>
    <li>
      <a href="lecture_11.html">Analysis with Continuous or Categorical X?</a>
    </li>
    <li>
      <a href="lecture_12.html">Multi-variable Modeling</a>
    </li>
    <li>
      <a href="lecture_13.html">Multi-variable Modeling - collinearity</a>
    </li>
    <li>
      <a href="lecture_14.html">Multi-variable Modeling - interactions</a>
    </li>
    <li>
      <a href="lecture_15.html">Generalized Linear Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="exercise_1.html">Exercise 1 - Data Summary Functions</a>
    </li>
    <li>
      <a href="exercise_2.html">Exercise 2 - Linear Regression</a>
    </li>
    <li>
      <a href="exercise_3.html">Exercise 3 - Linear Regression (cont.)</a>
    </li>
    <li>
      <a href="exercise_4.html">Exercise 4 - Analysis of Categorical Data</a>
    </li>
    <li>
      <a href="exercise_5.html">Exercise 5 - Multi-variable Analysis</a>
    </li>
    <li>
      <a href="exercise_6.html">Exercise 6 - TBD</a>
    </li>
    <li>
      <a href="exercise_7.html">Exercise 7 - TBD</a>
    </li>
    <li>
      <a href="exercise_8.html">Exercise 8 - TBD</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    More Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Links.html">Links</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Intro to multi-variable modeling</h1>
<h4 class="author">NRES 710</h4>
<h4 class="date">Last compiled: 2024-08-21</h4>

</div>


<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>
<div id="review-of-class-to-date" class="section level2">
<h2>Review of class to-date!</h2>
<p>We have gotten quite a bit done in class so far this semester! We are
at a good point to review what we have covered.</p>
<p>Many introductory statistics class for graduate students cover four
types of analysis: t-tests, ANOVA, post-hoc tests, and linear
regression. We have covered all of those topics so far as well – but by
using the linear regression model as a vehicle to do all of those
different tests but with one cohesive framework.</p>
<p>I also hope that we have emphasized that the purpose of statistics is
<strong>not</strong> to generate <strong>p-values</strong> or
<strong>statistically significant results</strong>. Rather, the purpose
of statistics is to use <strong>facts</strong> to estimate
<strong>truth</strong>. The truth we are trying to estimate is a
relationship in nature, a relationship between some X-variable and some
Y-variable.</p>
<ul>
<li>If the X-variable is continuous, we want to know what the slope is:
how much does Y change for each unit change in X.</li>
<li>If the X-variable is categorical, we want to understand the
difference between those groups. For example, consider a drug used for
medical treatment. We want to know, for example. We don’t want just know
whether there is a statistical differenc between treatment groups. We
want to know what the <strong>effect</strong> of that drug is.</li>
</ul>
<p>One convenient feature of what we have learned so far is that all of
X-variables can be included in this general linear model:</p>
<p><strong><span class="math inline">\(Y = \beta_0 + \beta_1 X_1 +
\epsilon \sim N(0, \sigma)\)</span></strong></p>
<p>This model can apply to both cases, with either a continuous or
categorical X-variable.</p>
<ul>
<li>With a continuous X-variable, <span
class="math inline">\(\beta_1\)</span> is the slope.</li>
<li>With a categorical X-variable, <span
class="math inline">\(\beta_1\)</span> is the difference between the
groups.
<ul>
<li>If you have more than two groups, you will need extra X-variables
and betas, which can be accomplished by dummy-coding using values of 1
and 0 to assign group ownership.</li>
</ul></li>
<li><span class="math inline">\(\beta_0\)</span> is always the average Y
value when all other X-variables are 0.
<ul>
<li>For categorical X-variable, <span
class="math inline">\(\beta_0\)</span> is the reference group</li>
<li>For a continuous X-variable, <span
class="math inline">\(\beta_0\)</span> is when X = 0.</li>
</ul></li>
</ul>
<p>Assumptions of the general linear model are:</p>
<ul>
<li>Continuous Y-variable</li>
<li>Residuals (i.e., the error in Y) is normally distributed</li>
<li>If X-variable is continuous, there must be a linear relationship
between X and Y (assumption does not apply if X-variable is
categorical)</li>
<li>Homescedasticity; i.e., variance is constant along the range of
X-values (however, t-tests and linear models can accommodate situations
with unequal variance)</li>
<li>There is no autocorrelation in the Y-variable</li>
</ul>
<p>And that’s pretty much what we have covered so far.</p>
<p>Does anybody have any questions or is there anything you are confused
about?</p>
</div>
<div id="multiple-x-variables" class="section level2">
<h2>Multiple X-variables</h2>
<p>I have focused much of the class to date on exploring this general
linear model and how we can <strong>extend it</strong> to accommodate
different circumstances. And a few times I may have mentioned that one
of these extensions is to accommodate situations where we have more than
one X-variable! These might be situations where we have several
categorical X-variables of interest, or several continuous X-variables
of interest, that we think all may influence the Y-variable. You could
analyze all those X-variables separately – and there is nothing wrong
with doing that. But, there are a lot of reasons why it would be better
to analyze all of these X-variables at the same time. And that’s exactly
what we will begin to learn about today.</p>
<p>Let’s extend the linear model to accommodate more than one
X-variable:</p>
<p><strong><span class="math inline">\(Y = \beta_0 + \beta_1 X_1 +
\beta_2 X_2 + \beta_3 X_3 + ... +\epsilon \sim N(0,
\sigma)\)</span></strong></p>
<p><span class="math inline">\(\beta_1\)</span> is a categorical
variable with two groups, <span class="math inline">\(\beta_2\)</span>
is a categorical variable with four groups, <span
class="math inline">\(\beta_3\)</span> is a continous variable, and so
on so forth.</p>
<p>In theory, you could have infinite effects! But in practice, with a
usual computer, you are restricted to having nine or ten X-variables in
the linear model.</p>
<p>And of course this depends on <strong>sample size.</strong> A good
<strong>rule of thumb</strong>: you will need 10 samples for every
X-variable in your model. If you have five X-variables, you should have
at least 50 observations (rows in your datafile).</p>
<p>A lot of people know that you can extend this model to include
multiple X-variables! But what a lot of people don’t know, or struggle
to recognize, is <em>why</em> you might do this…</p>
<p>What are the advantages of analyzing all of your X-variables at the
same time.</p>
</div>
<div id="six-reasons" class="section level2">
<h2>Six reasons</h2>
<p><strong>1. It is more <em>elegant</em></strong>.</p>
<ul>
<li>In this day and age, it is easier and more likely for you to publish
a paper if you say: “We ran a mixed-effects logistic regression model
with interactions and dealt with collinearity and random effect…” Rather
than to say: “we ran a t-test”. People often equate complicated
statistics with good science. Sometimes it does!</li>
<li>Othertimes, complicated statistics are not necessary. A
well-designed experiment does not require complicated statistics!</li>
<li>I would say, however, that a single, multi-variable analysis is
often much more <strong>concise</strong> than iterative versions of
univariate analyses – and I do see a lot of value in that.</li>
</ul>
<p><strong>2. Swamping – when the effect (<span
class="math inline">\(\beta\)</span>) of one variable is masked by
another</strong></p>
<ul>
<li>Swamping is automatically dealt with during multi-variable
analyses.</li>
<li>A rule in statistics (axiom) – every time you add an X into a model,
the p-values for the X-variables already in the model should go down.
The reason is because the p-values for your X-variables is determined by
your error. When you add more X-variables into your model, you get rid
of error, by explaining more noise. So, as a general rule, when you add
more variables to a model, the p-values for other variables go
down.</li>
<li>If the variable you added explains a lot, then the p-values for
other variables should go down a lot as well!</li>
</ul>
<p><strong>3. Collinearity – when X-variables are associated
(correlated) with each other</strong></p>
<ul>
<li>Collinearity is a big problem in ecology and natural resource
sciences.</li>
<li>Collinearity is automatically dealt with in multi-variable
models.</li>
<li>We will discuss this during the next class.</li>
</ul>
<p><strong>4. Interactions – the effect (<span
class="math inline">\(\beta\)</span>) of one X-variable depends on the
value of another X-variable</strong></p>
<ul>
<li>Let’s say you are studying the relationship between size and age in
two different species. You collect a bunch of data, and then you analyze
those two different relationships in separate analyses. This is a valid
approach.</li>
<li>However, what you have just done is that you have
<em>implicitly</em> assumed an interaction. You suggested that the slope
of the size-age relationship depends upon the species.</li>
<li>Instead, you could have analyzed them at the same time and shown
that those relationships are different.</li>
<li>Interactions are kinda… in vogue/popular?! People think it’s really
cool/interesting to find interactions and therefore it is useful to
examine these (?)</li>
<li>We will discuss these in two weeks.</li>
</ul>
<p><strong>5. Blocking variables (random variables)</strong></p>
<ul>
<li>Similar to swamping, something that explains noise.</li>
<li>Sometimes we are interested in examining a core process in nature,
and we have to measure things <strong>in different areas</strong>. We
know that three different forests are different, but examining the
differences in the forests areas is not our main purpose. We don’t
really care that the forests are different from eachother, but we want
to account for the fact that the forests are different from eachother.
We can use blocking or random variables to account for that random
variation among sites, areas, forests, plots, whatever that are not
relevant to our core process of interest.</li>
<li>We’ll talk about this at the end of the semester.</li>
</ul>
<p><strong>6. Pseudoreplication – a process of assuming that you have
more independent samples than you really do</strong></p>
<ul>
<li>This is a problem in the natural sciences that many folks know
about, but very few people understand.</li>
<li>A topic for the final class or two in the semester.</li>
</ul>
</div>
<div id="how-multi-variable-models-work" class="section level2">
<h2>How multi-variable models work</h2>
<p><strong><span class="math inline">\(Y = \beta_0 + \beta_1 X_1 +
\beta_2 X_2 + \epsilon \sim N(0, \sigma)\)</span></strong></p>
<p><strong>Y = Size</strong><br />
<strong><span class="math inline">\(X_1\)</span> = Age</strong>
(continuous)<br />
<strong><span class="math inline">\(X_2\)</span> = Sex</strong>
(categorical)</p>
<p>This model has traditionally been called ‘<strong>Analysis of
Covariance (ANCOVA)</strong>’.</p>
<p>If you have more than one categorical X, that model has historically
been called a ‘<strong>Multi-factor ANOVA</strong>’.</p>
<p>I don’t expect you to know these terms, because I won’t use them. But
you might hear them or see them, so giving them here for context…</p>
<p>If you have more than one continuous X, that model has historically
been called a ‘<strong>Multiple Regression</strong>’ – as opposed to our
simple linear regression with one X-variable.</p>
<p>However, we can call them all <strong>‘General Linear
Models’</strong> – much easier!</p>
<p><strong><span class="math inline">\(Y = \beta_0 + \beta_1 Age +
\beta_2 Sex + \epsilon \sim N(0, \sigma)\)</span></strong></p>
<p><strong>Y = Size</strong><br />
<strong><span class="math inline">\(X_1\)</span> = Age</strong>
(continuous)<br />
<strong><span class="math inline">\(X_2\)</span> = Sex</strong>
(categorical; male = 1)</p>
<p>Important point: when we run this model, it does not change the
meanings of the betas at all! They are still interpretted the same ways
as we have interpretted them before.</p>
<ul>
<li>The meaning of <span class="math inline">\(\beta_1\)</span> is the
effect (slope) of age on size – or the slope of the age-size
relationship</li>
<li>The meaning of <span class="math inline">\(\beta_2\)</span> is the
difference between male and females (or, the effect of being male
compared to the reference, female).</li>
</ul>
<p>The fact that we have an age effect in the model does not change our
interpretation of the sex effect – or vice versa!</p>
<p>So, this ‘more complicated model’ does not make it harder to interpet
these betas – they still have the same meaning!</p>
<p>Let’s see what this looks like.</p>
</div>
<div id="conceptual-example" class="section level2">
<h2>Conceptual example</h2>
<p>Continuing the size (Y) by age (X) and sex (X) example, consider this
plot of data:</p>
<p><img src="lecture_12_files/figure-html/example1-1.png" width="864" /></p>
<p>One of the things I want to point out is that… we can run a simple
regression between age and size!</p>
<p><strong>Q:</strong> If I did that, where would the line be?
<strong>Right down the middle – between the two clusters of
data.</strong></p>
<p><strong>Q:</strong> Would we get a good estimate of the age-sex
relationship? <strong>Yes!</strong> It would be pretty good.</p>
<p><strong>Error:</strong> But, our <strong>error</strong> would be
pretty large**. We can indicate this with a normal curve overlayed on
top of the regression line and the spread of the data.</p>
<p>Now let’s consider what would happen if we run the multivariable
model. What’s actually going to happen is we are going to get
<strong>two lines!</strong> One for females, and one for males. Let’s
break up the model and examine this:</p>
<p><strong><span class="math inline">\(Y(female) = \beta_0 + \beta_1 Age
+ \cancel{\beta_2 * 0} + \epsilon \sim N(0,
\sigma)\)</span></strong></p>
<p><br></p>
<p><img src="lecture_12_files/figure-html/example2-1.png" width="864" /></p>
<p>What happens to the Beta2 * Sex term for females? It goes away,
because Sex = 0 for females. So now we have a line just for female
data!</p>
<p><strong>Error:</strong> Our <strong>error</strong> for this line will
be <strong>much smaller</strong> than for a line for all the data. We
can indicate this with smaller normal curves overlayed on top of the
data and regression line for females.</p>
<p>The Y-intercept for this line is Beta0, the slope for this line is
Beta1.</p>
<p>Now let’s examine this for males.</p>
<p><strong><span class="math inline">\(Y(male) = \beta_0 + \beta_1 Age +
\beta_2 * 1 + \epsilon \sim N(0, \sigma)\)</span></strong></p>
<p>Let’s rearrange the equation to move <span
class="math inline">\(\beta_2\)</span> over next to <span
class="math inline">\(\beta_0\)</span>. We can do that because these are
both just being added.</p>
<p><strong><span class="math inline">\(Y(male) = (\beta_0 + \beta_2) +
\beta_1 Age + \epsilon \sim N(0, \sigma)\)</span></strong></p>
<p>So basically what this is saying is that our intercept is now <span
class="math inline">\(\beta_0 + \beta_2\)</span>. In other words, the
intercept for the male equation is the female intercept (<span
class="math inline">\(\beta_0\)</span>) plus the difference of being
male (<span class="math inline">\(\beta_2\)</span>).</p>
<p>The slope for the male line is the same term as the slope for the
female line ($_1)!</p>
<p>We can also visualize this on our graph:</p>
<p><br></p>
<p><img src="lecture_12_files/figure-html/example3-1.png" width="864" /></p>
<p><strong>Error:</strong> As with the line for females, our
<strong>error</strong> for this line will be <strong>much
smaller</strong> than for a line for all the data. We can indicate this
with smaller normal curves overlayed on top of the data and regression
line for females.</p>
<p>Ultimately, by using this equation, we can capture all of the
meaningful variation in the data, and it does so by allowing for much
smaller error than more simple models.</p>
<p>The meaning of the betas has not changed. <span
class="math inline">\(\beta_1\)</span> is the slope for both lines, and
<span class="math inline">\(\beta_2\)</span> is the difference between
the groups. Notice that <span class="math inline">\(\beta_2\)</span> is
not just the difference between the groups at age of zero, it’s the
difference everywhere! It’s constant along the variation of the
continuous X-variable.</p>
<p><strong>Questions?</strong></p>
<p>Previously I mentioned one of the reasons we like multi-variable
analysis is it automatically accounts for <strong>swamping</strong>. For
example, if we run univariate analysis (a model with one X-variable), we
may not observe a statistically significant result because a ton of
variation in the system is explained by another X-variable. The result
is ‘swamped’ out by the other X-variable that we did not include.
<strong>The effect of one X-variable is masked by another</strong>.</p>
<p>By adding in multiple X-variables, multi-variable analysis
automatically deals with swamping.</p>
<p>Because we have decreased the error related to all of the terms in
our model, we have also decreased the p-values for each term.</p>
<p>It’s also entirely possible that if we had not included one of the
X-variables in our model, we might NOT have recovered good estimates of
the <span class="math inline">\(\beta\)</span> and instead failed to
reject the null hypothesis of no effect.</p>
<p>For example, looking back at our original line fit to all of the
data. We may have recovered a significant slope for this line, but our
would have been higher, our confidence intervals would have been larger,
and our estimate would not have been certain.</p>
<p>What if we wouldn’t have included age in the model?</p>
<p><img src="lecture_12_files/figure-html/example4-1.png" width="864" /></p>
<p><strong>Q:</strong> Are these groups going to be significantly
different from eachother?</p>
<p>Maybe… but there is a lot of noise here, and the groups overlap
eachother. A linear model of sex alone [e.g., lm(Size ~ Sex)] may not
return significance and thus we would fail to reject the null hypothesis
of no difference between groups.</p>
<p><strong>Takehome message:</strong> by accounting for how both age and
sex influences size in the model, our models will, in general, explain
more variation and better recovery real, statistically-significant
results. This helps minimize negative effects of swamping.</p>
</div>
<div id="multi-variable-models-in-r" class="section level2">
<h2>Multi-variable models in R</h2>
<div id="simple-analysis" class="section level3">
<h3>Simple analysis</h3>
<p>So let’s load up some data in R and take a look at it. The dataset is
located <a href="lecture_12_dataset1.csv">here</a>.</p>
<pre class="r"><code># Load in the data and take a look at it
datum &lt;- read.csv(&quot;lecture_12_dataset1.csv&quot;)

# Take a look at it
head(datum)</code></pre>
<pre><code>##        Age    Sex Male      Size
## 1 3.588198 Female    0  9.584951
## 2 8.094746 Female    0 16.119282
## 3 4.680792 Female    0 10.986892
## 4 8.947157 Female    0 18.515617
## 5 9.464206 Female    0 18.015692
## 6 1.410008 Female    0  7.328189</code></pre>
<pre class="r"><code>tail(datum)</code></pre>
<pre><code>##          Age  Sex Male      Size
## 95  3.883359 Male    1 11.043810
## 96  2.689220 Male    1 10.109105
## 97  8.040649 Male    1 17.391569
## 98  1.842355 Male    1  9.813866
## 99  5.201011 Male    1 15.981604
## 100 5.603549 Male    1 13.875699</code></pre>
<p>The data include:</p>
<ul>
<li>observations of animals of different ages (1-10 years old,
roughly)</li>
<li>different sexes (males and females)</li>
<li>a dummy-coded variable for sex (0 = female, 1 = male)</li>
<li>body size</li>
</ul>
<p>What was the <strong>Truth</strong> used to simulate these data? The
code to simulate these data are included at the bottom of this page.
But, here’s a quick summary:</p>
<p><strong>Truth:</strong></p>
<ul>
<li><span class="math inline">\(\beta_1\)</span> = 1.5 (age effect)</li>
<li><span class="math inline">\(\beta_2\)</span> = 2.5 (beta for sex;
a.k.a., the effect of being male)</li>
<li><span class="math inline">\(\beta_0\)</span> = 4 (y-intercept for
reference group)</li>
<li><span class="math inline">\(\sigma\)</span> = 0.8</li>
</ul>
<p>Let’s plot it! Start with the effect of sex on size.</p>
<pre class="r"><code># Plot the effect of sex on size
plot(Size ~ as.factor(Sex), data = datum)</code></pre>
<p><img src="lecture_12_files/figure-html/analysis_2-1.png" width="384" /></p>
<p>Yeesh! A lot of noise here, a lot of interlap between our two groups.
This is not particularly useful.</p>
<p>Let’s look at the effect of age on size.</p>
<pre class="r"><code># Plot the effect of sex on size
plot(Size ~ Age, data = datum)</code></pre>
<p><img src="lecture_12_files/figure-html/analysis_3-1.png" width="384" /></p>
<p>Can we see a difference between males and females? Not really. Maybe
we could if we plotted the points using different colors or point types
for each sex. But there is not a clear separation of points suggestion
of two groups looking at it like this.</p>
<p>Let’s now start by running a regression between <strong>sex</strong>
and <strong>size</strong>.</p>
<pre class="r"><code># Simple regression between size and sex 
results1 &lt;- lm(Size ~ Sex, data = datum)
summary(results1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Size ~ Sex, data = datum)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.9825 -3.1260 -0.1583  3.3057  7.3875 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  12.6384     0.5512  22.930   &lt;2e-16 ***
## SexMale       1.5984     0.7795   2.051    0.043 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.897 on 98 degrees of freedom
## Multiple R-squared:  0.04114,    Adjusted R-squared:  0.03136 
## F-statistic: 4.205 on 1 and 98 DF,  p-value: 0.04297</code></pre>
<p>Remember, truth for the effect of being male, <span
class="math inline">\(\beta_2\)</span>, was 2.5. Our results here
suggest that this effect is 1.5. But it is statistically significant –
although barely (p = 0.04). Our confidence limits are about ~1.5 (2 *
SE). So our best guess from this model is that males are about 1.5 kg
larger than females (+/- 1.5). This is a pretty noisy estimate – but it
does contain truth, which was 2.5.</p>
<p>Notice the residual standard error is about ~4. Truth was 1! So this
is about four times larger than truth. There is a lot of extra noise
(error) in this simple model! Noise is often not just random variation,
but there might be other information that might explains that noise in
size. In this case, a lot of the variation in size is driven by age,
while very little information is driven by sex.</p>
<p><strong>Q:</strong> How much variation in size is explained by sex?
<strong>Only 4%!</strong> The multiple <span
class="math inline">\(R^2\)</span>.</p>
<p>Let’s now start by running a regression between <strong>age</strong>
and <strong>size</strong>.</p>
<pre class="r"><code># Plot
plot(Size ~ Age, data = datum)</code></pre>
<p><img src="lecture_12_files/figure-html/analysis_5-1.png" width="384" /></p>
<pre class="r"><code># Simple regression between size and age 
results2 &lt;- lm(Size ~ Age, data = datum)
summary(results2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Size ~ Age, data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.97536 -1.08735 -0.01096  1.07418  2.97800 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  5.45489    0.31583   17.27   &lt;2e-16 ***
## Age          1.45482    0.05219   27.88   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.332 on 98 degrees of freedom
## Multiple R-squared:  0.888,  Adjusted R-squared:  0.8869 
## F-statistic:   777 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The effect of Age is 1.45 (+/- 0.10), which is <strong>great</strong>
because it is very close to and overlaps with truth (1.5)! This is also
highly significant, P = 2x10^-16.</p>
<p>We don’t really need to include sex in the model to get a good
estimate of the effect of age. Why…?</p>
<p>Because age explains 88% of the variation in size! We get pretty good
estimates of this effect of age on size without including information on
sex. However, if we do, this R^2 value will increase also!</p>
<p>Let’s run the full model.</p>
<pre class="r"><code># Simple regression between size and age 
results3 &lt;- lm(Size ~ Age + Male, data = datum)
summary(results3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Size ~ Age + Male, data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.94453 -0.48216 -0.03362  0.43280  1.88202 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.18997    0.20133   20.81   &lt;2e-16 ***
## Age          1.48718    0.02994   49.68   &lt;2e-16 ***
## Male         2.17478    0.15281   14.23   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7618 on 97 degrees of freedom
## Multiple R-squared:  0.9637, Adjusted R-squared:  0.963 
## F-statistic:  1289 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Q:</strong> What are we seeing that is different about this
output than the previous two outputs?</p>
<ul>
<li>The effect of age is much closer to truth! And still highly
significant.</li>
<li>The effect of male is now much closer to truth!
<ul>
<li>The simple sex model had a p-value = 0.04, but now it’s 2e-16! We
drastically increased our ability to estimate this significant
effect.</li>
</ul></li>
<li>The y-intercept for the reference group (females) is closer to truth
(4.18 vs. 4).</li>
<li><span class="math inline">\(R^2\)</span> value jumped up to 0.96! We
are now explaining more information in size than either of the previous
two simple models (0.04, 0.88) combined. This model, as a whole,
explains 96% of the variation in size.</li>
<li>Standard error estimate is much closer! 0.76 is closer to truth
(0.80).</li>
</ul>
<p>This is a good illustration of <strong>swamping</strong>. Swamping
occurs when the effect of one variable is drowned out by another
variables effect. Univariate analysis may fail to recover all of the
significant effects; our ability to measure statistically significant
effects can be swamped out by other variables. By including all of these
variables in a single model, we can correctly account for variation
explained by different variables and appropriately measure and identify
these statistically-significant effects.</p>
<p>This is an advantage of using multi-variable analysis: by analyze all
of the variables together, it improves our ability to estimate
truth!</p>
<p>NOTE: something weird is happening. Even now with this improved
model, our effect of age (2.17; +/- 0.3, 95% CI) does not overlap with
truth!!</p>
<p>Sometimes this happens due to random chance during data simulation!
Basically, the simulation process ended up simulating pretty extreme
data for the male effect in this case, due to random chance. Given our
sample size and effect size, our estimate does not overlap in truth,
even after correctly specifying the true model. This happens maybe 1 out
of every 20 simulations. That’s just part of the randomness of data and
science!</p>
<p><strong>Reporting results</strong></p>
<p>One sentence for every X-variable, and those sentences will be the
same as we have used before.</p>
<p>For the continuous X-variable, provide a results sentence for a
continuous variable. “For each 1 year in increase in age, we observed a
1.49 (+/-0.06; +/-95% CI) kg increase in body size (P &lt; 2e-16).”</p>
<p>For the categorical X-variable, provide a results sentence for that
variable illustrating the difference between sexes. “We also found that
body size of males was 2.17 (+/-0.30; +/-95% CI) kg heavier than females
(P &lt; 2e-16).”</p>
<p>One sentence for each variable, and the same sentences we already
know!</p>
</div>
<div id="more-complex-analysis" class="section level3">
<h3>More complex analysis</h3>
<p>Let’s assume the same dataset as before, but we now have a third
group: hermaphrodites. Individual animals with sexual reproductive
organs for both sexes. The dataset is located <a
href="lecture_12_dataset2.csv">here</a>.</p>
<pre class="r"><code># Load in the data and take a look at it
datum &lt;- read.csv(&quot;lecture_12_dataset2.csv&quot;)

# Take a look at it
head(datum)</code></pre>
<pre><code>##        Age    Sex Male Herma      Size
## 1 8.061177 Female    0     0 16.139566
## 2 1.084869 Female    0     0  5.063627
## 3 8.011593 Female    0     0 15.443615
## 4 7.564516 Female    0     0 16.054494
## 5 6.671187 Female    0     0 13.194306
## 6 5.328197 Female    0     0 13.556531</code></pre>
<pre class="r"><code>tail(datum)</code></pre>
<pre><code>##          Age           Sex Male Herma     Size
## 145 8.798210 Hermaphrodite    0     1 21.82848
## 146 7.377167 Hermaphrodite    0     1 21.99917
## 147 7.843596 Hermaphrodite    0     1 19.44455
## 148 2.323757 Hermaphrodite    0     1 12.11445
## 149 4.222513 Hermaphrodite    0     1 15.99407
## 150 7.059992 Hermaphrodite    0     1 19.99809</code></pre>
<pre class="r"><code># Make sure Sex is a factor
datum$Sex &lt;- as.factor(datum$Sex)</code></pre>
<p>I simulated these data using the same ‘truth’ as before for males and
females; males were 2.5 kg larger than females on average. But now I
simulated the third hermaphrodite group where hermaphrodites are 5 kg
larger than females on average.</p>
<pre class="r"><code># Plot the data
plot(Size ~ as.factor(Sex), data = datum)</code></pre>
<p><img src="lecture_12_files/figure-html/analysis_2_2-1.png" width="384" /></p>
<pre class="r"><code># Plot again
plot(Size ~ Age, data = datum)</code></pre>
<p><img src="lecture_12_files/figure-html/analysis_2_2-2.png" width="384" /></p>
<p>Let’s jump straight to the full model:</p>
<pre class="r"><code># Multi-variable analysis
results &lt;- lm(Size ~ Age + Sex, data = datum)
summary(results)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Size ~ Age + Sex, data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.95435 -0.53198  0.03605  0.54930  2.06974 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       3.90356    0.18905   20.65   &lt;2e-16 ***
## Age               1.49374    0.02724   54.84   &lt;2e-16 ***
## SexHermaphrodite  5.29954    0.16797   31.55   &lt;2e-16 ***
## SexMale           2.52226    0.16803   15.01   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8398 on 146 degrees of freedom
## Multiple R-squared:  0.9644, Adjusted R-squared:  0.9636 
## F-statistic:  1317 on 3 and 146 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>So our interpretation of this is essentially the same as our
interpretation before.</p>
<ul>
<li>We have a continuous effect of age, which is different due to random
variables but still a good estimate.</li>
</ul>
<p><strong>Q:</strong> What is the interpretation for the ‘SexHerma’
effect? <strong>Difference between hermaphrodites and females</strong>.
We would use our standard sentence to explain that.</p>
<p><strong>Q:</strong> What is the interpretation for the ‘SexMale’
effect? <strong>Difference between males and females</strong>. Again,
standard sentence to explain that.</p>
<p><strong>Q:</strong> What if we want to know the difference between
males and hermaphrodites? <strong>Change the reference</strong></p>
<p>Let’s do that real quick.</p>
<pre class="r"><code># Use the relevel function to change order of sex
results2 &lt;- lm(Size ~ Age + relevel(Sex, ref = &quot;Male&quot;), data = datum)
summary(results2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Size ~ Age + relevel(Sex, ref = &quot;Male&quot;), data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.03163 -0.50182 -0.03787  0.59082  2.01269 
## 
## Coefficients:
##                                         Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                              6.61063    0.18105   36.51   &lt;2e-16
## Age                                      1.50211    0.02573   58.37   &lt;2e-16
## relevel(Sex, ref = &quot;Male&quot;)Female        -2.60280    0.16290  -15.98   &lt;2e-16
## relevel(Sex, ref = &quot;Male&quot;)Hermaphrodite  2.42728    0.16290   14.90   &lt;2e-16
##                                            
## (Intercept)                             ***
## Age                                     ***
## relevel(Sex, ref = &quot;Male&quot;)Female        ***
## relevel(Sex, ref = &quot;Male&quot;)Hermaphrodite ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8145 on 146 degrees of freedom
## Multiple R-squared:  0.9675, Adjusted R-squared:  0.9669 
## F-statistic:  1451 on 3 and 146 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>I haven’t changed anything about age, but just replaced sex with the
relevel function inside our linear model.</p>
<p>And voila! Here is our difference between hermaphrodites and
males.</p>
<pre class="r"><code># Back to original results
summary(results)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Size ~ Age + Sex, data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.95435 -0.53198  0.03605  0.54930  2.06974 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       3.90356    0.18905   20.65   &lt;2e-16 ***
## Age               1.49374    0.02724   54.84   &lt;2e-16 ***
## SexHermaphrodite  5.29954    0.16797   31.55   &lt;2e-16 ***
## SexMale           2.52226    0.16803   15.01   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8398 on 146 degrees of freedom
## Multiple R-squared:  0.9644, Adjusted R-squared:  0.9636 
## F-statistic:  1317 on 3 and 146 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>What if I asked you to give a p-value for the significance of the sex
variable as a whole? Before, we used the p-value in the bottom-right
hand output of our results. But now this new modeling approach also has
a continuous X-variable involved, so this p-value now relates to the
significance of our larger multi-variable modeling approach, which
includes both age and sex.</p>
<p>You could do it this way:</p>
<pre class="r"><code># ANOVA
anova(results)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Size
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## Age         1 2084.29 2084.29 2955.66 &lt; 2.2e-16 ***
## Sex         2  702.46  351.23  498.07 &lt; 2.2e-16 ***
## Residuals 146  102.96    0.71                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>But you shouldn’t – and here’s why.</p>
<p>The ANOVA function in R gives you Type I Sum of Squares.</p>
<p><strong>Type I p-values – sequential – such that, order
matters.</strong></p>
<p>When it runs the F-drop test for age: it compares two models:</p>
<ul>
<li><strong>Age: <span class="math inline">\(\beta_0 + \beta_1
Age\)</span> vs. <span
class="math inline">\(\beta_0\)</span></strong></li>
<li><strong>Sex: <span class="math inline">\(\beta_0 + \beta_1 Age +
\beta_2 Sex\)</span> vs. <span class="math inline">\(\beta_0 + \beta_1
Age\)</span></strong></li>
</ul>
<p>So when it tests for the effect of the sex variable, it asks how much
does sex improve the model? Does adding sex significantly improve the
model beyond having age alone. It does the same for age, but it tests
having age in the model vs. having nothing.</p>
<p>I would prefer this if it compared the full model with sex to the
full model without sex – and <em>order didn’t matter</em>.</p>
<p>Let’s see what happens if we switch this around.</p>
<pre class="r"><code># Switch order of effect
results3 &lt;- lm(Size ~ Sex + Age, data = datum)
anova(results3)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Size
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## Sex         2  626.76  313.38  472.35 &lt; 2.2e-16 ***
## Age         1 2260.26 2260.26 3406.85 &lt; 2.2e-16 ***
## Residuals 146   96.86    0.66                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The F-values change between these two outputs, which means our
results and resulting inference might change. I don’t like this because
the data are the same, and the models are the same! If we have the same
data and the same model, the results and inference should be the same.
Order of model parameters should not change p-values and results!</p>
<p>The sequential nature of p-values in this multi-variable ANOVA is
another reason why I am not a big fan of the canned ‘ANOVA’-style
analyses. These ANOVA functions use Type I Sum of Squares to calculate
p-values, and we are better off avoiding this.</p>
<p>Instead, we want to use Type III Sum of Squares, where it always does
an F-drop test.</p>
<p><strong>Q:</strong> Does anybody have an idea about how we might
evaluate the significance of the sex variable <em>as a whole</em>?</p>
<p>Hint: we learned how to do this last class…</p>
<p>We can build a more complex and a more simple models ourselves, and
then compare them ourselves, using an F-drop test.</p>
<pre class="r"><code># Simple model
results4 &lt;- lm(Size ~ Age, data = datum)

# More complex model
results5 &lt;- lm(Size ~ Age + Sex, data = datum)

# F-drop test
anova(results4, results5)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Size ~ Age
## Model 2: Size ~ Age + Sex
##   Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    
## 1    148 729.66                                 
## 2    146  96.86  2     632.8 476.9 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>This p-value is the significance of our Sex variable as a whole.</p>
<p>It is the same p-value and result that we saw before when we did
‘anova(lm(Size + Age + Sex))’, but only because we had sequentially
listed ‘age then sex’ did that analysis compare the ‘Age’ model vs ‘Age
+ Sex’ model.</p>
<p>The point is we can remove the ambiguity of sequential p-values and
force the analysis to use the Type III Sum of Squares and compare the
more complicated and more simple models manually, as we have shown
here.</p>
<p>Takehome: if you are performing a multi-variable analysis and want to
know the significance of an entire categorical variable with more than
two groups, use an F-drop test.</p>
<p>Another thing I want to show you is what happens if you try to use a
Tukey post-hoc test for this model.</p>
<pre class="r"><code># Model
results6 &lt;- lm(Size ~ Sex + Age, data = datum)
summary(results6)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Size ~ Sex + Age, data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.03163 -0.50182 -0.03787  0.59082  2.01269 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)       4.00784    0.18114   22.13   &lt;2e-16 ***
## SexHermaphrodite  5.03008    0.16290   30.88   &lt;2e-16 ***
## SexMale           2.60280    0.16290   15.98   &lt;2e-16 ***
## Age               1.50211    0.02573   58.37   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8145 on 146 degrees of freedom
## Multiple R-squared:  0.9675, Adjusted R-squared:  0.9669 
## F-statistic:  1451 on 3 and 146 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># Tukey
results6 &lt;- aov(results6)
TukeyHSD(results6, which = &quot;Sex&quot;)</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = results6)
## 
## $Sex
##                           diff       lwr       upr p adj
## Hermaphrodite-Female  5.005883  4.620149  5.391618     0
## Male-Female           2.595707  2.209972  2.981441     0
## Male-Hermaphrodite   -2.410176 -2.795911 -2.024442     0</code></pre>
<p>The betas changed! Difference between females and males changed
between outputs.</p>
<p>Post-hoc tests should not change effects! They should only change
confidence intervals and p-values.</p>
<p>I am not sure what happens here, but I think the ‘TukeyHSD()’ call
forced our multi-variable model to be a single X-variable model, with
just Age. It ignored Sex.</p>
<p>Post-hoc tests may not exist for your more complicated models! So
that is why we like to leverage the flexibility of the linear modeling
framework with ‘lm()’.</p>
</div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>You can run all of your variables at once in a multi-variable model!
The advantages include:</p>
<ul>
<li>Elegance (conciseness)</li>
<li>Swamping (does this automatically)</li>
<li>Collinearity (next class)</li>
<li>Interactions</li>
<li>Random variables</li>
<li>Pseudoreplication</li>
</ul>
<p>The last three, you have to do something to account for those – but
the first three are done automatically.</p>
<p>It does not change the meaning of the betas, and we can still report
them with our simple, concise sentences.</p>
<p>If you want to know the significance of categorical variables with
&gt;2 groups as a whole, we can test for it using the F-drop tests. If
there is only 2 groups, you can just see that from the ‘lm()’.</p>
<p><br></p>
</div>
<div id="truth" class="section level2">
<h2>Truth</h2>
<p>Here is code to simulate the data we analyzed in this lecture.</p>
<pre class="r"><code>### Lecture 12: code to simulate data for multi-variable analysis

# Set the seed for reproducibility &amp; set graphing parameter
set.seed(123); par(mar=c(4,4,0,0)); par(mfrow=c(1,2))

# First dataset
# X variable
n &lt;- 50
x1 &lt;- c(rep(&quot;Female&quot;, n), rep(&quot;Male&quot;, n))
x2 &lt;- runif(n * 2, 1, 10)
dummy &lt;- data.frame(model.matrix(~ x1 - 1))
colnames(dummy) &lt;- c(&quot;Female&quot;, &quot;Male&quot;)

# Simulate error
Error &lt;- rnorm(n * 2, 0, 0.8)

# Predict Y
Response &lt;- 4 + 1.5 * x2 + 2.5 * dummy$Male + Error

# Dataframe
datum &lt;- data.frame(Age = x2, Sex = x1, Male = dummy$Male, Size = Response)

# Save the data
write.csv(datum, &quot;lecture_12_dataset1.csv&quot;, row.names = FALSE)

# Second dataset
# X variable
n &lt;- 50
groups &lt;- 3
x1 &lt;- c(rep(&quot;Female&quot;, n), rep(&quot;Male&quot;, n), rep(&quot;Hermaphrodite&quot;, n))
x2 &lt;- runif(n * groups, 1, 10)
dummy &lt;- data.frame(model.matrix(~ x1 - 1))
colnames(dummy) &lt;- c(&quot;Female&quot;, &quot;Hermaphrodite&quot;, &quot;Male&quot;)

# Simulate error
Error &lt;- rnorm(n * groups, 0, 0.8)

# Predict Y
Response &lt;- 4 + 1.5 * x2 + 2.5 * dummy$Male + 5 * dummy$Hermaphrodite + Error

# Dataframe
datum &lt;- data.frame(Age = x2, Sex = x1, Male = dummy$Male, Herma = dummy$Hermaphrodite, Size = Response)

# Save the data
write.csv(datum, &quot;lecture_12_dataset2.csv&quot;, row.names = FALSE)</code></pre>
<p><a href="lecture_13.html">–go to next lecture–</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
