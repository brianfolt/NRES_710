---
title: "Multi-Variable Modeling - Interactions"
author: "NRES 710"
date: "Last compiled: `r Sys.Date()`"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
    css: styles.css
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
	)
```

```{r echo=FALSE}

#  NRES 710, Multi-Variable Modeling - Interactions
#     University of Nevada, Reno
#     Modeling interactions between X-variables

```

Any questions before we begin?

## Interactions

Today we are going to learn about **interactions**!

Last week I suggested that collinearity is critically important to understand, because it is a nuisance that we must account for to get unbiased estimates, and so we spent an entire week on the topic. This week, I think interactions are also critically important, so we will again spend a week on this topic. But interactions are actually really interesting and can greatly enhance the interest in our research and what we are learning.

What are interactions...? Let's start with an example.

- **Y -- reproductive output**
- **$X_1$ -- Placebo / Pill** -- a binomial variable describing groups in a clinical trials of whether the 'estrogen/progesterone/birth-control pill'.

**Q:** What is the effect of The Pill on reproductive output? ....does it cause reproduction to be ~0? *It depends....*

- **$X_2$ -- Sex**

The Pill reduces reproductive output of females to zero, but it does not influence the reproductive output of males. This is an interactions!

**interactions -- the effect (beta) of one X-variable depends on the value of another X-variable**

Sometimes people will implicitly assume there is an interaction, and separate out their data to analyze the effect of the pill on two groups separately: males and females. This is okay! BUT...

1. You are implicitly assuming there is an interaction, and,
2. if there is a difference, you cannot say the difference is statistically significant or not, because you did not test that.

Let's say that we find that the effect of the pill on males is some very small number close to ~0 (e.g., beta = 0.001), and then we get a large beta for the effect of the pill on females. How would we know that these effects are statistically different? Well, we can't know that if we analyzed these effects separately.

If you want to know if the effect is different between two groups (sexes, species, etc.), you need to test for an interaction.

## Example

Let's again consider an example we have seen before, that I like, because it is easy...

- **Y -- Size**
- **$X_1$ -- Age**
- **$X_2$ -- Sex**'

Previously we ran this multi-variable model: $Size = \beta_0 + \beta_1 * Size + \beta_2 * Sex$

This model is implicitly assuming that lines for each group are parallel -- they have the same slope:

```{r echo=FALSE, fig.height=4, fig.width=4.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Read in the size ~ age + sex data
datum <- read.csv("lecture_12_dataset1.csv")

# Model for females
results <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Female"))

# Model for males
results2 <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Male"))

# Determine the range of the x and y axes
xlim <- range(datum$Age)
ylim <- range(datum$Size)

# Make an empty plot with defined x and y limits
plot(xlim, ylim, type = "n", xlab = "Age", ylab = "Size")

# Add regression lines for females and males
abline(results, col = "red", lwd = 2) # Red line for females
abline(results2, col = "blue", lwd = 2) # Blue line for males

# Optionally, add a legend to distinguish between the two lines
legend("topleft", legend = c("Females", "Males"), col = c("red", "blue"), lwd = 2)

```

The effect of sex is the same for all ages, and the effect of age is the same for both sexes. This is an assumption that we have made. But this is *not* an assumption that we *have* to make...

What if we assume that these effects are different...?

```{r echo=FALSE, fig.height=4, fig.width=4.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Read in the size ~ age + sex + age*sex data
datum <- read.csv("lecture_14_dataset1.csv")

# Model for females
results <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Female"))

# Model for males
results2 <- lm(Size ~ Age, data = subset(datum, datum$Sex == "Male"))

# Determine the range of the x and y axes
xlim <- range(datum$Age)
ylim <- c(0, max(datum$Size))

# Make an empty plot with defined x and y limits
plot(xlim, ylim, type = "n", xlab = "Age", ylab = "Size")

# Add regression lines for females and males
abline(results, col = "red", lwd = 2) # Red line for females
abline(results2, col = "blue", lwd = 2) # Blue line for males

# Optionally, add a legend to distinguish between the two lines
legend("topleft", legend = c("Females", "Males"), col = c("red", "blue"), lwd = 2)

```

All individuals start out at about the same size, but then males grow faster and reach a larger size at an earlier age than females.

This is an interaction! The effect of age depends on sex. We get a different beta (slope) for the age-size relationship for males then we do for females.

Another interesting thing I want to point out is that for interactions, perspective does not matter:

- The effect of age depends on sex.
- The difference between the sexes also depends on age.

For example:

```{r echo=FALSE, fig.height=4, fig.width=4.5}
# Specify plot margins
par(mar=c(4,4,0,0))

# Make an empty plot with defined x and y limits
plot(xlim, ylim, type = "n", xlab = "Age", ylab = "Size")

# Add regression lines for females and males
abline(results, col = "red", lwd = 2) # Red line for females
abline(results2, col = "blue", lwd = 2) # Blue line for males

# Optionally, add a legend to distinguish between the two lines
legend("topleft", legend = c("Females", "Males"), col = c("red", "blue"), lwd = 2)

# Define ages for vertical lines (e.g., young age = 5, old age = 20)
young_age <- 3
old_age <- 8

# Add vertical lines at the chosen ages
segments(young_age, 8.5, young_age, 12, col = "black", lty = 2, lwd = 2)
segments(old_age, 16, old_age, 24.4, col = "black", lty = 2, lwd = 2)

```

At a young age, the difference between sexes is small; at an older age, the difference is larger.

You can take whatever perspective is most valid for you, and it depends on the questions you are asking. A little open to interpretation.

## Interactions in the linear model

How do we add an interaction into the linear model?

1) Like I said before, you could just do this analysis separately: one regression for males, one for females. But you will get two betas and you won't know if they are statistically significant.
2) Add in another effect to our linear model ($\beta_3$) and then multiply the two X-variables by eachother (times eachother).

So now the linear model looks like:

$Size = \beta_0 + \beta_1Size + \beta_2Sex + \beta_3 Age*Sex$








<br>

## Truth

```{r}
################### 'Truth' #################### 
### Lecture 14: code to simulate data for class

# Set the seed for reproducibility
set.seed(123)

# This is similar to the Age, Sex, and Size data we simulated for in Lecture 12.
# There is no collinearity between Age and Sex, but now there is an interaction
# between Sex and Age.

# First dataset
# X variable
n <- 50
x1 <- c(rep("Female", n), rep("Male", n))
x2 <- runif(n * 2, 1, 10)
dummy <- data.frame(model.matrix(~ x1 - 1))
colnames(dummy) <- c("Female", "Male")

# Simulate error
Error <- rnorm(n * 2, 0, 0.8)

# Predict Y
Response <- 4 + 1.5 * x2 + 1 * dummy$Male + 1 * x2 * dummy$Male + Error

# Dataframe
datum <- data.frame(Age = x2, Sex = x1, Male = dummy$Male, Size = Response)

# Save the data
write.csv(datum, "lecture_14_dataset1.csv", row.names = FALSE)
```

[--go to next lecture--](lecture_15.html)
