---
title: "Taxonomy of statistics"
author: "NRES 710"
date: "Fall 2022"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

```

```{r echo=FALSE}


#  NRES 710, Lecture 3 ---------------------------
#  University of Nevada, Reno   

#  Taxonomy of statistics                            


```


## Overview of statistical methods

Before we delve into common statistical tests, let's first give a broad overview of the analyses we will cover in this class and in which cases they are most appropriate. 

And before we do that, let's briefly talk about *parametric* vs *nonparametric* statstical tests...

### Aside: parametric vs nonparametric

As we saw in the previous lecture, **Parameters** are the arguments used to describe probability distributions - they describe the exact shape and location of the distribution. Different distribution families are associated with different parameters. For example, the normal distribution is described by 2 parameters: mean and standard deviation. The Poisson distribution has only one parameter (the Poisson mean, also known as Lambda). The binomial distribution is described by 2 parameters: size (number of trials) and prob (success probability for each trial). The t distribution has one parameter (degrees of freedom) -- the list goes on.  

#### Parametric statistics 

In *parametric statistics*, our hypotheses involve running tests that directly relate to thee parameters of well-defined probability distributions. That is, we assume a priori that our test statistic (e.g., a t statistic) follows a certain sampling distribution (e.g., a t distribution), and then we run tests -- for example we test to see if we can reject the null hypothesis.  

Because of the Central Limit Theorem, we have good reason (in many cases) to assume that our sampling distribution should follow a particular distribution (e.g., Normal, t, Chi-squared). However, this is not always a fair assumption, and we are fortunate that statisticians have developed a number of tests that do not depend on our data or data summaries following any defined probability distribution. These methods are the so-called **distribution-free** statistical tests, which are often referred to as **non-parametric tests**. 

#### Nonparametric statistics

Unlike parametric statistics, *nonparametric statistics* do not require us to make strong assumptions, such as that our test statistic follows a t-distribution (like in a t-test) or that our residuals are normally distributed (like in regression). Using these methods, we can still test similar hypotheses even if our data do not meet standard parametric assumptions

Confusingly, we often use the term "nonparametric" to refer to two different classes of statistical models:     
1. Statistical tests that do not require any assumptions about the distribution of data or residuals. These methods are also known as **distribution-free**.
2. Statistical models that do not require assumptions about the **shape** of the relationship between two or more variables. These methods include generalized additive models, spline regressions, gaussian processes, and many methods collectively known as **machine learning**. These methods are often referred to as **nonparametric regression**. 

#### Independence of data

It is important to note that most classical parametric and nonparametric statistics all make one very important assumption -- that observations are independent! Violation of this assumption are often called **pseudoreplication** and can wreak havoc with our type I error rates!

**Q** Given the CLT makes many classical tests robust to non-normal data distributions, why do we need non-parametric statistics?

Okay, let's get back to our main purpose, which is to provide a "dichotomous key" of sorts for determining which statistical analyses to run. This dichotomous key will involve characterizing the type of response variable and predictor variable we have at hand -- most importantly, whether we have continuous or categorical data for our response and predictor variables. 

### Continuous response variable

If your response variable is continuous, common classical statistical analyses include: t-tests, ANOVA, linear regression. Each of these tests is associated with non-parametric alternatives. In each case, we are interested in testing if the *population mean* of the continuous response variable is affected by the predictor variables (null hypothesis: nope, there is no effect!). Predictor variables, in turn, can be continuous or categorical (factor variables in R) -- but as you suspected, different predictor variable types will lead us to choose different statistical models!   

#### Continuous response, binary categorical predictor

If your categorical predictor is binary (two levels) and your response variable is continuous, you can use a **two-sample t-test**. 

The non-parametric alternative is called the **Mann-Whitney test**.

You can visualize the effect size using a boxplot or barplot with error bars. 

**Example**: Are females larger than males? (null hypothesis: no difference). The response variable (body size) is continuous. The predictor variable (sex) is binary. 

If your categorical predictor has more than two levels, you can use an **ANalysis Of Variance (ANOVA)** followed by *pairwise comparisons* to test which categories differ from one another. 

The non-parametric alternative is the **Kruskal-Wallis test**. 

You can visualize these relationships with (e.g.) a boxplot or barplot with error bars.  

**Example** Do three different brands of nail polish differ in their durability? (null hypothesis: no effect). The predictor variable (nail polish brand) is categorical, the predictor variable (time until product wears off) is continuous.

#### Continuous response, continuous predictor

If your predictor variable and response variable are both continuous, you can use **linear regression** analysis. 

If you just want to know if two variables are correlated but you are not interested in modeling one (the response) as a function of another (the predictor) you can run a **Pearson correlation test**. 

The non-parametric alternative is a **Spearman rank-correlation test**.    

**Example**: What is the relationship between tree diameter and age- can tree diameter be used to effectively predict the age of a tree? (null hypothesis: nope, there is no relationship). The predictor variable (age) is continuous, and the response variable (tree diameter) is also continuous.   

#### Continuous response, both continuous and categorical predictors

If you have one categorical variable that you hypothesize may be influencing a continuous response variable-- but there is also a "pesky" continuous variable that you also suspect is influencing your continuous response -- you can use **Analysis of Covariance (ANCOVA)**

More generally, if you have a set of continuous and categorical predictor variables that you hypothesize may be influencing your continuous response variable, you can use **multiple linear regression**.

You can visualize these data using scatterplots with regression lines (and confidence intervals).

Practically speaking, linear regression and ANOVA/ANCOVA are two sides of the same coin. Both involve modeling a continuous response as a function of one or more predictor variables. You can run all of these analyses using the workhorse of linear modeling in R, the `lm()` function. Basically, if you have a continuous response variable, you can use the 'lm' function! 

Linear regression is parametric, and assumes the resdiuals (differences between observed data and predictions) are normally distributed.

The non-parametric alternative might be something like a **distribution free regression tree** analysis (which makes no assumptions about the distribution of residuals or the shape of the relationship) or a **generalized additive model (GAM)** (which assumes the residuals follow a defined distribution, but makes not assumption about the shape of the functional response). 

### Discrete (count) response variable

With a discrete count response, you can either assume a continuous response variable and use the same techniques as you would for a continuous response (e.g., linear regression). If this is not justifiable, you can use **generalized linear models (GLM)** with an error distribution that follows a **discrete** probability distribution such as a Poisson distribution (see previous lecture).

This type of relationship can be visualized with a scatterplot and regression line (with confidence band)

**Generalized Linear Models (GLM)** are a widely used parametric class of models that enable researchers to perform regression analysis while *relaxing* the assumption that residuals must be normally distributed. In GLM, residual error can be distributed according to discrete distributions like Bernoulli (binary), Binomial, Poisson and Negative Binomial -- or according to non-normal continuous distributions like the Gamma or Exponential distributions. 

Nonparametric options include classification/regression trees and GAM.

Example: does the number of eggs produced per year by a desert tortoise depend on the availability of food in the prior year?

### Categorical response variable

If your response variable is categorical (factor variable, ordinal variable, binary variable) then your choice of statistical methods changes:

#### Categorical response variable, categorical predictor

If both your response variable and your predictor variable are categorical, then you can use a **chi-squared test** or a **Fisher exact test** to test for an association between the two variables. 

In this case, it can be informative to summarize your data as a *contingency table* like we did with the 'lady tasting tea' example. That is, we make a table that summarized the number of observation in each unique category of our response and predictor variables.  

Example: test for an association between salamander color morph (melanistic vs wild-type) and mating behavior ('sneaker' vs territory holder).  
 
#### Categorical response variable, continuous predictor 

If your response variable is binary (true/false, two levels) and your predictor variable is continuous, you can use **logistic regression** (which is a type of *generalized linear model (GLM)*).

Example: Does the probability of site-occupancy for American pika depend on elevation?

If your categorical response variable has more than two levels, you can use **multinomial logistic regression** (for categorical responses) or **ordinal logistic regression** (for ordinal responses). I don't plan to explore these methods in this class, but it is useful to know they exist!

### When the taxonomy breaks down

Not all statisical tests fall into this neat hierarchy. Take a one-sample t-test for example. Imagine you are asking the question: Is the sample mean equal to 32? What exactly is the predictor variable? 

Another example might be a normality test. We want to test if our data are normally distributed -- again, what is the predictor variable in this case? 

## Discussion: final projects

For the remainder of the class period, let's discuss your final projects. Take a minute to think through your data set and try to determine: what is your response variable, what is your predictor variable? Are these data continuous or discrete? Based on the taxonomy, what main analysis or analyses do you anticipate using for your final projects???

[--go to next lecture--](LECTURE4.html)













