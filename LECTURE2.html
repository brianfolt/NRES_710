<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 710" />


<title>Sampling distributions/uncertainty</title>

<script src="site_libs/header-attrs-2.27/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 710</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Syllabus</a>
</li>
<li>
  <a href="schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 710</a>
    </li>
    <li>
      <a href="LECTURE1.html">Basic Concepts</a>
    </li>
    <li>
      <a href="LECTURE2.html">Sampling uncertainty</a>
    </li>
    <li>
      <a href="LECTURE3.html">Taxonomy of common statistics</a>
    </li>
    <li>
      <a href="LECTURE4.html">t-test and z-test</a>
    </li>
    <li>
      <a href="LECTURE5.html">chi-squared tests</a>
    </li>
    <li>
      <a href="LECTURE6.html">Linear Regression</a>
    </li>
    <li>
      <a href="LECTURE7.html">ANOVA</a>
    </li>
    <li>
      <a href="LECTURE8.html">GLM</a>
    </li>
    <li>
      <a href="LECTURE9.html">GLMM</a>
    </li>
    <li>
      <a href="LECTURE10.html">Machine Learning</a>
    </li>
    <li>
      <a href="GIT_tutorial.html">Intro to GIT</a>
    </li>
    <li>
      <a href="LECTURE11.html">Next steps</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="EXERCISE1.html">Exercise 1 - Data Summary Functions</a>
    </li>
    <li>
      <a href="EXERCISE2.html">Exercise 2 - t-tests</a>
    </li>
    <li>
      <a href="EXERCISE3.html">Exercise 3 - Simple Linear Regression</a>
    </li>
    <li>
      <a href="EXERCISE4.html">Exercise 4 - Multiple Linear Regression</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    More Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Links.html">Links</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final Projects</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Sampling distributions/uncertainty</h1>
<h4 class="author">NRES 710</h4>
<h4 class="date">Last compiled: 2024-07-09</h4>

</div>


<div id="download-the-r-code-for-this-lecture" class="section level2">
<h2>Download the R code for this lecture!</h2>
<p>To follow along with the R-based lessons and demos, <a
href="LECTURE2.R">right (or command) click on this link and save the
script to your working directory</a></p>
</div>
<div id="statistics-inference-from-a-sample" class="section level2">
<h2>Statistics: inference from a sample</h2>
<p><img src="ylf.jpg" style="width:40.0%" /></p>
<p>Consider the following example:</p>
<p>Yellow-Legged frog example:<br />
<em>Population</em>: all yellow-legged frogs in ponds in the central
Sierra Nevada<br />
<em>Parameter</em>: mean body size (SVL) of adult yellow-legged frogs in
all ponds in the central Sierra Nevada<br />
<em>Sample</em>: As many frogs are captured and measured as possible in
10 ponds randomly sampled from the central Sierra Nevada<br />
<em>Statistic</em>: Sample mean</p>
<p><img src="statistics1.png" style="width:40.0%" /></p>
<p>The goal of statistics is to infer something meaningful about a
population from a sample. In the yellow-legged frog example above, there
is a “true” mean body size of frogs in ponds in the central Sierra
Nevada. We just don’t know what it is!</p>
<p>After collecting data from a sample, statistics will help us to say
something about the mean body size in the population – both about what
we know AND what we don’t know about the population!</p>
<p>First of all, we assume that the summary statistic computed from our
sample (n&gt;&gt;1) is representative of the population. How? Why?
Because of the <em>Central Limit Theorem</em>.</p>
<div id="the-central-limit-theorem" class="section level3">
<h3>The Central Limit Theorem</h3>
<p>The Central Limit Theorem (CLT) says that if you have a sample with a
reasonably large number of observations (the larger, the better!), and
each observation is independently sampled from the population, then the
statistics we compute from the sample (e.g., the sample mean) should be
reflective of the population.</p>
<p>For the yellow-legged frog example: our sample mean should be
representative of the mean of ALL yellow-legged frogs in the central
Sierra Nevada.</p>
<p>And as the sample size gets bigger, the sample mean will become more
representative of the true mean (it will converge on the true mean as
sample size approaches infinity).</p>
<p>The concept of <strong>regression to the mean</strong> is a natural
consequence of the Central Limit Theorem!</p>
<p>[In-class R demo: regression to the mean]</p>
<p>The CLT is the magic wand of statistics. It does enormous amounts of
work for us. Why?</p>
<p>The CLT also implies that the sampling distribution (distribution of
hypothetical samples collected from repeated sampling) for the sample
mean (and many other summary statistics) will be approximately normally
distributed – even if the underlying data themselves are NOT normally
distributed.</p>
<p>Did you ever wonder why the normal (Gaussian) distribution is so
common in statistics? It’s because of the CLT- many summary statistics
derived from a sample are expected to have a sampling distribution that
is <em>approximately</em> normally distributed (based on the CLT)!</p>
<p><img src="clt1.png" /></p>
</div>
<div id="example" class="section level3">
<h3>Example</h3>
<p>How does this work? Let’s use the yellow-legged frog example.</p>
<p>Let’s say that we could measure ALL the frogs in ALL the ponds in CA.
What would that look like?</p>
<p>Let’s simulate it using a log-normal distribution that is strongly
right skewed (positively skewed), suggesting that there are a lot of
frogs out there that are relatively small-bodied, and a few that are
giants! NOTE: this is not necessarily biologically realistic, but it
makes a point.</p>
<p>First, let’s set the population/parameter (the truth about which we
hope to make inference but can never know in reality)</p>
<pre class="r"><code># Yellow-legged frog example ---------------------

### ALL FROGS IN CA (the statistical population- all the frogs!)

allfrogs.bodysize &lt;- rlnorm(10000,1.5,0.4)        # statistical &#39;population&#39;
hist(allfrogs.bodysize,main=&quot;&quot;,xlab=&quot;SVL (mm)&quot;)   # plot out histogram</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>truemean_SVL &lt;- mean(allfrogs.bodysize)           # the &#39;parameter&#39;
truemean_SVL </code></pre>
<pre><code>## [1] 4.86589</code></pre>
<p>Now let’s take a sample!</p>
<pre class="r"><code>mysample &lt;- sample(allfrogs.bodysize,10)    # take sample of size 10 (10 frogs measured)
mean(mysample)   # compute the sample mean</code></pre>
<pre><code>## [1] 4.490253</code></pre>
<p>And another, this time with n=20</p>
<pre class="r"><code>mysample &lt;- sample(allfrogs.bodysize,20)    # take sample of size 20 (20 frogs measured)
mean(mysample)   # compute the sample mean</code></pre>
<pre><code>## [1] 4.741507</code></pre>
<p>Since sampling is random, sampling will produce a different result
every time.</p>
<p>To get a better picture of the sampling variance, let’s sample many
times!</p>
<pre class="r"><code>lotsofsamples &lt;- list()

for(s in 1:5000){
  lotsofsamples[[paste0(&quot;sample&quot;,s)]] &lt;- sample(allfrogs.bodysize,30)    # take sample of size 30 (30 frogs measured)
}

lotsofsamples$sample1</code></pre>
<pre><code>##  [1] 9.870530 2.585398 3.333185 4.691382 5.738534 8.773529 3.249062 2.860118
##  [9] 3.800551 3.865068 3.622419 4.949816 4.941926 6.162584 5.316123 1.819894
## [17] 5.966691 5.435843 8.389564 4.032064 3.437772 3.326147 8.020591 2.584070
## [25] 3.778267 2.423480 7.759992 6.451879 6.646355 3.087408</code></pre>
<pre class="r"><code>lotsofsamples$sample99</code></pre>
<pre><code>##  [1]  3.735208  5.907859  3.563188  5.658130  3.671438  3.791738  3.759257
##  [8]  2.367941  3.660289  5.817977  5.349160  2.817195  3.263106  3.369762
## [15]  4.889541  5.033721  2.174314  7.899127  8.148929  3.521663  4.299664
## [22]  4.573797  2.029409  3.834115  7.176607 10.522934 12.306219  4.918912
## [29]  2.702932  5.297337</code></pre>
<pre class="r"><code>lotsofsamples$sample732</code></pre>
<pre><code>##  [1]  3.628170  3.265468  2.979275  5.841681  2.924863  6.213154  2.968529
##  [8]  8.824904  4.786010  2.422554  7.157891  3.214569  5.830407  6.503452
## [15]  5.291989  6.187573  2.546349  3.898771  3.083737  4.128964  4.950483
## [22]  4.630313  7.101422  4.058735  4.203144 12.026801  3.645135  8.468668
## [29]  1.496934  5.726627</code></pre>
<p>Now we can compute the sample means and the sampling variance for the
summary statistic (mean body size)</p>
<pre class="r"><code>samplemeans &lt;- sapply(lotsofsamples,mean)

hist(samplemeans,xlab=&quot;mean body size (n=30)&quot;)    # visualize the sampling distribution!</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Interesting- does this look skewed to you? Doesn’t it look like a
normal distribution??</p>
<p>It’s the CLT at work!!</p>
<p>Of all the samples you could get, there are very few that are all at
one end of the distribution. There are a lot more possible random
samples that span the full distribution of values, from low to high.
Take the average af all those values, low and high, and you get
something in the middle. The normal distribution is humped right in the
middle, because of the tendency for low and high observations to
‘average out’ within a sample.</p>
</div>
<div id="coin-flipping-example" class="section level3">
<h3>Coin flipping example</h3>
<p>Here’s the sampling distribution for the number of heads out of a
single coin flip (either 0 or 1!):</p>
<pre class="r"><code>barplot(table(rbinom(10000,1,.5))/10000,xlab=&quot;N heads out of 1&quot;,ylab=&quot;Probability&quot;)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Now let’s build up sample size and see how the sampling distribution
changes.</p>
<pre class="r"><code>par(mfrow=c(3,2))
for(i in seq(2,12,2)){
   barplot(table(rbinom(10000,i,.5))/10000,xlab=sprintf(&quot;N heads out of %s&quot;,i),ylab=&quot;Probability&quot;,main=paste0(&quot;sample size = &quot;,i))
   #hist(rbinom(10000,i,.5),main=paste0(&quot;sample size = &quot;,i),xlab=sprintf(&quot;N heads out of %s&quot;,i)) 
}</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>And with really big sample size:</p>
<pre class="r"><code>hist(rbinom(10000,1000,.5),xlab=&quot;N heads out of 1000&quot;,freq = F, main=&quot;&quot;)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>The larger the sample size, the more closely the sampling
distribution (number of heads out of N flips of a fair coin) looks like
a normal distribution! Again, the CLT at work!</p>
</div>
</div>
<div id="sampling-distributions-in-null-hypothesis-testing"
class="section level2">
<h2>Sampling distributions in null hypothesis testing</h2>
<p>Pretty much all of classical null hypothesis testing (NHT) works like
this:</p>
<ol style="list-style-type: decimal">
<li>We compute a summary statistic from our data. This statistic
represents the “signal” in your data.<br />
</li>
<li>Using the known (theoretical) sampling distribution for our summary
statistic under the null hypothesis (worked out by statisticians!), we
inquire whether or not our summary statistic (signal) could have been a
result of random sampling error under the null hypothesis.</li>
<li>If it is implausble that random noise could have produced our result
(if <span class="math inline">\(p\le\alpha\)</span>) then we reject the
null hypothesis. Otherwise we fail to reject the null…</li>
</ol>
<p>Sampling distributions that are commonly used include:</p>
<ul>
<li>t distribution (sampling distribution for the t statistic under the
null hypothesis)</li>
<li>z distribution (assume your test statistic is a standard normal
distribution)</li>
<li>Chi-squared distribution (sampling distribution for the Chi-squared
statistic under the null hypothesis)</li>
<li>F distribution (sampling distribution for the F statistic under the
null hypothesis- used in ANOVA and regression)</li>
</ul>
<p>Sampling distributions are thought experiments! What would our test
statistic look like under repeated sampling from a population (e.g.,
under the null hypothesis)?</p>
</div>
<div id="summary-metrics-calculated-from-sample-data"
class="section level2">
<h2>Summary metrics (calculated from sample data)</h2>
<p>We often summarize our samples by their centers (e.g., average) and
their spread (dispersion). These informative data summaries are useful
on their own, and are also used to compute statistics like the t or F
statistics.</p>
<div id="center-statistics-means-medians-geometric-mean"
class="section level3">
<h3>“Center” statistics: means, medians, geometric mean</h3>
<p>Sample Mean (arithmetic mean) = sum of all sampled values divided by
the sample size<br />
Sample Median (midway point) = 50% quantile. Order the values and select
the value at the center.</p>
<p>Sample Geometric mean: product of numbers taken to the nth root. For
two numbers, 3 and 4, you’d have the sq root of 3*4 = 3.46</p>
</div>
<div id="data-spread-or-dispersion" class="section level3">
<h3>Data spread, or dispersion:</h3>
<p>Standard deviations and variances are calculated differently
depending on whether we are computing these quantities for an entire
population of interest vs a sample drawn from a larger population!</p>
<p>Standard Deviation – sigma (<span
class="math inline">\(\sigma\)</span>) for population standard
deviation, <em>s</em> for the sample standard deviation.</p>
<p>Variance- (<span class="math inline">\(\sigma^2\)</span>) for
population variance, (<span class="math inline">\(s^2\)</span>) for the
sample variance</p>
<p>The variance represents the average squared difference from the mean.
The standard deviation represents the square root of the variance.</p>
<p>Standard deviation is much more commonly reported than variance
because it is in the same units/scale as the original measurements.</p>
<p>Coefficient of variation (cv) is the standard deviation represented
as a fraction of the mean.</p>
<div id="variance-and-std-deviation-calculation-example"
class="section level4">
<h4>Variance and Std deviation calculation example</h4>
<p>For a population: <span class="math inline">\(\sigma =
\sqrt{\sum_{n=1}^{i}{\frac{(x_i-\mu)^2}{N}}}\)</span></p>
<p>For example: compute the variance of 5 numbers: 4, 3, 5, 5, 2</p>
<p><span class="math inline">\(\mu = (4+3+5+5+2)/5 = 3.8\)</span></p>
<p>(4-3.8)^2 = 0.2^2 = 0.04<br />
(3-3.8)^2 = 0.64<br />
(5-3.8)^2 = 1.44<br />
(5-3.8)^2 = 1.44<br />
(2-3.8)^2 = 3.24</p>
<p>Sum these = 6.8<br />
Divide by 5 = population variance = 6.8/5 = 1.36 Take square root =
<span class="math inline">\(\sigma\)</span> = 1.17</p>
<p>For a sample (estimating population variance from a sample): <span
class="math inline">\(s =
\sqrt{\sum_{n=1}^{i}{\frac{(x_i-\bar{x})^2}{(N-1)}}}\)</span></p>
<p><span class="math inline">\(\bar{x}  = (4+3+5+5+2)/5 =
3.8\)</span></p>
<p>(4-3.8)^2 = 0.2^2 = 0.04<br />
(3-3.8)^2 = 0.64<br />
(5-3.8)^2 = 1.44<br />
(5-3.8)^2 = 1.44<br />
(2-3.8)^2 = 3.24</p>
<p>Sum these = 6.8<br />
Divide by 4 = sample variance = 6.8/4 = 1.7<br />
Take square root = <span class="math inline">\(s\)</span> = 1.30</p>
<p>So the population sd is 1.36 whereas the sample sd is 1.7.</p>
</div>
<div id="aside-degrees-of-freedom" class="section level4">
<h4>Aside: degrees of freedom</h4>
<p>OK, so why the different estimates of dispersion for population
vs. sample?</p>
<p>Which is larger? Which are we less confident in?</p>
<p>This has to do with a concept called <em>degrees of freedom</em>.</p>
<p>Sigma can be computed with 100% accuracy. Since you have the entire
population measured, you can compute a measure of dispersion for the
population and that measure is perfect. It is not an estimate.</p>
<p>The sample standard deviation <span class="math inline">\(s\)</span>,
on the other hand, is an imperfect estimate of dispersion for a much
larger population! In fact, if we used the population formula for a
sample, it would underestimate the dispersion of the target
population.</p>
<p>Th reason for this is that the formula for sample stdev <em>uses the
sample mean</em>, not the population mean. In fact, the same sample data
were used to compute the sample mean! If you know the values of four of
the five sampled values AND we know that the sample mean, we know what
the value of the final observation must be. Therefore, even though we
have 5 data points, we have only 4 <em>degrees of freedom</em> if the
sample mean is included in our formula. In this case, we have four
independent pieces of information that we can use for computing standard
deviation (we ‘spent’ one degree of freedom already to compute the
sample mean!).</p>
<p>By dividing the sum of squared deviations from the sample mean by 4
instead of 5, we are <em>unbiasing</em> the estimate of dispersion to
account for the fact that we are using the sample data twice- once for
computing the mean, next for computing the standard deviation!</p>
</div>
</div>
</div>
<div id="sampling-distributions" class="section level2">
<h2>Sampling distributions</h2>
<p>Statisticians worked out the sampling distributions (often called the
sampling variance- don’t confuse this with the ‘sample variance’ we
discussed above!) for some common summary statistics.</p>
<div id="standard-error-of-the-mean" class="section level3">
<h3>Standard error of the mean</h3>
<p>Standard error of the mean = sample std deviation divided by the
square root of the sample size</p>
<p><span class="math inline">\(se = \frac{s}{\sqrt{N}}\)</span></p>
<p>The standard error of the mean is used to help us describe the
sampling distribution for the sample mean (the expected distribution of
sample means if you collected thousands of new samples and computed the
mean).</p>
<pre class="r"><code># Survey of common sampling distributions -----------------

# Sampling distribution: the sample mean

mysample &lt;- c(4.1,3.5,3.7,6.6,8.0,5.4,7.3,4.4)
mysample</code></pre>
<pre><code>## [1] 4.1 3.5 3.7 6.6 8.0 5.4 7.3 4.4</code></pre>
<pre class="r"><code>n &lt;- length(mysample)    # sample size
sample.mean &lt;- mean(mysample)  # sample mean
sample.stdev &lt;- sd(mysample)   # sample standard deviation (r uses denominator of n-1 by default!)
std.error &lt;- sample.stdev/sqrt(n) 

std.error </code></pre>
<pre><code>## [1] 0.6122995</code></pre>
<p>Now we have all the information we need to compute the sampling
distribution for the sample mean.</p>
<p>Our sample mean is 5.375. But if we collected different samples of
size n=8, we would get different values - even if the true population
mean was 5.375. What does this distribution of values look like?</p>
<pre class="r"><code>sampdist &lt;- function(x){dt((x-sample.mean)/std.error,n-1)}
curve(sampdist,0,11,ylab=&quot;probability density&quot;,xlab=&quot;value&quot;,main=&quot;sampling distribution for the sample mean!&quot;)
abline(v=sample.mean,col=&quot;green&quot;,lwd=3)
confint &lt;- c(sample.mean+std.error*qt(0.025,n-1),sample.mean+std.error*qt(0.975,n-1))
abline(v=confint,col=&quot;blue&quot;,lty=2)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The vertical blue lines indicate the <em>confidence interval</em>
around the mean with the <em>confidence level</em> set at 95% - the
confidence interval helps us visualize what might happen if we repeated
our sampling over and over and over- how might our result change?</p>
<p>Note the use of the <em>t distribution</em> in the above code block.
The t distribution is a theoretical sampling distribution representing
sampling error in units of standard error.</p>
</div>
<div id="confidence-intervals" class="section level3">
<h3>Confidence intervals</h3>
<p>Confidence intervals, like p-values, are commonly misinterpreted!</p>
<p>In classical frequentist statistics, the population parameter of
interest is fixed- there is no uncertainty associated with the
population parameter itself – it’s just that we can only collect and
assess a small sample from the much larger population. So it does not
make sense to say something like “the true parameter has a 90% chance of
falling within the confidence interval”. The true parameter is either in
the interval or it is not in the interval. All we can say is something
more like “90% of confidence intervals generated from different random
samples would contain the true parameter”.</p>
<p><img src="confint1.png" style="width:40.0%" /></p>
<p>We unfortunately have no idea if our particular confidence interval
is one that includes the true parameter, or one that does not!!</p>
<p>Don’t worry if you find this difficulty- everyone does! And
practically speaking, just about everyone interprets a 95% confidence
interval as having a 95% probability of including the true parameter -
and it doesn’t really matter that much!</p>
</div>
</div>
<div id="probability-distributions--the-basics-and-how-to-use-them-in-r"
class="section level2">
<h2>Probability distributions- the basics (and how to use them in
R)</h2>
<div id="discrete-vs.-continuous" class="section level3">
<h3>Discrete vs. continuous</h3>
<p>In <em>discrete distributions</em>, each outcome (value that could be
sampled under this probability distribution) has a specific
<em>probability mass</em> (like the probability of flipping a coin 10
times and getting 4 heads). For example, let’s consider the Poisson
distribution:</p>
<pre class="r"><code># Probability distributions ---------------------

# Discrete probability distributions 

mean &lt;- 5
rpois(10,mean)    # note: the random numbers sampled from this distribution have no decimal component</code></pre>
<pre><code>##  [1]  6 11  3  5  2  7  8  4  6 14</code></pre>
<pre class="r"><code>             # plot discrete probabilities of getting particular outcomes!
xvals &lt;- seq(0,15,1)
probs &lt;- dpois(xvals,lambda=mean)
names(probs) &lt;- xvals
               
barplot(probs,ylab=&quot;Probability Mass&quot;,main=&quot;Poisson distribution (discrete)&quot;)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>barplot(cumsum(probs),ylab=&quot;Cumulative Probability&quot;,main=&quot;Poisson distribution (discrete)&quot;)   # cumulative distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-12-2.png" width="672" /></p>
<pre class="r"><code>sum(probs)   # just to make sure it sums to 1!  Does it???</code></pre>
<pre><code>## [1] 0.999931</code></pre>
<p>In <em>continuous distributions</em>, each possible value/quantity
that could be randomly sampled is associated with a <em>probability
density</em>, <span class="math inline">\(f(x)\)</span>, not probability
mass <span class="math inline">\(Prob(x)\)</span>. This is because the
probability of getting any particular value in a continuous distribution
is effectively zero. This arises from the problem of precision. The sum
of the probability distribution must be 1 (there is only 100% of
probability to go around). In a continuous distribution, there are an
infinite number of possible values of x. So any individual probability
is always divided by infinity, which makes it zero. Therefore we have to
talk about probability density, unless we want to specify a particular
range of values – we can’t calculate <span class="math inline">\(Prob(x
= 5)\)</span>, but we can calculate <span class="math inline">\(Prob(4
&lt; x &lt; 6)\)</span> or <span class="math inline">\(Prob(x &gt;
5)\)</span>. The probability density is defined as the probability of
getting a value within an infinitesimally small range of a particular
value, divided by that infinitesimally small interval. No worries if you
don’t understand that - you can just think of probability density as the
relative likelihood of sampling one value versus another. Let’s consider
the beta distribution:</p>
<pre class="r"><code># continuous distributions

shape1 = 0.5
shape2 = 0.5

rbeta(10,shape1,shape2)   # generate 10 random numbers from a continuous distribution</code></pre>
<pre><code>##  [1] 0.9994135545 0.1227895102 0.9982528837 0.1963592479 0.1678191284
##  [6] 0.5322601653 0.0002699926 0.7129923751 0.1518480336 0.8368569283</code></pre>
<pre class="r"><code>curve(dbeta(x,shape1,shape2),ylab=&quot;probability density&quot;,xlab=&quot;possibilities&quot;)   # probability density function (PDF)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>curve(pbeta(x,shape1,shape2),ylab=&quot;cumulative probability&quot;,xlab=&quot;possibilities&quot;)   # cumulative distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-13-2.png" width="672" /></p>
<pre class="r"><code>integrate(f=dbeta,lower=0,upper=1,shape1=shape1,shape2=shape2)    # just to make sure it integrates to 1!!</code></pre>
<pre><code>## 1 with absolute error &lt; 3e-06</code></pre>
</div>
<div id="probability-distributions-in-r" class="section level3">
<h3>Probability distributions in R</h3>
<div id="random-number-generators" class="section level4">
<h4>Random number generators</h4>
<p><strong>Random number generators</strong> are functions for
generating random values from a specified probability distribution
(e.g., ‘rnorm’, ‘rpois’, ‘rt’)</p>
<pre class="r"><code># random number generators

rnorm(10)    # generate 10 random numbers from a standard normal distribution</code></pre>
<pre><code>##  [1]  1.38606637  1.48803220  1.06464652  0.03827372  1.04792179  1.62113488
##  [7] -1.18348124 -1.25397255 -1.00410726 -0.05334176</code></pre>
<pre class="r"><code>rnorm(5,25,5)  # generate 5 random numbers from a normal distribution with mean=25 and sd=5</code></pre>
<pre><code>## [1] 32.61277 28.48059 16.98099 27.47501 27.01813</code></pre>
<pre class="r"><code>rpois(8,18)  # generate 8 random numbers from a poisson distribution with mean=18</code></pre>
<pre><code>## [1] 19 23 19 30 23 24 23 15</code></pre>
</div>
<div id="probability-density-functions" class="section level4">
<h4>Probability density functions</h4>
<p>Continuous distributions are associated with <strong>PDFs</strong>,
or probability density functions (e.g., ‘dnorm’,‘dt’,‘dgamma’). These
functions give you the probability density (relative probability) of any
particular value/quantity that could be randomly sampled under this
distribution.</p>
<pre class="r"><code>## probability density function example 

curve(dt(x,8),-4,4,xlab=&quot;possibilities&quot;,ylab=&#39;relative probability (prob density)&#39;)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="probability-mass-functions" class="section level4">
<h4>Probability mass functions</h4>
<p>For discrete distributions, <strong>PMFs</strong> - Probability mass
functions (e.g., ‘dpois’,‘dbinom’) – give you the exact probability of
obtaining any particular value that could be sampled under this
distribution.</p>
<pre class="r"><code>## probability mass function example 

x &lt;- barplot(sapply(0:10,function(t) dpois(t,2)),xlab=&quot;possibilities&quot;,ylab=&#39;probability&#39;)
axis(1,at=x,labels=0:10)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="cumulative-distribution-function" class="section level4">
<h4>Cumulative distribution function</h4>
<p>For continuous AND discrete probability distributions,
<strong>CDFs</strong> - Cumulative distribution functions (e.g.,
‘pnorm’,‘pt’,‘pchisq’,‘pbinom’,‘ppois’) give you the probability of
obtaining a value less than or equal to any particular value that could
be sampled under the distribution.</p>
<pre class="r"><code>## cumulative distribution function  

    # for continuous distribution
curve(pt(x,df=8),-4,4,xlab=&quot;possibilities&quot;,ylab=&#39;cumulative probability&#39;)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre class="r"><code>    # for discrete distribution
x &lt;- barplot(sapply(0:10,function(t) ppois(t,2)),xlab=&quot;possibilities&quot;,ylab=&#39;cumulative probability&#39;)
axis(1,at=x,labels=0:10)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-17-2.png" width="672" /></p>
</div>
<div id="quantile-functions" class="section level4">
<h4>Quantile functions</h4>
<p><strong>Quantile functions</strong> - inverse of CDF- give you the
values below which a specific percent of random samples should fall
(e.g., ‘qnorm’,‘qt’,‘qpois’,‘qchisq’). The quantile function is the
inverse of the cumulative distribution function.</p>
<pre class="r"><code>## quantile function  

    # for continuous distribution
curve(qt(x,df=8),0,1,xlab=&quot;cumulative probability&quot;,ylab=&#39;quantile&#39;)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>    # for discrete distribution
curve(qpois(x,4),0,1,xlab=&quot;cumulative probability&quot;,ylab=&#39;quantile&#39;)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-18-2.png" width="672" /></p>
</div>
</div>
<div id="moments" class="section level3">
<h3>Moments</h3>
<p><strong>Moments</strong> are important descriptors of a distribution.
The collection of all the moments (of all orders, from 0 to infinity)
uniquely determines the shape of the distribution.</p>
<ul>
<li>The zeroth central moment (<span class="math inline">\(\int \left (
x-\mu  \right )^{0}Prob(x)\partial x\)</span>) is the total probability
(i.e. one),<br />
</li>
<li>The first central moment (<span class="math inline">\(\int \left (
x-\mu  \right )^{1}Prob(x)\partial x\)</span>) is <span
class="math inline">\(\mu - \mu = 0\)</span>.<br />
</li>
<li>The second central moment (<span class="math inline">\(\int \left (
x-\mu  \right )^{2}Prob(x)\partial x\)</span>) is the variance.<br />
</li>
<li>The third central moment (<span class="math inline">\(\int \left (
\left (x-\mu   \right )/\sigma  \right )^{3}Prob(x)\partial x\)</span>)
is the skewness.<br />
</li>
<li>The fourth central moment is the kurtosis.</li>
</ul>
</div>
<div id="some-common-probability-distributions" class="section level3">
<h3>Some common probability distributions</h3>
<p>Here are some common probability distributions. Pay particular
attention to the type of <em>process</em> described by each
distribution. The key to using these distributions to represent random
variables is to figure out which statistical process best matches the
ecological process you’re studying, then use that distribution. e.g., am
I counting independent, random events occurring in a fixed window of
time or space (like sampling barnacles in quadrats on an intertidal
bench)? Then the distribution of their occurrence is likely to follow a
Poisson or Negative Binomial distribution.</p>
<div id="binomial" class="section level4">
<h4>Binomial</h4>
<pre class="r"><code># Binomial

size &lt;- 10
prob &lt;- 0.3
rbinom(10,size,prob)</code></pre>
<pre><code>##  [1] 2 3 4 3 3 3 2 4 2 3</code></pre>
<pre class="r"><code>xvals &lt;- seq(0,size,1)
probs &lt;- dbinom(xvals,size,prob)
names(probs) &lt;- xvals
               
barplot(probs,ylab=&quot;Probability&quot;,main=&quot;Binomial distribution&quot;)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>barplot(cumsum(probs),ylab=&quot;Cumulative Probability&quot;,main=&quot;Binomial distribution&quot;)   # cumulative distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-19-2.png" width="672" /></p>
<pre class="r"><code>sum(probs)   # just to make sure it sums to 1!  Does it???</code></pre>
<pre><code>## [1] 1</code></pre>
</div>
<div id="normal" class="section level4">
<h4>Normal</h4>
<pre class="r"><code># Gaussian (normal)

mean = 7.1
stdev = 1.9

rnorm(10,mean,stdev)</code></pre>
<pre><code>##  [1]  6.830874  3.339121  7.945878  4.998624  7.040611  1.969721  8.125616
##  [8]  4.569595  3.958652 10.008928</code></pre>
<pre class="r"><code>curve(dnorm(x,mean,stdev),0,15)   # probability density</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>curve(pnorm(x,mean,stdev),0,15)   # cumulative distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-20-2.png" width="672" /></p>
<pre class="r"><code>integrate(f=dnorm,lower=-Inf,upper=Inf,mean=mean,sd=stdev)    # just to make sure it integrates to 1!!</code></pre>
<pre><code>## 1 with absolute error &lt; 1.1e-05</code></pre>
</div>
<div id="t-distribution" class="section level4">
<h4>t distribution</h4>
<pre class="r"><code># t distribution

df = 6

rt(10,df)     # random numbers from the t distribution</code></pre>
<pre><code>##  [1]  1.8950749  0.2146613  0.2134575  1.0122429  0.7770777  0.3850882
##  [7] -0.3507519 -0.2418912 -0.1792895 -0.9069788</code></pre>
<pre class="r"><code>curve(dt(x,df),-4,4)   # probability density</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r"><code>curve(pt(x,df),-4,4)   # cumulative distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-21-2.png" width="672" /></p>
<pre class="r"><code>integrate(f=dt,lower=-Inf,upper=Inf,df=df)    # just to make sure it integrates to 1!!</code></pre>
<pre><code>## 1 with absolute error &lt; 1.9e-05</code></pre>
</div>
<div id="chi-squared-distribution" class="section level4">
<h4>Chi-squared distribution</h4>
<pre class="r"><code># Chi-squared distribution

df = 6

rchisq(10,df)     # random numbers from the chi squared distribution</code></pre>
<pre><code>##  [1]  1.042111 10.944370  2.683156  2.802392  5.554620  1.030739  4.249502
##  [8]  9.024725  1.266405  3.147394</code></pre>
<pre class="r"><code>curve(dchisq(x,df),0,15)   # probability density</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>curve(pchisq(x,df),0,15)   # cumulative distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-22-2.png" width="672" /></p>
<pre class="r"><code>integrate(f=dchisq,lower=0,upper=Inf,df=df)    # just to make sure it integrates to 1!!</code></pre>
<pre><code>## 1 with absolute error &lt; 2.3e-05</code></pre>
</div>
<div id="exercise-in-class-not-graded" class="section level4">
<h4>Exercise (in class, not graded):</h4>
<p>Visualize (in R) the following distributions as above: Gamma,
Exponential, Lognormal, Negative Binomial.</p>
<p><a href="LECTURE3.html">–go to next lecture–</a></p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
