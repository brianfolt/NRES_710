<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 710" />


<title>t-tests</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 710</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 710</a>
    </li>
    <li>
      <a href="LECTURE1.html">Basic Concepts</a>
    </li>
    <li>
      <a href="LECTURE2.html">Analysis- t test</a>
    </li>
    <li>
      <a href="LECTURE3.html">Analysis- chi2 test</a>
    </li>
    <li>
      <a href="LECTURE4.html">Analysis- linear regression</a>
    </li>
    <li>
      <a href="LECTURE5.html">Analysis- ANOVA</a>
    </li>
    <li>
      <a href="LECTURE6.html">Analysis- Multivariate</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    More Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Links.html">Links</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final Projects</a>
    </li>
    <li>
      <a href="STUDYGUIDE.html">Study Guide</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">t-tests</h1>
<h4 class="author">NRES 710</h4>
<h4 class="date">Fall 2020</h4>

</div>


<div id="download-the-r-code-for-this-lecture" class="section level2">
<h2>Download the R code for this lecture!</h2>
<p>To follow along with the R-based lessons and demos, <a href="LECTURE2.R">right (or command) click on this link and save the script to your working directory</a></p>
</div>
<div id="overview-of-statistical-methods" class="section level2">
<h2>Overview of statistical methods</h2>
<p>Okay before we delve further into t-tests, let’s first give a broad overview of the analyses we will cover in this class (at least a tiny bit) and in which cases they are most appropriate:</p>
<div id="continuous-response-variable" class="section level3">
<h3>Continuous response variable</h3>
<p>If your response variable is continuous (ratio, interval), common classical parametric statistical analyses include: t-tests, ANOVA, linear regression. Each of these tests is associated with non-parametric alternatives. In each case, we are interested in testing if the <em>population mean</em> of the continuous response variable is affected by the predictor variables (null hypothesis: nope, there is no effect!). Predictor variables, in turn, can be continuous or categorical (factor variables in R).</p>
<div id="continuous-response-categorical-predictor" class="section level4">
<h4>Continuous response, categorical predictor</h4>
<p>If your categorical predictor is binary (two levels), you can use a <strong>two-sample t-test</strong>. The non-parametric alternative is the <strong>Mann-Whitney test</strong>.</p>
<p>Example: Are females larger than males? (null hypothesis: no difference)</p>
<p>If your categorical predictor has more than two levels, you can use an <strong>ANalysis Of Variance (ANOVA)</strong> followed by <em>pairwise comparisons</em> to test which categories differ from one another. You can visualize these relationships with (e.g.) a boxplot. The non-parametric alternative is the <strong>Kruskal-Wallis test</strong>.</p>
<p><strong>Q</strong>: What is the predictor variable for a one-sample t-test?</p>
</div>
<div id="continuous-response-continuous-predictor" class="section level4">
<h4>Continuous response, continuous predictor</h4>
<p>If your predictor variable is continuous, you can use <strong>linear regression</strong>. If you just want to know if two variables are correlated but you are not interested in modeling one (the response) as a function of another (the predictor) you can run a <strong>Pearson correlation test</strong>. The non-parametric alternative is a <strong>Spearman correlation test</strong>.</p>
<p>Example: What is the relationship between tree diameter and age- can tree diameter be used to effectively predict the age of a tree?</p>
</div>
<div id="continuous-response-both-continuous-and-categorical-predictors" class="section level4">
<h4>Continuous response, both continuous and categorical predictors</h4>
<p>In this case you can use <strong>Analysis of Covariance (ANCOVA)</strong> or just use <strong>multiple linear regression</strong>. In truth, linear regression and ANOVA/ANCOVA are two sides of the same coin. You can run both analyses using the workhorse of linear modeling in R, the ‘lm’ function. Basically, if you have a continuous response variable, you can use the ‘lm’ function! The non-parametric alternative might be something like a <strong>regression tree</strong> analysis.</p>
</div>
</div>
<div id="discrete-count-response-variable" class="section level3">
<h3>Discrete (count) response variable</h3>
<p>With a discrete count response, you can use the same techniques as with a continuous response OR you can use <strong>generalized linear models</strong> with a Poisson error distribution (or other discrete probability distribution).</p>
</div>
<div id="categorical-response-variable" class="section level3">
<h3>Categorical response variable</h3>
<p>If your response variable is categorical (factor variable, ordinal variable, binary variable) then your choice of statistical methods changes:</p>
<div id="categorical-response-variable-categorical-predictor" class="section level4">
<h4>Categorical response variable, categorical predictor</h4>
<p>If both your response variable and your predictor variable are categorical, then you can use a <strong>chi-squared test</strong> or a <strong>Fisher exact test</strong> to test for an association between the two variables. You may first summarize your data as a <em>contingency table</em> like we did with the ‘lady tasting tea’ example.</p>
<p>Example: test for an association between salamander color morph (melanistic vs wild-type) and mating behavior (‘sneaker’ vs territory holder).</p>
</div>
<div id="categorical-response-variable-continuous-predictor" class="section level4">
<h4>Categorical response variable, continuous predictor</h4>
<p>If your response variable is binary (true/false, two levels) and your predictor variable is continuous, you can use <strong>logistic regression</strong> (which is a type of <em>generalized linear model (GLM)</em>).</p>
<p>If your categorical response variable has more than two levels, you can use <strong>multinomial logistic regression</strong> (for categorical responses) or <strong>ordinal logistic regression</strong> (for ordinal responses).</p>
</div>
</div>
</div>
<div id="overview-t-tests" class="section level2">
<h2>Overview: t-tests</h2>
<p>Okay, now let’s move to the main topic of this lecture: t-tests! Here we have a continuous response variable and either no predictor variable (one sample t-test) or a binary predictor variable (two-sample t-test).</p>
<p>We have already performed simple versions of the t-test (one-sample t-tests). But t-tests are quite flexible and can be applied to a wide variety of null-hypothesis testing scenarios:</p>
<div id="one-sample-t-test" class="section level3">
<h3>One-sample t-test</h3>
<p>A one-sample t-test tests the consistency of the sample data with the null hypothesis that the sample was generated from a population with a specified mean (often zero, but not necessarily). As we have seen before, the t-statistic in this case is expressed as:</p>
<p><span class="math inline">\(t = \frac{(\bar{x}-\mu_0)}{s.e.}\)</span></p>
<p>Where <span class="math inline">\(\bar{x}\)</span> is the sample mean, <span class="math inline">\(s.e.\)</span> is the standard error of the mean, and <span class="math inline">\(\mu_0\)</span> is the population mean under the null hypothesis.</p>
<p>The t-statistic can be interpreted as the difference between the sample mean and the null population mean in units of standard errors.</p>
<p>Under the null hypothesis, the sampling distribution of the t-statistic should follow a t distribution with n-1 degrees of freedom.</p>
<pre class="r"><code>### one sample t-test (paired t-test is a type of one sample t-test)

sample.data &lt;- rgamma(10,2,.1)
null.mean &lt;- 10

sample.size &lt;- length(sample.data)
sample.mean &lt;- mean(sample.data)
sample.sd &lt;- sd(sample.data)
std.err &lt;- sample.sd/sqrt(sample.size)
t.stat &lt;- (sample.mean-null.mean)/std.err

t.crit &lt;- abs(qt(0.025,sample.size-1))   # for 2-tailed test

p.val &lt;- (1-pt(abs(t.stat),sample.size-1))*2   # 


### alternatively use the t.test function:

t.test(sample.data,mu=null.mean)   # should get the same answer!</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  sample.data
## t = 3.3424, df = 9, p-value = 0.008628
## alternative hypothesis: true mean is not equal to 10
## 95 percent confidence interval:
##  13.13360 26.25834
## sample estimates:
## mean of x 
##  19.69597</code></pre>
<div id="paired-t-test" class="section level4">
<h4>Paired t-test</h4>
<p>A common version of the one-sample t-test is the ‘paired t-test’, in which a measurement is taken on a single individual before and after a treatment is applied. An example of a paired t-test is the ‘weight loss drug’ example from the ‘basic concepts’ lecture. In the weight-loss drug’ example, we measured patients before and after taking a drug and measured the total weight change in each patient. Even though we have two measurements per patient (repeated measures!) we collapse the data for each patient into a single number (difference in weight) before running our t-test.</p>
</div>
</div>
<div id="two-sample-t-test" class="section level3">
<h3>Two-sample t-test</h3>
<p>A two-sample t-test tests the consistency of the sample data with the null hypothesis that sample A was generated from the same underlying population as sample B. The t-statistic in this case is expressed as:</p>
<p><span class="math inline">\(t = \frac{(\bar{x_A}-\bar{x_B})}{s.e._{pooled}}\)</span></p>
<p>Where <span class="math inline">\(\bar{x}\)</span> is the sample mean, <span class="math inline">\(s.e._{pooled}\)</span> is the pooled standard error of the mean across both samples, and <span class="math inline">\(\mu_0\)</span> is the population mean under the null hypothesis.</p>
<p>The t-statistic in the 2-sample case can be interpreted as the difference between the sample mean from population A and the sample mean from population B, in units of (pooled) standard errors.</p>
<p>The formula for the pooled standard error depends on whether your sample size is equal in the two samples and whether you are able to assume that the standard deviation is the same in the two underlying populations.</p>
<p>For example, the formula for the pooled standard error when we assume equal standard deviation in the two population but we have unequal sample size is:</p>
<p><span class="math inline">\(\sqrt{((N_1-1)*\sigma_1^2 + (N_2-1)*\sigma_2^2)/(N_{pooled}-2)}\)</span></p>
<p>For the 2-sample t-test, the degrees of freedom is equal to the total sample size across both samples minus 2.</p>
<pre class="r"><code>### two sample t-test 

sample.data.1 &lt;- rnorm(15,55,10)
sample.data.2 &lt;- rnorm(10,45,10)

sample.size.1 &lt;- length(sample.data.1)
sample.size.2 &lt;- length(sample.data.2)
sample.size.pooled &lt;- length(sample.data.1) + length(sample.data.2)

sample.mean1 &lt;- mean(sample.data.1)
sample.mean2 &lt;- mean(sample.data.2)

sample.sd1 &lt;- sd(sample.data.1)
sample.sd2 &lt;- sd(sample.data.2)
sample.sd.pooled &lt;- sqrt(((sample.size.1-1)*sample.sd1^2 + (sample.size.2-1)*sample.sd2^2)/(sample.size.pooled-2))

std.err.pooled &lt;- sample.sd.pooled*sqrt(1/sample.size.1+1/sample.size.2)
t.stat &lt;- (sample.mean1-sample.mean2)/std.err.pooled

t.crit &lt;- abs(qt(0.025,sample.size-1))   # for 2-tailed test

p.val &lt;- (1-pt(abs(t.stat),sample.size.pooled-2))*2   # 2-tailed test



### alternatively use the t.test function:

t.test(sample.data.1,sample.data.2,var.equal = T)   # should get the same answer!</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  sample.data.1 and sample.data.2
## t = 0.93552, df = 23, p-value = 0.3592
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -4.03479 10.69700
## sample estimates:
## mean of x mean of y 
##  55.40201  52.07090</code></pre>
<p><strong>Q</strong> What are the key assumptions of the 2-sample t-test?</p>
<p><strong>Q</strong> What is the null hypothesis for the 2-sample t-test?</p>
<div id="one-tailed-vs-two-tailed-tests" class="section level4">
<h4>One tailed vs two-tailed tests</h4>
<p>Tailed-ness is a common point of confusion when running t-tests and z-tests, so it’s worth taking a little time to review.</p>
<p>It all has to do with what we mean by ‘more extreme than the observed test statistic’ (from the definition of a p-value).</p>
<p>In a one-tailed test in which our (alternative) hypothesis is that our sample mean is <em>greater than</em> than the null hypothesis, we mean ‘more positive than the observed test statistic’.</p>
<p>In a one-tailed test in which our (alternative) hypothesis is that our sample mean is <em>less than</em> than the null hypothesis, we mean ‘more negative than the observed test statistic’.</p>
<p>In a two-tailed test, we mean ‘more positive or more negative than the absolute value of our observed test statistic’</p>
<p>Here is an R demo to illustrate the concept:</p>
<p>We have the following data:</p>
<pre class="r"><code># one vs two tailed demo

#my.data &lt;- rnorm(15, 0.5, 1)   # generate sample data
my.data &lt;- c(0.20119786,1.41700898,-0.72426698,0.44006284,0.01487128,-0.19031680,1.75470699,-0.81992816,2.31978530,  2.71442595,-0.31461411,0.52086138,-0.50580117,1.52260888,0.76454698)
samp.mean &lt;- mean(my.data)
samp.sd &lt;- sd(my.data)
samp.n &lt;- length(my.data)
std.err &lt;- samp.sd/sqrt(samp.n)

null.mean &lt;- 0

t.statistic &lt;- (samp.mean-null.mean)/std.err

### Two-tailed
curve(dt(x,samp.n-1),-3,3, main=&quot;Meaning of more extreme (two tailed version)&quot;,
      ylab=&quot;probability density&quot;,xlab=&quot;t statistic&quot;)    # visualize the sampling distribution of the t-statistic
abline(v=t.statistic,lwd=2,col=&quot;blue&quot;)

xs &lt;- seq(abs(t.statistic),10,0.05)                
ys &lt;- dt(xs,samp.n-1)
polygon(x=c(xs,rev(xs)),y=c(ys,rep(0,times=length(ys))),col=&quot;green&quot;,border=NA)
polygon(x=c(-xs,rev(-xs)),y=c(ys,rep(0,times=length(ys))),col=&quot;green&quot;,border=NA)

p.twosided &lt;- pt(-abs(t.statistic),samp.n-1)*2     # two-tailed p-value

text(-2,0.3,paste(&quot;p =&quot;,round(p.twosided,4)))</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>### One-sided (alternative = &#39;greater&#39;)
curve(dt(x,samp.n-1),-3,3, main=&quot;Meaning of more extreme (one tailed version: greater than)&quot;,
      ylab=&quot;probability density&quot;,xlab=&quot;t statistic&quot;)    # visualize the sampling distribution of the t-statistic
abline(v=t.statistic,lwd=2,col=&quot;blue&quot;)

xs &lt;- seq(t.statistic,10,0.05)                
ys &lt;- dt(xs,samp.n-1)
polygon(x=c(xs,rev(xs)),y=c(ys,rep(0,times=length(ys))),col=&quot;green&quot;,border=NA)

p.onesided &lt;- pt(-abs(t.statistic),samp.n-1)     # one-tailed p-value

text(-2,0.3,paste(&quot;p =&quot;,round(p.onesided,4)))</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<p>In the same way, the critical value for a t-statistic is different for a two-tailed vs a one-tailed test:</p>
<pre class="r"><code>### t-crit in one tailed vs two tailed test

sample.size=7

curve(dt(x,sample.size-1),-8,4, main=&quot;2-tailed vs 1-tailed critical value&quot;,
      xlab=&quot;t-statistic&quot;,ylab=&quot;probability density&quot;)

alpha &lt;- 0.1
t.crit.twosided &lt;- qt(alpha/2,sample.size-1) 

abline(v=c(t.crit.twosided,abs(t.crit.twosided)),col=&quot;red&quot;,lwd=2)


t.crit.twosided &lt;- qt(alpha/2,sample.size-1) 

abline(v=c(t.crit.twosided,abs(t.crit.twosided)),col=&quot;red&quot;,lwd=2)

t.crit.onesided &lt;- qt(alpha,sample.size-1) 

abline(v=abs(t.crit.onesided),col=&quot;green&quot;,lwd=2)
abline(v=t.crit.onesided,col=&quot;blue&quot;,lwd=2)

legend(&quot;topleft&quot;,lwd=c(2,2,2),col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;),bty=&quot;n&quot;,legend=c(&quot;two-tailed crit value&quot;,&quot;one-tailed crit value (greater than)&quot;,&quot;one-tailed crit value (less than)&quot;))</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>All of these critical values have the same percentage of the sampling distribution that are more extreme than them (the percentage is alpha, which is set at 0.1 in this example)– but the definition of ‘extremeness’ differs for the one-tailed and two-tailed cases!</p>
</div>
<div id="welchs-t-test" class="section level4">
<h4>Welch’s t-test</h4>
<p>Welch’s t-test allows for unequal sample sizes AND unequal variance in the two populations. The Welch’s t-test is like the two-sample t-test we already ran, but it is more flexible. Like other t-tests, this test is robust against violations of normality due to the CLT. This is the DEFAULT version of a t-test in R. If you want a classical Student’s t-test, you need to tell it to do so (otherwise it will do the Welch’s test; use the ‘var.equal = TRUE’ argument to ru the classical test)!</p>
<p>I won’t go through the math for the Welch’s t-test (you can look it up if you’d like) because the basic concepts and interpretation are the same. The differences lie in the computation of the pooled variance and the degrees of freedom.</p>
</div>
</div>
</div>
<div id="t-tests-in-r" class="section level2">
<h2>t-tests in R</h2>
<p>Previously, we have mostly avoided using R’s built in ‘t-test’ function. Let’s use it now!</p>
<pre class="r"><code>######
# More t-test examples


#### 
# T-tests
####

#Ttest
# Are my data greater than zero? 
Group0 &lt;- c(0.5, -0.03, 4, 2.5, 0.89, 2.2, 1.7, 1.125)
hist(Group0)
t.test(Group0,alternative=&quot;greater&quot;) # This gets at directionality</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  Group0
## t = 3.5487, df = 7, p-value = 0.00468
## alternative hypothesis: true mean is greater than 0
## 95 percent confidence interval:
##  0.7507388       Inf
## sample estimates:
## mean of x 
##  1.610625</code></pre>
<pre class="r"><code>#Are my data different than zero? 
Group0 &lt;- c(0.5, -0.03, 4, 2.5, 0.89, 2.2, 1.7, 1.125)
hist(Group0)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>t.test(Group0) # Okay, that&#39;s to zero. What about if it&#39;s different than 1? </code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  Group0
## t = 3.5487, df = 7, p-value = 0.00936
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  0.5374007 2.6838493
## sample estimates:
## mean of x 
##  1.610625</code></pre>
<pre class="r"><code># are my data different than 1? 
t.test(Group0, mu=1)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  Group0
## t = 1.3454, df = 7, p-value = 0.2205
## alternative hypothesis: true mean is not equal to 1
## 95 percent confidence interval:
##  0.5374007 2.6838493
## sample estimates:
## mean of x 
##  1.610625</code></pre>
<pre class="r"><code># Now let&#39;s test two groups. 
# are the means equal? 
group1 &lt;- c(7,9,6,6,6,11,6,3,8,7)
group2 &lt;- c(11,13,8,6,14,11,13,13,10,11)
t.test(group1, group2, var.equal=T) # Notice how we set equal variance? Look at the output - &quot;Two Sample t-test.&quot;</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  group1 and group2
## t = -3.9513, df = 18, p-value = 0.000936
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -6.27997 -1.92003
## sample estimates:
## mean of x mean of y 
##       6.9      11.0</code></pre>
<pre class="r"><code># is this one-tail or two? 

group1 &lt;- c(7,9,6,6,6,11,6,3,8,7)
group2 &lt;- c(11,13,8,6,14,11,13,13,10,11)
t.test(group1, group2) #  &quot;Welch&#39;s Two Sample t-test&quot;</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  group1 and group2
## t = -3.9513, df = 17.573, p-value = 0.0009743
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -6.283771 -1.916229
## sample estimates:
## mean of x mean of y 
##       6.9      11.0</code></pre>
<pre class="r"><code># WELCH&#39;S TEST IS THE DEFAULT IN R</code></pre>
</div>
<div id="t-test-assumptions" class="section level2">
<h2>t-test assumptions</h2>
<div id="normality-of-the-data" class="section level3">
<h3>Normality of the data</h3>
<p>Like we have mentioned before, the t-test is quite robust to non-normality of the raw data – this is due to the Central Limit Theorem!</p>
<p>However, if the data are strongly skewed or otherwise clearly non-normal you may want to go with a non-parametric alternative (see below)!</p>
<div id="testing-for-normality" class="section level4">
<h4>Testing for normality!</h4>
<p>Before ‘accepting’ the results of a statistical test, we often want to run ‘goodness-of-fit’ tests to see if there is any reason to think we may be violating key assumptions of our statistical analyses. For the t-test and many other parametric tests, one ‘goodness-of-fit’ test we might want to run is a test for normality. For linear regression analyses, we test for normality of the residuals. For the t-test, we test for normality of the raw data.</p>
<p>Here we will discuss two tests- one visual and one quantitative. The visual test is called a ‘quantile-quantile (Q-Q) plot’ and the quantitative test is called a Shapiro-Wilk test.</p>
<p>The quantile-quantile plot compares the quantiles of the observed data (y axis) with the theoretical quantiles from a normal distribution (x axis). If the q-q plot is highly non-linear, your data are probably not normally distributed and you might want to run a non-parametric alternative.</p>
<p>The Shapiro-Wilk test is a way of quantitatively assessing the linearity of the q-q plot such that you can get a p-value. The p-value here is interpreted the same as any other p-value. The catch is that the null hypothesis is that the data are normally distributed. If p&gt;alpha you fail to reject the null - that is, you can move forward assuming that the data are normally distributed!</p>
<pre class="r"><code>## testing for normality

my.data.nonnorm &lt;- rgamma(20,0.1,0.1)    # simulate some non-normal data
hist(my.data.nonnorm,main=&quot;non-normal&quot;)     # visualize the data distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>my.data.norm &lt;- rnorm(20,6,0.9)    # simulate some normal data
hist(my.data.norm,main=&quot;normal&quot;)     # visualize the data distribution</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<pre class="r"><code># visualize q-q plot

qqnorm(my.data.nonnorm,main=&quot;non-normal&quot;)   # visual test for normality</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-7-3.png" width="672" /></p>
<pre class="r"><code>qqnorm(my.data.norm,main=&quot;normal&quot;)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-7-4.png" width="672" /></p>
<pre class="r"><code># run shapiro-wilk test

shapiro.test(my.data.nonnorm)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  my.data.nonnorm
## W = 0.45799, p-value = 1.355e-07</code></pre>
<pre class="r"><code>shapiro.test(my.data.norm)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  my.data.norm
## W = 0.91826, p-value = 0.09172</code></pre>
</div>
</div>
<div id="outliers" class="section level3">
<h3>Outliers</h3>
<p>The t-test can be highly affected by outliers. If you suspect your data contain highly influential outliers, you might want to run a non-parametric alternative, as non-parametric tests are much less influenced by outliers.</p>
<p>To look for outliers, the first step is to simply visualize a histogram of the raw data and look for any observations that fall far from the others.</p>
</div>
</div>
<div id="non-parametric-alternatives" class="section level2">
<h2>Non-parametric alternatives</h2>
<p>If your data are highly non-normal or contain obvious outliers, you probably want to run one of the non-parametric alternatives to the classical t-test. These tests do not assume normality of the data nor are they highly affected by outliers!</p>
<div id="one-sample-t-test-1" class="section level3">
<h3>One sample t-test</h3>
<p>The non-parametric alternative to the one-sample t-test is called the <strong>one sample Wilcoxon signed rank test</strong>. This analysis first reduces the data to signs- negative 1 if the observation is greater than the null and -1 if the data are less than the null. Then, like many non-parametric tests, you sort the data from smallest to largest absolute value. You then multiply the ranks times the signs and sum across all observations. This test statistic (W) has a known sampling distribution (approximately anyway!) under the null hypothesis and therefore you can compute a p-value. The null hypothesis for the Wilcoxon test refers to the median of the distribution and not the mean (the null hypothesis for the t-test refers to the mean). Therefore the two tests are not strictly comparable!</p>
<pre class="r"><code># Wilcoxon signed rank test

my.data &lt;- rgamma(20,0.1,0.1)-2

hist(my.data)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>wilcox.test(my.data)</code></pre>
<pre><code>## 
##  Wilcoxon signed rank exact test
## 
## data:  my.data
## V = 2, p-value = 5.722e-06
## alternative hypothesis: true location is not equal to 0</code></pre>
<pre class="r"><code>t.test(my.data)   # t-test for comparison</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  my.data
## t = -9.5629, df = 19, p-value = 1.076e-08
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -2.070901 -1.327165
## sample estimates:
## mean of x 
## -1.699033</code></pre>
</div>
<div id="two-sample-t-test-1" class="section level3">
<h3>Two sample t-test</h3>
<p>The non-parametric alternative to the two-sample t-test is called the <strong>Mann Whitney U test (or Wilcoxon rank sum test)</strong>. This analysis first reduces the data to ranks- that is, you first sort the data from smallest to largest absolute value. You then essentially test to see if the ranks of one group are higher on average than the ranks of the other group. This test statistic (U) has a known sampling distribution (approximately anyway!) under the null hypothesis and therefore you can compute a p-value. The null hypothesis for the Mann-Whitney test is that there is a 50% probability that a single sample from one distribution is larger than a single sample drawn from the other distribution. The null hypothesis for the t-test is that the means of two distributions are the same. Therefore the two tests are not strictly comparable!</p>
<pre class="r"><code># Wilcoxon signed rank test

my.data1 &lt;- rgamma(20,0.1,0.1)-2
my.data2 &lt;- rgamma(20,0.2,0.1)-2

median(my.data1)</code></pre>
<pre><code>## [1] -1.980554</code></pre>
<pre class="r"><code>median(my.data2)</code></pre>
<pre><code>## [1] -1.745295</code></pre>
<pre class="r"><code>wilcox.test(my.data1,my.data2)</code></pre>
<pre><code>## 
##  Wilcoxon rank sum exact test
## 
## data:  my.data1 and my.data2
## W = 147, p-value = 0.1572
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<pre class="r"><code>t.test(my.data1,my.data2)   # t-test for comparison</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  my.data1 and my.data2
## t = 0.66995, df = 22.092, p-value = 0.5098
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -2.271395  4.439958
## sample estimates:
##  mean of x  mean of y 
##  0.2931888 -0.7910931</code></pre>
<pre class="r"><code>#### perform rank test from scratch!

allobs &lt;- c(my.data1,my.data2)
inorder &lt;- order(allobs)

rank1 &lt;- inorder[1:20]
rank2 &lt;- inorder[21:40]

t.test(rank1,rank2,var.equal = T)    # perform t-test on the ranks (usually similar to Mann-Whitney test)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  rank1 and rank2
## t = -0.26726, df = 38, p-value = 0.7907
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -8.574589  6.574589
## sample estimates:
## mean of x mean of y 
##        20        21</code></pre>
</div>
</div>
<div id="power-analysis-for-t-test" class="section level2">
<h2>Power analysis for t-test</h2>
<p>Okay, before we leave the t-test, let’s run a power analysis! Remember that a power analysis is a way of assessing whether you are likely to get a ‘significant’ result given your null hypothesis is false. Power analyses can be a great way to assess if your sampling design is adequate to detect a signal. If not, you may want to alter your sampling design by adding more replicates!</p>
<pre class="r"><code>###### First we will set the population parameters:

true.mean.A &lt;- 13.5
true.mean.B &lt;- 13.9

true.sd &lt;- 3.4

# Assume a two-tailed test- alternative hypothesis is that the mean of A is different from the mean of B

###### Now let&#39;s set the sampling scenario

sampsize.A &lt;- 10
sampsize.B &lt;- 12

###### now we will simulate a single &#39;experiment&#39;

samp.A &lt;- rnorm(sampsize.A,true.mean.A,true.sd)
samp.B &lt;- rnorm(sampsize.B,true.mean.B,true.sd)

###### and now we can run a test!

this.test &lt;- t.test(samp.A,samp.B,var.equal = T)

###### and determine if we rejected our null hypothesis (which we know is not true!)

this.test$p.value &lt; 0.05</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>Okay, but if we really want to compute power we need to do this many times and see what percent of samples allow us to reject the null!</p>
<pre class="r"><code>### full power analysis:

###### First we will set the population parameters:

true.mean.A &lt;- 13.5
true.mean.B &lt;- 13.9

true.sd &lt;- 3.4

# Assume a two-tailed test- alternative hypothesis is that the mean of A is different from the mean of B

###### Now let&#39;s set the sampling scenario

sampsize.A &lt;- 10
sampsize.B &lt;- 12

###### now we will simulate LOTS of  &#39;experiments&#39;

pvals &lt;- numeric(1000)

for(i in 1:1000){
  samp.A &lt;- rnorm(sampsize.A,true.mean.A,true.sd)
  samp.B &lt;- rnorm(sampsize.B,true.mean.B,true.sd)
  
  ###### and now we can run a test!
  
  this.test &lt;- t.test(samp.A,samp.B,var.equal = T)
  
  ###### and determine if we rejected our null hypothesis (which we know is not true!)
  
  pvals[i] &lt;- this.test$p.value
}

 # hist(pvals)

length(which(pvals&lt;0.05))/1000</code></pre>
<pre><code>## [1] 0.056</code></pre>
<p>Of course, if your power is not adequate, you may want to increase sample size. But what is the minimum sample size that gives you sufficient power? We can use simulation to answer this question!</p>
<pre class="r"><code>### full power analysis WITH SAMPLE SIZE DETERMINATION:

###### First we will set the population parameters:

true.mean.A &lt;- 13.5
true.mean.B &lt;- 13.9

true.sd &lt;- 3.4

# Assume a two-tailed test- alternative hypothesis is that the mean of A is different from the mean of B


###### now we will simulate LOTS of  &#39;experiments&#39; under different sample sizes

sampsize &lt;- seq(5,400,10)

power &lt;- numeric(length(sampsize))

for(j in 1:length(sampsize)){
  
  pvals &lt;- numeric(1000)
  for(i in 1:1000){
    samp.A &lt;- rnorm(sampsize[j],true.mean.A,true.sd)
    samp.B &lt;- rnorm(sampsize[j],true.mean.B,true.sd)
    
    ###### and now we can run a test!
    
    this.test &lt;- t.test(samp.A,samp.B,var.equal = T)
    
    ###### and determine if we rejected our null hypothesis (which we know is not true!)
    
    pvals[i] &lt;- this.test$p.value
  }

 # hist(pvals)

  power[j] &lt;- length(which(pvals&lt;0.05))/1000
}

names(power) &lt;- sampsize

plot(power~sampsize)</code></pre>
<p><img src="LECTURE2_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>So what sample size would you need (in each of the two groups) to detect a signal in this case?</p>
<p><a href="LECTURE3.html">–go to next lecture–</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
