<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 710" />


<title>Linear Regression</title>

<script src="site_libs/header-attrs-2.27/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 710</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="syllabus.html">Syllabus</a>
</li>
<li>
  <a href="schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 710</a>
    </li>
    <li>
      <a href="LECTURE1.html">Basic Concepts</a>
    </li>
    <li>
      <a href="LECTURE2.html">Sampling uncertainty</a>
    </li>
    <li>
      <a href="LECTURE3.html">Taxonomy of common statistics</a>
    </li>
    <li>
      <a href="LECTURE4.html">t-test and z-test</a>
    </li>
    <li>
      <a href="LECTURE5.html">chi-squared tests</a>
    </li>
    <li>
      <a href="LECTURE6.html">Linear Regression</a>
    </li>
    <li>
      <a href="LECTURE7.html">ANOVA</a>
    </li>
    <li>
      <a href="LECTURE8.html">GLM</a>
    </li>
    <li>
      <a href="LECTURE9.html">GLMM</a>
    </li>
    <li>
      <a href="LECTURE10.html">Machine Learning</a>
    </li>
    <li>
      <a href="GIT_tutorial.html">Intro to GIT</a>
    </li>
    <li>
      <a href="LECTURE11.html">Next steps</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="EXERCISE1.html">Exercise 1 - Data Summary Functions</a>
    </li>
    <li>
      <a href="EXERCISE2.html">Exercise 2 - t-tests</a>
    </li>
    <li>
      <a href="EXERCISE3.html">Exercise 3 - Simple Linear Regression</a>
    </li>
    <li>
      <a href="EXERCISE4.html">Exercise 4 - Multiple Linear Regression</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    More Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Links.html">Links</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final Projects</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Linear Regression</h1>
<h4 class="author">NRES 710</h4>
<h4 class="date">Fall 2024</h4>

</div>


<div id="download-the-r-code-for-this-lecture" class="section level2">
<h2>Download the R code for this lecture!</h2>
<p>To follow along with the R-based lessons and demos, <a
href="LECTURE6.R">right (or command) click on this link and save the
script to your working directory</a></p>
</div>
<div id="overview-linear-regression" class="section level2">
<h2>Overview: Linear Regression</h2>
<p>Classical linear regression involves testing for a relationship
between a continuous response variable (dependent variable) and a
continuous predictor variable (independent variable).</p>
<p>You can have multiple explanatory variables (multiple linear
regression). But we will focus on <em>simple linear regression</em> here
(i.e., only one predictor variable).</p>
<p>The null hypothesis is that there is no relationship between the
response variable and the predictor variable in your population of
interest. That is, values of your predictor variable are useless for
predicting your response variable.</p>
<div id="simple-example" class="section level3">
<h3>Simple example</h3>
<p>Imagine we are testing for a relationship between the brightness of
artificial lighting on coastal beaches (e.g., from hotels and other
forms of development) and the total number of hatchling sea turtles per
nest that successfully make it to the ocean.</p>
<p><img src="statistics1.png" style="width:30.0%" /></p>
<p><em>Population</em>: All nests deposited on coastal beaches across
the region of interest.<br />
<em>Parameter(s)</em>: The relationship between the mean number of
successful hatchlings per nest and the brightness of artificial
lighting.<br />
<em>Sample</em>: All monitored nests (hopefully a representative sample
of the population)<br />
<em>Statistic(s)</em>: Slope and intercept of the linear relationship
between the measured response variable (number of successful
ocean-arrivers per nest) and the predictor variable (brightness of
artificial lighting)</p>
</div>
</div>
<div id="some-more-specifics" class="section level2">
<h2>Some more specifics!</h2>
<div id="the-process-model" class="section level3">
<h3>The process model</h3>
<p>We assume there is some true model out there describing the expected
(mean) value of our response variable <em>y</em> as a linear function of
our predictor variable <em>x</em>:</p>
<p><span class="math inline">\(E(y)= \beta_0 + \beta_1\cdot
x\)</span></p>
<p>To interpret this equation: the true mean of our response variable
(<span class="math inline">\(E(y)\)</span>) is computed by taking the
true intercept (<span class="math inline">\(\beta_0\)</span>) and adding
the product of the true slope term (<span
class="math inline">\(\beta_1\)</span>) and our predictor variable. This
is just another way of saying that the expected value of the response
variable is computed as a linear function of the predictor variable.
<span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_0\)</span> are both <em>parameters</em> that
we wish to estimate!</p>
</div>
<div id="the-noise" class="section level3">
<h3>The noise</h3>
<p>To complete the thought, we also assume that there is some “noise” in
the system. The “noise” term in regression and ANOVA is also known as
the <em>error</em>. Specifically, we assume that the noise is normally
distributed with mean of zero and standard deviation of <span
class="math inline">\(\sigma\)</span>.</p>
<p>Mathematically, we are assuming that our data/sample was generated
from this process:</p>
<p><span class="math inline">\(y= \beta_0 + \beta_1\cdot x +
\epsilon\)</span></p>
<p>OR:</p>
<p><span class="math inline">\(y = E(y) + \epsilon\)</span></p>
<p>WHERE:</p>
<p><span class="math inline">\(\epsilon \equiv Normal(0,
\sigma)\)</span></p>
<p>This is actually the same assumption we made for a one-sample
t-test!</p>
<p>For a t-test, we assumed that there is a true population mean <span
class="math inline">\(\mu\)</span> (equivalent to <span
class="math inline">\(E(y)\)</span>) and that the true “noise” is
normally distributed with standard deviation of <span
class="math inline">\(\sigma\)</span>.</p>
<p>As with a t-test, where we can only approximate the true population
mean by computing the sample mean, we can only approximate the linear
relationship between our response and predictor variables:</p>
<p><span class="math inline">\(\bar{y} = \hat{B_0} + \hat{B_0}\cdot
x\)</span></p>
<p>Just like any other statistical test, we assume that our observed
linear relationship (defined by coefficients <span
class="math inline">\(\hat{B_0}\)</span> and <span
class="math inline">\(\hat{B_1}\)</span>) is just one of many such
possible relationships that <em>could have been derived</em> from random
sampling from our population of interest. If we collected a different
sample, we would get a different linear relationship.</p>
<p>NOTE: in linear regression we are generally far more interested in
the slope of the linear relationship (<span
class="math inline">\(\hat{B_1}\)</span> rather than the intercept). So
for now, we assume <span class="math inline">\(\hat{B_1}\)</span> (slope
between response and predictor, computed from the sample) is the main
test statistic of interest!</p>
<p>So.. what is the sampling distribution for our test statistic <span
class="math inline">\(\hat{B_1}\)</span> under the null hypothesis in
this case? Well, the answer is that it (when converted to units of
standard error) is t-distributed! Let’s look into this a bit more.</p>
</div>
</div>
<div id="regression-and-t-tests--the-link" class="section level2">
<h2>Regression and t-tests- the link!</h2>
<p>Our discussion of t-tests actually rolls us straight into linear
regression. Why? How?</p>
<p>In a one-sample t-test we are interested in estimating the true
population mean, and we assume that our t-statistic (i.e., deviation of
the sample mean from the null-hypothesis mean, in units of standard
error) is t-distributed.</p>
<p>What is the null hypothesis of a linear regression? (Slope = 0 - that
is, the true relationship is zero).</p>
<p>So already we are seeing a bit of a similarity.</p>
<p>In a t-test we assume that the population mean is equal to the null
mean and that the data are normally distributed. We could write this as
(using regression notation):</p>
<p><span class="math inline">\(y = \beta_0 + \epsilon\)</span></p>
<p>WHERE:</p>
<p><span class="math inline">\(\epsilon \equiv Normal(0,
\sigma)\)</span></p>
<p>In the above equation, <span class="math inline">\(\beta_0\)</span>
represents the population mean under the null hypothesis.</p>
<p>We approximate our population mean using the sample mean <span
class="math inline">\(\bar{\beta_0}\)</span> (formerly known as <span
class="math inline">\(\bar{x}\)</span>) and we use the CLT and other
statistical theories to show that the t-statistic:</p>
<p><span class="math inline">\(t =
\frac{\bar{\beta_0}-\beta_0}{StdErr(\beta_0)}\)</span></p>
<p>Is t-distributed with df computed as the sample size minus the number
of parameters estimated in the model (there is only one estimated
parameter- the sample mean <span
class="math inline">\(\bar{\beta_0}\)</span>).</p>
<p>In linear regression we assume that the mean of our response is
determined by two parameters- the intercept and the slope (linear
relationship with the predictor variable). The null hypothesis (usually)
is that the true mean is defined only by the intercept term (overall
mean response value) and there is no relationship with the predictor
variable (slope term is equal to zero).</p>
<p>The slope term of the linear regression can be computed as:</p>
<p><span class="math inline">\(\hat{\beta_1} =
\frac{\sum_{i=1}^{n}{(x_i-\bar{x})(y_i-\bar{y})}}{\sum_{i-1}^{n}{(x_i-\bar{x})^2}}\)</span></p>
<p>And the intercept term can be computed as:</p>
<p><span class="math inline">\(\hat{\beta_0} = \bar{y} -
\beta_1*\bar{x}\)</span></p>
<p>Simple linear regression models (that is, the slope and intercept
terms above) are fitted using “least squares”. The best fit model
minimizes the sum of the squared residuals. DRAW THIS OUT.</p>
<p>The standard error of the slope term (as opposed to the standard
error of the mean) is computed as:</p>
<p><span class="math inline">\(std.err_{\hat{\beta_1}} =
\sqrt{\frac{\frac{1}{n-2}\sum_{i=1}^n{\hat\epsilon_i^2}}{\sum_{i=1}^n{(x_i-\bar{x})^2}}}\)</span></p>
<p>Where the <span class="math inline">\(\hat\epsilon_i\)</span> refers
to the residual errors.</p>
<p>We can then compute a t-statistic for the slope term (difference from
the sample slope term and the null slope term in units of standard
error):</p>
<p><span
class="math inline">\(t=\frac{\hat{\beta_1}-\beta_{1null}}{std.err_{\hat{\beta_1}}}\)</span></p>
<p>Just like with the t-test, we assume that this t-statistic is
t-distributed under the null hypothesis. This time the degrees of
freedom is 2 less than the sample size (since computing the residual
error requires computing two parameters: mean and slope).</p>
</div>
<div id="simple-linear-regression-examples" class="section level2">
<h2>Simple linear regression: examples</h2>
<p>Okay let’s consider the sea turtle example from the beginning of
lecture:</p>
<p>Imagine we are testing for a relationship between the brightness of
artificial lighting on coastal beaches of the eastern USA (e.g., from
hotels and other forms of development) and the total number of hatchling
sea turtles per nest that successfully make it to the ocean.</p>
<p>First we will simulate some data under a known process model (don’t
worry if you don’t totally follow this data simulation code):</p>
<pre class="r"><code># Examples --------------------------------------

eggs.per.nest &lt;- 100
n.nests &lt;- 15
light &lt;- rnorm(n.nests,50,10)   # make up some light pollution values (predictor var)

probsucc &lt;- function(light){    # egg success as a function of light pollution
  plogis(1.5-0.01*light)
}

hatchlings.successful &lt;- rbinom(n.nests,eggs.per.nest,probsucc(light))   # determine number of successful eggs (response var)

#curve(probsucc,0,100)

plot(hatchlings.successful~light)  # plot the data</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-2-1.png" width="672" />
Now that we have data, let’s run a simple linear regression analysis!
This one time we will compute the statistics directly from the data
without using R functions.</p>
<pre class="r"><code>slope &lt;- sum((light-mean(light))*(hatchlings.successful-mean(hatchlings.successful)))/sum((light-mean(light))^2)
intercept &lt;- mean(hatchlings.successful) - slope*mean(light)

exp.successful &lt;- intercept+slope*light # expected number of eggs for each observation
residuals &lt;- hatchlings.successful-exp.successful

stderr &lt;- sqrt(((1/(n.nests-2))*sum(residuals^2))/(sum((light-mean(light))^2)))    # standard error

t.stat &lt;- (slope-0)/stderr    # t statistic

pval &lt;- 2*pt(t.stat,n.nests-2)    # p value</code></pre>
<p>A much easier approach is to use R’s <code>lm()</code> function:</p>
<pre class="r"><code># use lm() function instead (easy way!)

model &lt;- lm(hatchlings.successful~light)

summary(model)   # get the same t stat and p-value hopefully!</code></pre>
<pre><code>## 
## Call:
## lm(formula = hatchlings.successful ~ light)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -7.691 -4.095  1.051  3.135  9.201 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   87.390      6.363  13.734 4.07e-09 ***
## light         -0.252      0.128  -1.969   0.0706 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.116 on 13 degrees of freedom
## Multiple R-squared:  0.2298, Adjusted R-squared:  0.1705 
## F-statistic: 3.878 on 1 and 13 DF,  p-value: 0.07062</code></pre>
<p>And we can plot the regression line on top of a scatterplot:</p>
<pre class="r"><code># plot regression line!

plot(hatchlings.successful~light)  # plot the data
abline(intercept,slope,col=&quot;blue&quot;)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Of course, we can’t end with a single regression line, since there is
sampling error associated with our best-fit regression line. There is
error associated with the intercept and there is error associated with
the slope term. To do this in r, we can use the ‘predict’ function:</p>
<pre class="r"><code># add confidence interval on the regression line

newdata &lt;- data.frame(    # make a data frame containing the light values we want to make predictions for (spanning the range of light values in our data)
  light = seq(20,80,1)
)

my.predict &lt;- predict(model, newdata = newdata, interval = &quot;confidence&quot;)  # 95% conf int by default

plot(hatchlings.successful~light)  # plot the data
abline(intercept,slope,col=&quot;blue&quot;)
lines(newdata$light,my.predict[,&quot;upr&quot;],col=&quot;red&quot;,lty=2)   # add upper bound
lines(newdata$light,my.predict[,&quot;lwr&quot;],col=&quot;red&quot;,lty=2)   # add lower bound</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>This confidence interval loosely represents the range of plausible
best-fit lines we could fit if we were to gather multiple alternative
datasets. If some of the plausible lines that fit within the upper and
lower bounds of the confidence interval on the regression line have a
positive slope and others have a negative slope, we might remain
unconvinced that the relationship between the response and predictor
variable is ‘statistically significant’!</p>
<p><strong>Q:</strong> take a couple minutes to run a simple linear
regression of tree Volume as a function of tree Girth using the built-in
<code>trees</code> dataset.</p>
</div>
<div id="assumptions-of-simple-linear-regression"
class="section level2">
<h2>Assumptions of simple linear regression</h2>
<div id="normal-error-distribution" class="section level3">
<h3>Normal error distribution</h3>
<p>Simple linear regression assumes that the model
<strong>residuals</strong> are normally distributed.</p>
<p>Let’s look at the residuals for our sea turtle example:</p>
<pre class="r"><code>my.intercept &lt;- model$coefficients[&quot;(Intercept)&quot;]
my.slope &lt;- model$coefficients[&quot;light&quot;]
expected.vals &lt;- my.intercept+my.slope*light 
my.residuals &lt;- hatchlings.successful-expected.vals
my.residuals</code></pre>
<pre><code>##  [1] -6.38350623  1.48524346  9.20122195 -7.69060240  3.39148668  1.05121837
##  [7]  2.46384427 -0.02049669  2.87815053  4.10643391 -4.14153963 -4.04784268
## [13] -2.07932376  5.88339043 -6.09767819</code></pre>
<pre class="r"><code>### alternative way of getting residuals (best way!)

my.residuals2 &lt;- model$residuals

### alternative way using predict function

my.residuals3 &lt;- hatchlings.successful-predict(model)

### histogram of residuals

hist(my.residuals)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>### test for normality

qqnorm(my.residuals)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<pre class="r"><code>shapiro.test(my.residuals)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  my.residuals
## W = 0.96481, p-value = 0.7753</code></pre>
</div>
<div id="linear-model" class="section level3">
<h3>Linear model</h3>
<p>The true relationship between the response and predictor variables is
linear!</p>
<p>Which one of the following plots violates this assumption?</p>
<pre class="r"><code>layout(matrix(1:4,nrow=2,byrow = T))
plot(anscombe$y1~anscombe$x1,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y2~anscombe$x2,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y3~anscombe$x3,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y4~anscombe$x4,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="independence-of-observations" class="section level3">
<h3>Independence of observations</h3>
<p>Of course! All classical analyses make this assumption!</p>
</div>
<div id="equal-variance-homoscedasticity-or-lack-of-heteroscedasticity"
class="section level3">
<h3>Equal variance (homoscedasticity, or lack of
heteroscedasticity)</h3>
<p>In simple linear regression, we assume that the model residuals are
normally distributed– and that the spread of that distribution does not
change as your predictor variable or response variable goes from small
to large. That is, the residuals are <strong>homoskedastic</strong> –
which means they have equal variance across the range of the predictor
and response variables.</p>
<p>Let’s look at the sea turtle example:</p>
<pre class="r"><code># evaluate assumptions -----------------------------

my.residuals &lt;- model$residuals
my.standardized.residuals &lt;- rstandard(model)    # even better...

plot(my.residuals~predict(model))    # plot residuals against fitted values</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code># plot(my.standardized.residuals~predict(model))</code></pre>
<p>Do you see any evidence for <strong>heteroskedasticity</strong>?
Heteroskedasticity means non-homogeneity of variance across the range of
the predictor variable. Serious violations of the equal variance
assumption may warrant a <strong>transformation</strong> of your data,
or you may choose to use an alternative analysis like a
<strong>generalized linear model</strong>.</p>
</div>
<div id="predictor-variable-is-known-with-certainty"
class="section level3">
<h3>Predictor variable is known with certainty</h3>
<p>That is, the observed (e.g., measured) values for your predictor
variable are correct and known with precision. In standard regression
analyses, the randomness in linear regression is associated only with
the response variable - and that randomness is normally distributed.</p>
</div>
</div>
<div id="diagnostic-plots" class="section level2">
<h2>Diagnostic plots</h2>
<p>Simple linear regression analyses are generally accompanied by
‘diagnostic plots’, which are intended to diagnose potential violations
of key assumptions, or other potential pitfalls of regression.</p>
<p>When you use the ‘plot’ function in R to evaluate a model generated
with the ‘lm’ function, R returns four diagnostic plots:</p>
<pre class="r"><code>layout(matrix(1:4,2,byrow = T))
plot(model)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The first diagnostic plot (residuals vs fitted) lets you check for
possible non-linear patterns. This plot should have no obvious pattern
to it- it should look like random noise across the range of fitted
values.</p>
<p>The second diagnostic plot (normal Q-Q) lets you check for normality
of residuals. You already know how to do this.</p>
<p>The third diagnostic plot (scale-location) lets you check for
homoskedasticity. This plot should have no obvious pattern to it- it
should look like random noise across the range of fitted values.</p>
<p>The fourth diagnostic plot (residuals vs leverage) lets you check to
make sure your regression model isn’t driven by a few
<strong>influential points</strong>. Look for points that fall far to
the right of the other points- these are high leverage points. Also look
specifically at the upper and lower right hand side of this figure-
points in these regions (with large values for “Cook’s distance”) have
the property that if they were removed from the analysis the results
would change substantially.</p>
<p>These four figures, taken together, should be a guide to interpreting
how robust your regression is. Analyze these plots and make decisions
about what you’re willing to accept.</p>
<div id="influential-points" class="section level3">
<h3>Influential points</h3>
<p>Which of the following plots has a highly influential point?</p>
<pre class="r"><code># which one has high influence point?

layout(matrix(1:4,nrow=2,byrow = T))
plot(anscombe$y1~anscombe$x1,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y2~anscombe$x2,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y3~anscombe$x3,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y4~anscombe$x4,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Can you try to draw a regression line if this point was removed?</p>
</div>
</div>
<div id="performance-evaluation" class="section level2">
<h2>Performance evaluation</h2>
<div id="r-squared" class="section level3">
<h3>R-squared</h3>
<p>The coefficient of determination, also known as R-squared, is a
statistic that is commonly used to indicate the performance of a
regression model. Specifically, R-squared tells you how much of the
total variance in your response variable is explained by your predictor
variable. The maximum value for R-squared is 1- values close to 1
indicate a very “good” model! An R-squared of zero indicates that the
model with the predictor variable is no better than a model that simply
predicts the overall mean of the response variable no matter what the
predictor variable is.</p>
<p>R-squared can be computed as:</p>
<p><span class="math inline">\(R_2 =
1-\frac{SS_{res}}{SS_{tot}}\)</span></p>
<p>where <span class="math inline">\(SS_{tot} =
\sum(y_i-\bar{y})^2\)</span> and <span class="math inline">\(SS_{res} =
\sum(y_i-y_{pred})^2\)</span>.</p>
<p>In the model summary output for <code>lm()</code> objects in R, you
will see an R-squared value, and also an “adjusted R-squared” value.
This is only valuable for multiple linear regression. The adjusted
R-squared value represents the proportion of variance in your response
variable that is explained by “significant” predictor variables. If you
include a bunch of meaningless predictor variables, the overall R2 will
still go up (some of the variation in your data will be “explained” by
the meaningless noise). The adjusted R-squared corrects for this
effect!</p>
</div>
</div>
<div id="regression-outcomes" class="section level2">
<h2>Regression outcomes</h2>
<p>Let’s explore some possible outcomes of linear regression:</p>
<p>Try to come up with scenarios (with plots) for each of the
following:</p>
<ol style="list-style-type: decimal">
<li>Non-significant p, high R2<br />
</li>
<li>Significant p, low R2<br />
</li>
<li>Significant p, high R2<br />
</li>
<li>Non-significant p, low R2</li>
</ol>
</div>
<div id="linear-regression-vs-correlation" class="section level2">
<h2>Linear regression vs correlation</h2>
<p>When you run a simple linear regression you are assuming that one
variable is your response variable and the other variable is a predictor
variable. You are essentially modeling only one of these variables (the
response) as a random variable and the other (the predictor variable) as
a set of fixed values (a fixed effect).</p>
<div id="assessing-correlation-in-r" class="section level3">
<h3>Assessing correlation in R</h3>
<p>In R you can use the <code>cor</code> function (part of base R) to
assess correlation. To run a correlation test (assess if the correlation
is ‘significant’) you can use the ‘cor.test’ function. By default, the
<code>cor.test()</code> function runs a Pearson correlation test, which
is a parametric test that assumes both variables are normal, no
outliers, etc. If you want you can run a non-parametric version- the
Spearman’s correlation (or Kendall’s tau).</p>
<pre class="r"><code># assess correlation -----------------------

## load the &#39;mtcars&#39; data set

data(mtcars)

## define which variables to assess correlation for.

myvars &lt;- c(&quot;disp&quot;,&quot;hp&quot;,&quot;wt&quot;)

## grab only the variables of interest

mtcars.reduced &lt;- mtcars[,myvars]

## compute (pearson) correlations for all pairs of variables (correlation matrix)

cor.mat &lt;- cor(mtcars.reduced)

cor.mat</code></pre>
<pre><code>##           disp        hp        wt
## disp 1.0000000 0.7909486 0.8879799
## hp   0.7909486 1.0000000 0.6587479
## wt   0.8879799 0.6587479 1.0000000</code></pre>
<pre class="r"><code>## visualize correlations with the &#39;pairs&#39; function

pairs(mtcars.reduced)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>## run a correlation test- with 95% confidence interval
    # default is Pearson product-moment correlation.

cor.test(mtcars$disp,mtcars$wt)</code></pre>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  mtcars$disp and mtcars$wt
## t = 10.576, df = 30, p-value = 1.222e-11
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.7811586 0.9442902
## sample estimates:
##       cor 
## 0.8879799</code></pre>
<pre class="r"><code>## now try a non-parametric version

cor.test(mtcars$disp,mtcars$wt, method = &quot;kendall&quot;) # or spearman (kendall generally preferred)</code></pre>
<pre><code>## Warning in cor.test.default(mtcars$disp, mtcars$wt, method = &quot;kendall&quot;): Cannot
## compute exact p-value with ties</code></pre>
<pre><code>## 
##  Kendall&#39;s rank correlation tau
## 
## data:  mtcars$disp and mtcars$wt
## z = 5.9278, p-value = 3.07e-09
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##       tau 
## 0.7433824</code></pre>
</div>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2>Multiple linear regression</h2>
<p>Multiple, or multi-variable, linear regression simply refers to the
case where you have more than one predictor variable. NOTE: avoid using
the term ‘multivariate’ because that often refers to the case where you
have multiple <strong>response</strong> variables.</p>
<p>If we were running a multiple linear regression with the variables
“disp”,“hp”,“wt” from the <code>mtcars</code> dataset as predictor
variables (see above), we might be somewhat concerned about
multicollinearity. What is that, you ask?</p>
<div id="multicollinearity-for-multiple-linear-regression"
class="section level3">
<h3>Multicollinearity (for multiple linear regression)</h3>
<p>If you have multiple predictor variables, you should first check that
your predictor variables are not too correlated with one another. If the
correlation between all pairs of quantitative covariates is less than
around 0.7 or 0.8 you should generally not have a big problem with
multicollinearity (although you might still check your variance
inflation factors!). If you have a major problem with multicollinearity,
you should generally use only one variable out of every pair of highly
correlated variables for your final model.</p>
<p>You might also consider “penalized regression” approaches like ‘ridge
regression’ or ‘lasso regression’. This, however, is outside the scope
of this class!</p>
<div id="variance-inflation-factors" class="section level4">
<h4>Variance Inflation Factors</h4>
<p>Variance Inflation Factors (VIFs) are a diagnostic tool to help you
evaluate if you have a multicollinearity problem. In general VIFs less
than 4 are considered okay. On the other hand, VIFs greater than 5 or 10
indicate problems!</p>
<p>To compute variance inflation factors, you can use the ‘vif’ function
in the ‘car’ package. Let’s try this using the trees dataset!</p>
<pre class="r"><code># variance inflation factors ---------------------

###
# load the car package

library(car)

###
# load the trees dataset

data(trees)

###
# visualize relationships among predictor vars

pairs(trees[,c(&quot;Girth&quot;,&quot;Height&quot;)])</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>###
# check for correlation in predictor variables

cor(trees[,c(&quot;Girth&quot;,&quot;Height&quot;)])   # predictor variables not highly correlated</code></pre>
<pre><code>##            Girth    Height
## Girth  1.0000000 0.5192801
## Height 0.5192801 1.0000000</code></pre>
<pre class="r"><code># run a multiple regression model

my.mod &lt;- lm(Volume~Girth+Height,data=trees)


# check variance inflation factors

car::vif(my.mod)</code></pre>
<pre><code>##   Girth  Height 
## 1.36921 1.36921</code></pre>
<p>Here it looks like we don’t have to worry!</p>
<p>What about if we use the ‘mtcars’ dataset? It seemed there were
stronger correlations among predictor variables in that case!</p>
<pre class="r"><code>###
## mtcars multicollinearity example

mymod &lt;- lm(mpg~disp+hp+wt,data=mtcars) 

vif(mymod)</code></pre>
<pre><code>##     disp       hp       wt 
## 7.324517 2.736633 4.844618</code></pre>
<p>Now we are in the danger zone, and we might consider reducing our set
of variables. First we might find our most highly correlated pair of
variables and eliminate one of these!</p>
<pre class="r"><code>cor(mtcars.reduced)</code></pre>
<pre><code>##           disp        hp        wt
## disp 1.0000000 0.7909486 0.8879799
## hp   0.7909486 1.0000000 0.6587479
## wt   0.8879799 0.6587479 1.0000000</code></pre>
<pre class="r"><code>#  alternative: use &quot;findCorrelation&quot; from caret package

## let&#39;s remove the &#39;disp&#39; variable and keep weight...

mymod &lt;- lm(mpg~hp+wt,data=mtcars) 

vif(mymod)</code></pre>
<pre><code>##       hp       wt 
## 1.766625 1.766625</code></pre>
<p>Looks much better now!!!</p>
</div>
</div>
</div>
<div id="what-if-assumptions-are-violated" class="section level2">
<h2>What if assumptions are violated?</h2>
<p>Let’s look at what to do if some of the basic assumptions of linear
regression are violated?</p>
<div id="non-linearity" class="section level3">
<h3>Non-linearity</h3>
<p>If we suspect a non-linear functional relationship between our
response and predictor variables, we might perform a <strong>non-linear
regression</strong> analysis.</p>
<p>We can perform a non-linear regression using the <code>nls()</code>
function in R (for non-linear least squares). Let’s use the “DNase”
dataset in R to explore this.</p>
<pre class="r"><code># ?nls

data(DNase)

plot(DNase$density~DNase$conc)   # looks non-linear!</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre class="r"><code>### run linear regression model

model1 &lt;- lm(density~conc,data=DNase)

### run diagnostic plots

par(mfrow=c(2,2))
plot(model1)     # clear non-linearity (and non-normal residuals, and heteroskedasticity!)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-17-2.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(1,1))      # plot data with regression line - obvious issues!
plot(DNase$density~DNase$conc)
abline(model1)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-17-3.png" width="672" /></p>
<pre class="r"><code>### run non-linear regression model - use saturation curve

model2 &lt;- nls(density ~ (max*conc)/(K+conc),data=DNase,start=list(max=2,K=1))
summary(model2)</code></pre>
<pre><code>## 
## Formula: density ~ (max * conc)/(K + conc)
## 
## Parameters:
##     Estimate Std. Error t value Pr(&gt;|t|)    
## max  2.28032    0.02189  104.16   &lt;2e-16 ***
## K    3.68241    0.08677   42.44   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.04909 on 174 degrees of freedom
## 
## Number of iterations to convergence: 6 
## Achieved convergence tolerance: 2.373e-06</code></pre>
<pre class="r"><code>### run non-linear regression model - use logistic function 

model3 &lt;- nls(density ~ Asym/(1 + exp((xmid - log(conc))/scal)),
                 data = DNase,
                 start = list(Asym = 3, xmid = 0, scal = 1))

summary(model3)</code></pre>
<pre><code>## 
## Formula: density ~ Asym/(1 + exp((xmid - log(conc))/scal))
## 
## Parameters:
##      Estimate Std. Error t value Pr(&gt;|t|)    
## Asym  2.48532    0.06287   39.53   &lt;2e-16 ***
## xmid  1.51812    0.06397   23.73   &lt;2e-16 ***
## scal  1.09831    0.02442   44.98   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.04687 on 173 degrees of freedom
## 
## Number of iterations to convergence: 6 
## Achieved convergence tolerance: 4.739e-07</code></pre>
<pre class="r"><code>plot(DNase$density~DNase$conc)
abline(model1)
concs.toplot &lt;- seq(0.01,14,0.1)
densities.toplot &lt;- predict(model2,newdata=data.frame(conc=concs.toplot))
lines(concs.toplot,densities.toplot,col=&quot;blue&quot;)
densities.toplot &lt;- predict(model3,newdata=data.frame(conc=concs.toplot))
lines(concs.toplot,densities.toplot,col=&quot;red&quot;)

legend(&quot;topleft&quot;,lty=c(1,1,1),col=c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;),legend=c(&quot;linear&quot;,
                  &quot;saturation&quot;,&quot;logistic&quot;))</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-17-4.png" width="672" /></p>
<p>You can run residual plots with non-linear regression just like you
can with ordinary linear regression:</p>
<pre class="r"><code>## residual plots

resids &lt;- DNase$density-predict(model3)

residuals(model3)   # alternative method! This works for &#39;lm&#39; objects as well (and many other model objects)</code></pre>
<pre><code>##   [1] -0.0222835640 -0.0212835640 -0.0124539463 -0.0094539463 -0.0335367172
##   [6] -0.0245367172 -0.0380629138 -0.0410629138 -0.0662506055 -0.0712506055
##  [11] -0.0115300720 -0.0295300720 -0.0853516109 -0.0553516109 -0.0458016613
##  [16] -0.0658016613  0.0057164360  0.0107164360  0.0035460537 -0.0104539463
##  [21] -0.0145367172 -0.0325367172 -0.0140629138 -0.0320629138 -0.0082506055
##  [26]  0.0007493945  0.0854699280  0.0474699280  0.1346483891  0.1066483891
##  [31]  0.1561983387  0.1381983387  0.0307164360  0.0287164360  0.0395460537
##  [36]  0.0315460537  0.0374632828  0.0084632828  0.0189370862  0.0109370862
##  [41]  0.0227493945  0.0087493945  0.0364699280  0.0464699280  0.2096483891
##  [46]  0.0596483891  0.2271983387  0.1081983387 -0.0282835640 -0.0232835640
##  [51] -0.0154539463 -0.0254539463 -0.0395367172 -0.0335367172 -0.0510629138
##  [56] -0.0550629138 -0.0602506055 -0.0402506055 -0.0515300720 -0.0575300720
##  [61]  0.0046483891 -0.0203516109 -0.0358016613 -0.0438016613 -0.0042835640
##  [66] -0.0042835640 -0.0014539463  0.0015460537 -0.0155367172 -0.0195367172
##  [71] -0.0300629138 -0.0250629138 -0.0222506055 -0.0332506055  0.0294699280
##  [76]  0.0004699280  0.0056483891 -0.0103516109 -0.0258016613 -0.0378016613
##  [81]  0.0467164360  0.0637164360  0.0575460537  0.0555460537  0.0324632828
##  [86]  0.0374632828  0.0249370862  0.0109370862  0.0057493945 -0.0042506055
##  [91]  0.0314699280  0.0414699280  0.0046483891  0.0396483891 -0.0078016613
##  [96]  0.0301983387  0.0547164360  0.0527164360  0.0485460537  0.0485460537
## [101]  0.0424632828  0.0334632828  0.0289370862  0.0239370862  0.0057493945
## [106] -0.0122506055  0.0214699280  0.0044699280 -0.0103516109 -0.0273516109
## [111] -0.0168016613 -0.0368016613  0.0147164360  0.0147164360  0.0185460537
## [116]  0.0145460537 -0.0135367172 -0.0175367172 -0.0230629138 -0.0320629138
## [121] -0.0222506055 -0.0362506055  0.0124699280 -0.0285300720  0.0466483891
## [126] -0.0383516109 -0.0328016613 -0.0518016613 -0.0072835640  0.0037164360
## [131]  0.0085460537  0.0215460537 -0.0005367172  0.0024632828  0.0049370862
## [136] -0.0200629138 -0.0562506055  0.0247493945  0.0154699280 -0.0045300720
## [141] -0.0213516109 -0.0143516109 -0.0828016613 -0.0468016613  0.0127164360
## [146]  0.0547164360  0.0305460537  0.0325460537  0.0194632828  0.0164632828
## [151]  0.0239370862  0.0239370862  0.0097493945  0.0207493945  0.0114699280
## [156]  0.0444699280 -0.0793516109 -0.0133516109 -0.0768016613 -0.0678016613
## [161]  0.0077164360  0.0177164360  0.0255460537  0.0215460537  0.0064632828
## [166]  0.0124632828  0.0119370862 -0.0040629138  0.0237493945  0.0037493945
## [171] -0.0365300720 -0.0505300720  0.0016483891 -0.0343516109 -0.0608016613
## [176] -0.0548016613
## attr(,&quot;label&quot;)
## [1] &quot;Residuals&quot;</code></pre>
<pre class="r"><code>## check for normality

qqnorm(resids)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>shapiro.test(resids)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  resids
## W = 0.90166, p-value = 1.978e-09</code></pre>
<pre class="r"><code>## check for heteroskedasticity
plot(resids~predict(model3))</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-18-2.png" width="672" /></p>
</div>
<div id="heteroskedasticity" class="section level3">
<h3>Heteroskedasticity</h3>
<p>If you have severely heteroskedastic residuals, you can sometimes run
a <strong>transformation</strong> of your response variable that
improves the situation.</p>
<p>Alternatively, you might run a <strong>generalized linear model
(GLM)</strong> that doesn’t assume homoskedasticity (e.g., run a model
that assumes the residuals are gamma distributed, or Poisson
distributed). In a gamma or Poisson distribution, the residuals are
expected to get larger as the expected mean value gets larger!</p>
<p>We will talk more about GLM soon. But for now, let’s run an example
with a transformed response variable:</p>
<pre class="r"><code>###
# heteroskedasticity

### first, simulate data with heteroskedastic residuals

simulated.x &lt;- runif(100,0.1,5)
simulated.y &lt;- exp(rnorm(100,1.1+0.3*simulated.x,0.7))

plot(simulated.y~simulated.x)</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>### run linear model

model1 &lt;- lm(simulated.y~simulated.x)
par(mfrow=c(2,2))
plot(model1)  # run diagnostic plots    heteroskedasticity issues</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-19-2.png" width="672" /></p>
<pre class="r"><code>### Take a minute to test for normality of residuals!

### run linear model with log transformation of response variable

model2 &lt;- lm(log(simulated.y)~simulated.x)
par(mfrow=c(2,2))
plot(model2)  # run diagnostic plots - no issues!</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-19-3.png" width="672" /></p>
<pre class="r"><code># can also run &quot;bptest&quot; in &quot;lmtest&quot; package to test for violation of homoskedasticity</code></pre>
</div>
<div id="non-independence-of-observations" class="section level3">
<h3>Non-independence of observations</h3>
<p>If you have a known source of non-independence in your observations,
you can try to thin (rarefy) your data by throwing away non-independent
observations (e.g., keeping only one out of every set of known
non-independent observerations).</p>
<p>There are also more advanced regression models that explicitly
account for sources of non-independence:</p>
<p><em>Regression with phylogenetic correction</em> – use this if
phylogenetic relatedness among your sampling units is the main source of
non-independence</p>
<p><em>Autoregression (more generally, ARIMA-type time series
modeling)</em> – use this if the main source of non-independence is
temporal- observations taken closer together in time are more likely to
be related</p>
<p><em>Spatial autoregression</em> (more generally, spatial regression
models) – use this if the main source of non-independence is spatial –
observations taken closer together in space are more correlated.</p>
<p>These models are outside the scope of this class, but it’s useful to
know they exist!</p>
</div>
<div id="non-normality-of-residuals" class="section level3">
<h3>Non-normality of residuals</h3>
<p>First of all, regression analysis is somewhat robust to non-normality
of the residuals- due to the CLT!</p>
<p>But if your residuals are highly non-normal AND you can’t seem to
correct this using non-linear regression, transformations, or GLM, you
might need to use a non-parametric alternative.</p>
<p>One such alternative is to run a non-parametric correlation analysis
such as a Spearman rank correlation test.</p>
</div>
</div>
<div id="regression-and-anova" class="section level2">
<h2>Regression and ANOVA</h2>
<p>Before we move on to ANOVA, I just wanted to point out the essential
similarity between regression and ANOVA. The only difference between
regression and ANOVA is that in regression, the response variable is
numeric/continuous and in ANOVA your response variable(s) is
categorical.</p>
<p>How similar are regression and ANOVA? Well, we can fit regression and
ANOVA models with the <code>lm()</code> function, the assumptions are
essentially the same, and the diagnostic plots are essentially the
same.</p>
<p>There are differences- we will touch on this in the next lecture- but
for now let’s focus on the similarities. Let’s run an ANOVA in R using
the same approaches we have just learned for linear regression!</p>
<p>In this example we will use the famous iris dataset.</p>
<pre class="r"><code>## ANOVA as regression example

data(iris)    # load the iris dataset

plot(iris$Sepal.Length ~ iris$Species)   # r uses a boxplot by default for categorical predictor</code></pre>
<p><img src="LECTURE6_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>my.mod &lt;- lm(Sepal.Length~Species,data=iris)    # run an ANOVA!
summary(my.mod)    # look at the results</code></pre>
<pre><code>## 
## Call:
## lm(formula = Sepal.Length ~ Species, data = iris)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.6880 -0.3285 -0.0060  0.3120  1.3120 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         5.0060     0.0728  68.762  &lt; 2e-16 ***
## Speciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***
## Speciesvirginica    1.5820     0.1030  15.366  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5148 on 147 degrees of freedom
## Multiple R-squared:  0.6187, Adjusted R-squared:  0.6135 
## F-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>anova(my.mod)     # produce an analysis of variance table</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Sepal.Length
##            Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## Species     2 63.212  31.606  119.26 &lt; 2.2e-16 ***
## Residuals 147 38.956   0.265                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>###
# alternative!

my.mod &lt;- aov(Sepal.Length~Species,data=iris)   # same model!!
summary(my.mod)     # but produces an anova table by default</code></pre>
<pre><code>##              Df Sum Sq Mean Sq F value Pr(&gt;F)    
## Species       2  63.21  31.606   119.3 &lt;2e-16 ***
## Residuals   147  38.96   0.265                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p><a href="LECTURE7.html">–go to next lecture–</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
