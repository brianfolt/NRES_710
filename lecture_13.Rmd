---
title: "Multi-variable modeling - collinearity"
author: "NRES 710"
date: "Last compiled: `r Sys.Date()`"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
    css: styles.css
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
	)
```

```{r echo=FALSE}

#  NRES 710, Multi-variable Modeling - Collinearity
#     University of Nevada, Reno

```

## Collinearity

### Introduction

Today's class is an important one. The topic is on something that very few people understand well. So the plan is to spend 1-2 days on this topic so that you will understand it well. Many people have a decent idea of what collinearity is -- but tend to make bad decisions about how to handle it.

Let's start with a question for the class. We often say that "*correlation does not equal causation*". More specifically, you might run a regression between X and Y and find a relationship, but that does not mean that X causes Y.

**Q:** Why not?

Let's consider an example. Let's say there was a study that came out -- and generated some interest from the press -- that suggested that for each 1 additional hour of TV that you watch per day, it increases the risk of mortality by 11%.

Let's think about this for a minute. Do you think the actual act of watching TV increases the risk of mortality? How would that happen? Is the TV going to fall out of the wall and potentially injure you? No, probably not. Watching TV itself is not going to cause this relationship.

What's the problem here? Well, there are a lot of other things that are **correlated** to watching TV that have their own effect on the Y variable (mortality).

**Confounding factors -- an X-variable that is correlated with the X-variable of interest that has its own effect on Y**

What is correlated with watching TV?

- You aren't being active (e.g., exercising)!
- You might be snacking, and your snacks might not be something healthy (you probably aren't eating carrots if you are watching TV).
- You aren't working, which might relate to your socioeconomic status.
- You may be less likely to have healthcare.
- Etc.

This things might all be correlated to how much an individual watches TV and might all have their own effects on mortality. 

**Q:** Here's the key takehome. If we really want to know what the true effect of TV watching on mortality is, what would would we do? A MANIPULATIVE EXPERIMENT. In an ideal world, we would do a manipulative experiment, which is the only true way to show causation.

- Option 1: We would try to control for all of the above factors: everyone has the same job, goes to the gym the same amount of time, no smoking, etc. Let's assume this class is our sample! We would assign half of the class to watch TV for 1 hour a day, while the other half of the class watches for 10 hours per day.
- Option 2: We wouldn't try to control for any of those factors, but we would randomly assign for 1 vs. 10 hours of TV per day. This is a lower-quality experiment, because all of those other factors may *swamp out* our ability to detect an effect of watching TV. We are not controlling for anything, so all of these other factors may create *noise*.
  - But, we could collect data on those factors, and include those data in the models, and then we could avoid the swamping issue.
  - Of course, this experimental design is super impractical. There's no way we could get our sample to watch TV for 10 hours a day -- or even 1 hour per day. 
  
So what do we do? We do an **observation study** and then collect data on all of these other features that might also influence the response variable. Diet, exercise, socioeconomic status, smoking, alcohol, and maybe a whole slew of other things that we might hypothesize also influence the response variable (mortality). We could then put all those variables into the model and *statistically* control for those other variables. When we include correlated variables in a model, it statistically controls for them -- and we'll demonstrate this today.

We may not be able to show causation, because there may be some other confounding variable(s) in the world that we did not measure and collect data.

**Important takehome:** When you are conducting studies and experiments, you need to *think carefully* about what are the potential confounding factors -- the other variables -- that are correlated to the X-variable you are interested in and that might influence your results.

- This is less of a concern in a well-designed manipulative study, where random assignment of treatment removes collinearity. For example, if I randomly assign to the class who watches 1 vs. 10 hours of TV, then there won't be any collinearity between exercise and TV watching. Presumably people that go the gym often will be randomly assigned to 10 hours of TV, and people who never go to the gym will be made to watch 1 hour of TV. This random assignment removes potential collinearity between these variables.

If you have confounding variables and you don't include them in your model/analysis, you get biased (incorrect) estimates.

**If confounding variables are not included in model/analysis --> biased (incorrect) estimates ($\beta$s)**

People kind of understand this... but I want you to really understand it. If there are variables that are correlated to your X-variable of interest and you don't collect those data and include them, then your estimates are biased. This is scary! That's why this is one of the more important topics in this class.

Let's see how this works.

## Biased estimates without confounding variables

Let's reconsider an example that we examined in the last class.

- **Y = size**
- **$X_1$ = age**
- **$X_2$ = sex**

However, this time around when you were sampling the animals of interest, for some reason you had a hard time collecting data on young females.

```{r example_1, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=4.5}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0,0))

# Load in the data
datum <- read.csv("lecture_12_dataset1.csv")

# Get rid of old animals for this example
datum <- subset(datum, Age < 7)

# Subset to just older females
females <- subset(datum, Sex == "Female" & Age > 4)

# Plot the data
plot(Size ~ Age, data = females,
     xlim=c(0,max(datum$Age)), ylim=c(8, max(datum$Size)), pch = 1)
```

And you also had a really hard time catching old males...

```{r example_2, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=4.5}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0,0))

# Subset to just older females
males <- subset(datum, Sex == "Male" & Age < 4)
males$Size <- males$Size + 4

# Plot the data
plot(Size ~ Age, data = females,
     xlim=c(0,max(datum$Age)), ylim=c(8, max(datum$Size)), pch = 1)
points(males$Age, males$Size, pch=2)
```

Let's say I run this model:

**$Y = \beta_0 + \beta_1 Age$**

Sex is not included in this model... but Sex is correlated with age! Our sample only includes old females and young males. (A T-test comparing age between females and males would be different.)

**Q:** If I ran a regression, where would the line go? 

```{r example_3, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=4.5}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0,0))

# Plot the data
plot(Size ~ Age, data = females,
     xlim=c(0,max(datum$Age)), ylim=c(8, max(datum$Size)), pch = 1)
points(males$Age, males$Size, pch=2)

# Bind data and run simple lm()
datum <- rbind(females, males)
results <- lm(Size ~ Age, data = datum)
abline(results)
```

For these data, it would be ~horizontal across the data. This is a biased estimate. This estimate incorrectly estimates the effect of age on size of these animals.

However, if we run this model:

**$Y = \beta_0 + \beta_1 Age + \beta_2 Sex$**

We get two lines! And these lines will look like:

```{r example_4, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=4.5}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0,0))

# Plot the data
plot(Size ~ Age, data = females,
     xlim=c(0,max(datum$Age)), ylim=c(8, max(datum$Size)), pch = 1)
points(males$Age, males$Size, pch=2)

# Bind data and run simple lm()
datum <- rbind(females, males)
results <- lm(Size ~ Age, data = datum)

# lm() for males
results2 <- lm(Size ~ Age, data = males)
abline(results2, lty=2)

# lm() for females
results3 <- lm(Size ~ Age, data = females)
abline(results3, lty=2)
```

Now, we get *unbiased* estimates of the effect of Age on Size for each Sex.

If we leave out Sex in the first model, some of that Sex effect is being transmitted to the effect of Age.

- Females are smaller than males, and it's biasing that effect on Size.
- The difference in size is transmitted to the effect of Age.

The same thing might be happening in the TV-effect on mortality study. There were many other variables that were correlated to watching TV that were really influencing mortality. Their effects, which were left out of the statistical model, were being transmitted to the effect of watching TV.

**Q:** Does that make sense? Questions?

Reiterating the important point: if we leave confounding variables out of our model, we get biased estimates of the variables that we do leave in the model.

- So, we want to collect data on confounding variables and include them in the model.
- And we want to do this even if they are not statistically significant. We'll talk about model building later in the class and when you should/should not remove variables from models.

In general though, we want to leave variables in to statistically control for confounding effects.

## Tradeoff

**If confounding variables *are* included in model --> Variance Inflation**

When we run a model with multiple X-variables, the computer package is trying to figure out which of the two variables are having an effect on Y. But they are correlated to eachother, which makes it tough for the software to figure out out which X-variable is having what effect. And the more strongly they are associated with eachother, it becomes more difficult for the software to disentangle these effects.

A consequence of this is that more variables cause greater uncertainty in the effects ($\beta$s). Increased uncertainty is reflected in:

- **increased SE**
- **increased CI**
- **increased P-values**

This can lead to problematic results.

I mentioned during the last class that, generally, if we add a variable to a model, the p-values for the variables in a model go down. <u>However, collinearity is the one exception to that rule!</u> And in fact, it's a red flag.

My recommendation is that, before you even start your analysis, you need to know what the collinearity is between your X-variables! Plot your X-variables, run some linear models (e.g., between Age and Sex), and learn about what the collinearity is.

- Some people fail to do this and problems occur. For example, they might build a model where Age was significant... and then they add Sex, and Age is no longer significant...
- If the P-value goes up a lot when you add a variable to a model, this is a symptom of collinearity! This is happening because of collinearity between X-variables.

## Symptoms of collinearity

1) Collinearity between independent variables
  - High $r^2$ values
  - Statistically-significant relationships between X-variables
2) High variance inflation factors (VIF) of variables in model
3) Variables significant in simple regression, but not in multi-variable regression
4) Individual variables not significant in multi-variable regression model, but the overall multi-variable regression model is significant
5) Large changes in coefficient estimates between full and reduced models
6) Large SE in multi-variable regresion models, despite high power

Breaking this list down a bit...

1) The easiest way to know if you have collinearity is to examine fit a linear model between your two independent variables and see if they have **high $r^2$ values** or if there is a **statistically-significant relationship** between them.

2) High variance inflation factors (VIF) of variables in model. Next class I will show you how to examine VIF for variables in a model.

3) As I said before, if you have significant variables in a simple linear regression, but then you add another variable(s) to create a multi-variable regression and the individual variable(s) is no longer significant, then you have collinearity. 

4) If you build a multi-variable model where (i) none of the individuals variables are significant, but (ii) the overall whole model is significant (the p-value at the bottom of the model0), then you have collinearity. The whole model is significant, but none of the individual variables is significant.

- How does that happen? All of the X-variables are explaining a lot of the Y-variable, but the software is having a hard-time figure out which X-variable is explaining the response. And the collinearity causes the p-values for individual variables to go up.

5) Generally, when you add variables to a model, the betas *should not change too much*. What do I mean by 'too much'? E.g., when you have a simple model with beta and CI, and then you add another X-variable to the model to create a multi-variable model, then the new estimate for your beta should be within the 95% CI of the original estimate. If it moves to be outside of the 95% CI of the original estimate, that's a considerable change and is a symptom of collinearity of the X-variables.

6) What if you have 25,000 samples, and you have huge standard errors. Why are the SE so big, when we have such a large sample size? Collinearity in the system can increase SE, increase CI, and decrease p-values.

**Questions?**

## Simulation exercise

When confronted with collinearity, a common approach taken by many folks in our field is to <u>take variables out of the model</u>. However, this has a consequence that they do not understand -- it biases our estimates! -- so this is not the approach I recommend.

I'm going to show you a simulation exercise that demonstrates:

1) Collinearity influences parameter **estimates**, **uncertainty**, and **p-values** when confounding variables are not included in the model, and
2) When all the necessary confounding variables are included, collinearity increases uncertainty and p-values but <u>does not bias estimates</u>.

### Truth

$\beta_1 = 3$
$\beta_2 = 3$
$z = U[0.5,20]$ -- an estimate of how much correlation (collinearity) there is between $X_1$ and $X_2$

For the simulation exercise, I simulated 1,000 datasets with varying degrees of collinearity (correlation) between two X-variables. Here is truth:

- Simulations: $n = 1,000$
- $y = 10 + 3X_1 + 3X_2 + \epsilon \sim N(0, 2)$
- $X_1 = U[0,10]$
- $X_2 = X_1 + N(0, z)$
- For each simulation, I used a different value of *z* from a uniform distribution: $z = U[0.5, 20]$.

```{r simulation_1, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=4.5}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0,0))

# Number of simulations
s <- 1000

# Empty vectors to save results from each simulation
simple_beta1 <- numeric(s)
simple_se1 <- numeric(s)
simple_p1 <- numeric(s)
multi_beta1 <- numeric(s)
multi_se1 <- numeric(s)
multi_p1 <- numeric(s)
r2 <- numeric(s)
vif <- numeric(s)

# x2 = x1 + error(0, z), where
# z can range from 0.5 (highly correlated to x1) to 20 (not correlated at ~all)
z_values <- seq(0.5, 20, length.out = s)

# Loop through each simulation replicate to measure everything
for (i in 1:s){
  
  # Number of datapoints per simulation
  n <- 100

  # x1
  x1 <- runif(n, 0, 10) # Random, uniform variable; only simulated once

  # error for y; only simulated once
  error <- rnorm(n, 0, 2)
  
  # X2 for each simulation
  x2 <- x1 + rnorm(n, 0, z_values[i])
  
  # Y for each
  y <- 10 + 3 * x1 + 3 * x2 + error

  # Simple model
  results1 <- lm(y ~ x1)
  simple_beta1[i] <- summary(results1)$coefficients[2,1]
  simple_se1[i] <- summary(results1)$coefficients[2,2]
  simple_p1[i] <- summary(results1)$coefficients[2,4]

  # Multi-variable model
  results2 <- lm(y ~ x1 + x2)
  multi_beta1[i] <- summary(results2)$coefficients[2,1]
  multi_se1[i] <- summary(results2)$coefficients[2,2]
  multi_p1[i] <- summary(results2)$coefficients[2,4]

  # R^2 between X-variables
  model_x12 <- lm(x2 ~ x1)
  r2[i] <- summary(model_x12)$r.squared
  
  # VIF for multi-variable model
  vif[i] <- car::vif(results2)["x1"]
}
```

Low values of *z* (e.g., *z* = 0.5) caused high collinearity between the X-variables, because only a little bit of noise gets added to $X_1$ to calculate $X_2$. Alternatively, high values of *z* (e.g., *z* = 20) had low collinearity, because large noise was added to $X_1$ to calculate $X_2$. In other words, the low values of *z* caused very little noise (and high collinearity) between $X_1$ and $X_2$, while the high values caused great noise (and low collinearity) between the variables.

For each simulation, I did a few things:

- Fit a simple model ($Y ~ X_1$) and measured the estimate, SE, and p-value for $\beta1$
- Fit a multi-variable model ($Y ~ X_1 + X_2$) that included both of the collinear, confounding variables, and measured the effect, SE, and p-value for $\beta1$.

Most importantly, I measured how collinearity between $X_1$ and $X_2$ (i.e., $r^2$) influenced the the Variance Inflation Factor from the multi-variable model.

I'm going to show a number of graphs, and for all of these graphs the X-axis is a measure of collinearity (correlation; $r^2$) between $X_1$ and $X_2$. If it's collinearity equals 1, then the variables are perfectly correlated, whereas low values show weak/no correlation.

The first graph I am going to show you describes how the **Variance Inflation Factor** score is influenced by collinearity. The **Variance Inflation Factor** is a metric used to diagnose whether there may be collinearity between X-variables in a multiple-variable model.

**Variance Inflation Factor (VIF) -- the amount (in *times*) that the variance ($SE^2$) in the $\beta$ increases due to collinearity**

- E.g., if VIF = 4, then the variance is 4 times what it would be if you didn't have collinearity -- and the SE would be doubled ($SE = \sqrt4 = 2$).

First, let's see how the VIF is influenced by collinearity:

```{r simulation_2, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plotting VIF against collinearity values
plot(r2, vif, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = "VIF")

# Add horizontal line for when VIF = 2
abline(h = 2, lty = 2)
```

This graph shows how VIF scores increase with increasing collinearity. 

Some people say that if the VIF > 10, you need to worry about collinearity. In this case, the SE would be triple what it would otherwise be without collinearity. This happens when $r^2 = 0.9$, and the two X-variables are almost perfectly correlated. This is too high, in my opinion.

Rule-of-thumb: **VIF > 2, recognize that you have collinearity in your data.**

VIF = 2 ~ $r^2$ = 0.6 -- so another rule of thumb is that an $r^2 > 0.6$ may also indicate collinearity.

### Simple regression model: $Y \sim X_1$

Now, let's consider results from the simple model ($Y \sim X_1$) across the simulations.

The most important thing I want to emphasize to you all is in this first graph. How is the **beta for $X_1$** influenced by collinearity in the simple model?

```{r simulation_3, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the simple model
plot(r2, simple_beta1, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("Estimate of " ~ beta[1]))
```

This graph shows, again, the $r^2$ between $X_1$ and $X_2$ (how correlated they are) on the X-axis, and the Y-axis shows the estimate for the beta for $X_1$. If there is no collinearity, the estimate is 3 -- which is truth! Bottom-left-hand side of the graph.

However, even with a very little bit of collinearity ($r^2 < 0.2$), the parameter estimate is biased! As collinearity increases, the parameter estimate jumps pretty quickly up to 6!

**Q:** Why 6? 6 = 3 + 3. Because the two X-variables are correlated, the estimation model starts to assign the effect of $X_2$ onto $X_1$. 

Not including the confounding variable introduces bias in our estimates and negatively influences our inference.

Now, often in this class we ask: how do different approaches influence betas, uncertainty, and p-values. Next we might ask ourselves: how is **standard error (SE)** influenced by collinearity in the simple model?

```{r simulation_4, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot SE estimate by collinearity values from the simple model
plot(r2, simple_se1,
     xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]),
    ylab = expression("SE of estimate of" ~ X[1]))
```

This is where it gets a little scary. 

Not only is the parameter estimate biased, the standard error gets smaller and smaller. As collinearity increases, your estimates are getting more and more precise...

How are **p-values** influenced by collinearity in the simple model?

```{r simulation_5, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4.5, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot p-values by collinearity values from the simple model
plot(r2, log(simple_p1),
     xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]),
     ylab = expression("P-values of estimate of" ~ X[1] ~ " (log-transformed)"))
```

As uncertainty gets smaller and samller, our p-values also get smaller and smaller, because the effect of collinearity gets bigger and bigger...

So, collinearity is **really dangerous**! If you have collinearity in your data, and you leave out some confounding variable, you are going to get biased estimates, and it's going to seem like it's a really precise estimate with a clear result. Unless...

**Q:** What if the effect of $X_2$ was -3? The two effects might cancel eachother out, and our estimate would be zero, with a small SE, and *false confidence that there is no effect*.

If the effect of the unaccounted confounding variable, $X_2$, has an effect in the opposite direction, the collinear effects may partially or wholly cancel eachother out, and you may fail to observe any true effect of X on Y.

### Multi-variable model: $Y \sim X_1 + X_2$

Now, let's consider results from the multi-variable model ($Y \sim X_1 + X_2$) across the simulations. This is the model where we have *statistically controlled* for that collinearity. 

Let's start here by asking: how does collinearity influence the **beta for $X_1$** in the multi-variable model?

```{r simulation_6, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the multi-variable model
plot(r2, multi_beta1, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("Estimate of " ~ beta[1]))

# Add a horizontal line for when r^2 = 0.6
abline(v = 0.6, lty=2)
```

On the Y-axis, we have the coefficient estimate for $\beta_1$, and the X-axis is again the correlation between the two X-variables.

The estimate is good, right around 3 (truth)!! But as collinearity increases, the estimate can be more variable. So by including the confounding variables, your estimate is good, but the bad news is that due to the collinearity, you have less certainty in your estimate. But at least the estimate is unbiased, which is the most important thing.

Remember the rule of thumb about $r^2$ indicating collinearity? Previously I said that when $r^2 = 0.6$, collinearity should be on your radar. In this graph, we can see that simulations with collinearity greater than 0.6 start to have parameter estimates that are more variable. However, the mean is still centered on ~3 (Truth) after this rule-of-thumb. So no matter how much collinearity there is, on average we will continue to get an unbiased estimate of the effect of X-variables in the model. In statistics, any one estimate may be bad -- but on average, for this multi-variable model, the estimates will be unbiased.

Another thing that we can see from this graph is the **variance inflation** itself! As collinearity increases along the X-axis, the cone of simulation results gets bigger, because there is more and more uncertainty in what the parameter estimate really is.

```{r simulation_7, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the multi-variable model
plot(r2, multi_se1, xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("SE of estimate of " ~ beta[1]))
```

And we see this here as well with the SE estimates. As collinearity increases, SE of the estimate increases, again suggesting increasing uncertainty due to collinearity.

```{r simulation_8, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Set the seed for reproducibility & set graphing parameter
set.seed(123); par(mar=c(4,4,0.5,1))

# Plot beta1 estimate by collinearity values from the multi-variable model
plot(r2, log(multi_p1), xlab = expression("Collinearity ("~ r^2 ~") between" ~ X[1] ~ "and" ~ X[2]), ylab = expression("P-value of estimate of " ~ beta[1] ~ "(log-transformed)"))
```

And again, as uncertainty increases, p-values increase as well.

But hopefully the takehome message is clear. Failing to account for confounding variables in a model will bias estimates. Accounting for those confounding variables will produce unbiased estimates, although the uncertainty may increase.

## Practical guide to handling collinearity

1) Be careful to identify potential confounding variables prior to data collection. 

- Think carefully about what these might be, and include them in your data collection method so that you can masure them.
- A helpful way to do this is to build a <u>conceptual model</u> describing hypotheses for how all variables might be related in your system and then use that model to guide what variables you might want to measure.

2) Calculate collinearity and VIF among independent variables -- before you start your analysis.

- Don't forget to do this! If you add a variable and coefficient estimates and variable significance change, this is likely due to collinearity. 

3) Pay attention to how coefficient estimates and variable significance change as variables are removed or added.

### Confounding variables

1) Sample in a manner that eliminates collinearity. Collinearity may be real, or it may be due to sampling artifact. E.g., in the example above, collinearity was introduced due to non-random sampling of the different sexes.

2) Use multi-variable regression. If you have collinear X-variables, it may cause your SE to be large, but <u>your estimates will be unbiased</u>. If you fail to include confounding variables, then estimates will be biased.

3) Include confounding variables, even if they are non-significant.

Scenarios: two collinear variables (Age and Sex), and you run the full model. VIF is really high (e.g., 10). SE and CI are very inflated, but everything is still significant. Do you do anything? No.

- Your estimates are unbiased, because you ran the full model.
- Your CI are large, but still statistically significant.
- You have identified that you have collinearity, which caused the large CI. But the betas are unbiased.

But sometimes you run the analysis but X-variables are not significant. What do you do then?

4) Get more data! This decreases SE and VIF.

Most problems in statistics can be improved by getting more data. Increasing N decreases SE, CI, and P-values... If none of your data are significant due to collinearity, maybe you just need to collect more data.

5) There are statistical techniques that can eliminate collinearity. One of those is Principal Component's Analysis, which we may talk about in this class. Popular analysis. What this analysis does is it creates *new variables* that are orthogonal -- which means they have zero collinearity.

### Redundant variables

If collinearity between X-variables results in biased estimates, why do so many people throw out their variables in favor of reduced models that will suffer from collinearity and thus have biased estimates?

Well, you can do this <u>*sometimes*</u> -- primarily when your collinear X-variables <u>don't have an effect on Y</u>.

These are sometimes called **redundant variables -- collinear X-variables that don't have an effect on the Y-variable**.

- If they don't really have an effect (i.e., $\beta$ = 0), there is no problem when you take them out.
- If you leave out a confounding variable, it assigns it's effect to the other collinear variable you left in. (In this case, the effect is zero.)

In truth, there are no confounding or redundant variables, but rather it's a continuum depending on the $\beta$.

- If the $\beta \neq 0$, it's a confounding variable.
- If the $\beta = 0$, it's a redundant variable. 

You don't need to leave in redundant variables, and leaving them in will generate variance inflation.

What's an example of a redundant variable? Watching TV and it's effect on mortality. If you leave this in your analysis, you will measure a $\beta = 0$ after you include all of the other important things that truly do influence mortality. And by leaving it in the model you will increase SE, CI, and p-values of all of the other important variables.

<br>

## Truth

```{r collinearity_simulation, echo=TRUE, message=FALSE, warning=FALSE}
### Lecture 13: code to simulate collinear data and to demonstrate how failing to account for collinearity influences estimates and uncertainty

# Set the seed for reproducibility
set.seed(123)

# Number of simulations
s <- 1000

# Empty vectors to save results from each simulation
simple_beta1 <- numeric(s)
simple_se1 <- numeric(s)
simple_p1 <- numeric(s)
multi_beta1 <- numeric(s)
multi_se1 <- numeric(s)
multi_p1 <- numeric(s)
r2 <- numeric(s)
vif <- numeric(s)

# x2 = x1 + error(0, z), where
# z can range from 0.5 (highly correlated to x1) to 20 (not correlated at ~all)
z_values <- seq(0.5, 20, length.out = s)

# Loop through each simulation replicate to measure everything
for (i in 1:s){
  
  # Number of datapoints per simulation
  n <- 100

  # x1
  x1 <- runif(n, 0, 10) # Random, uniform variable; only simulated once

  # error for y; only simulated once
  error <- rnorm(n, 0, 2)
  
  # X2 for each simulation
  x2 <- x1 + rnorm(n, 0, z_values[i])
  
  # Y for each
  y <- 10 + 3 * x1 + 3 * x2 + error

  # Simple model
  results1 <- lm(y ~ x1)
  simple_beta1[i] <- summary(results1)$coefficients[2,1]
  simple_se1[i] <- summary(results1)$coefficients[2,2]
  simple_p1[i] <- summary(results1)$coefficients[2,4]

  # Multi-variable model
  results2 <- lm(y ~ x1 + x2)
  multi_beta1[i] <- summary(results2)$coefficients[2,1]
  multi_se1[i] <- summary(results2)$coefficients[2,2]
  multi_p1[i] <- summary(results2)$coefficients[2,4]

  # R^2 between X-variables
  model_x12 <- lm(x2 ~ x1)
  r2[i] <- summary(model_x12)$r.squared
  
  # VIF for multi-variable model
  vif[i] <- car::vif(results2)["x1"]
}
```

## Footnote

TV-effect on mortality study: Wijndaele et al. 2011 *International Journal of Epidemiology*.

[--go to next lecture--](lecture_14.html)
