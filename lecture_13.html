<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 710" />


<title>Multi-variable modeling - collinearity</title>

<script src="site_libs/header-attrs-2.27/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 710</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Syllabus</a>
</li>
<li>
  <a href="schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="lecture_1.html">Syllabus and the Purpose of Statistics</a>
    </li>
    <li>
      <a href="lecture_2.html">P-value Discussion and Intro to R</a>
    </li>
    <li>
      <a href="lecture_3.html">Sampling Uncertainty</a>
    </li>
    <li>
      <a href="lecture_4.html">Linear Regression</a>
    </li>
    <li>
      <a href="lecture_5.html">Linear Regression - results</a>
    </li>
    <li>
      <a href="lecture_6.html">Linear Regression - assumptions</a>
    </li>
    <li>
      <a href="lecture_7.html">Linear Regression - predictions</a>
    </li>
    <li>
      <a href="lecture_8.html">Analysis of Categorical Data - 2 groups</a>
    </li>
    <li>
      <a href="lecture_9.html">Analysis of Categorical Data - &gt;2 groups</a>
    </li>
    <li>
      <a href="lecture_10.html">Analysis of Categorical Data - posthoc tests</a>
    </li>
    <li>
      <a href="lecture_11.html">Analysis with Continuous or Categorical X?</a>
    </li>
    <li>
      <a href="lecture_12.html">Multi-variable Modeling</a>
    </li>
    <li>
      <a href="lecture_13.html">Multi-variable Modeling - collinearity</a>
    </li>
    <li>
      <a href="lecture_14.html">Multi-variable Modeling - interactions</a>
    </li>
    <li>
      <a href="lecture_15.html">Generalized Linear Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="exercise_1.html">Exercise 1 - Data Summary Functions</a>
    </li>
    <li>
      <a href="exercise_2.html">Exercise 2 - Linear Regression</a>
    </li>
    <li>
      <a href="exercise_3.html">Exercise 3 - Linear Regression (cont.)</a>
    </li>
    <li>
      <a href="exercise_4.html">Exercise 4 - Analysis of Categorical Data</a>
    </li>
    <li>
      <a href="exercise_5.html">Exercise 5 - Multi-variable Analysis</a>
    </li>
    <li>
      <a href="exercise_6.html">Exercise 6 - TBD</a>
    </li>
    <li>
      <a href="exercise_7.html">Exercise 7 - TBD</a>
    </li>
    <li>
      <a href="exercise_8.html">Exercise 8 - TBD</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    More Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Links.html">Links</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Multi-variable modeling - collinearity</h1>
<h4 class="author">NRES 710</h4>
<h4 class="date">Last compiled: 2024-08-21</h4>

</div>


<div id="collinearity" class="section level2">
<h2>Collinearity</h2>
<p>Today’s class is an important one. The topic is on something that
very few people understand well. So the plan is to spend 1-2 days on
this topic so that you will understand it well. Many people have a
decent idea of what <strong>collinearity</strong> is – but tend to make
bad decisions about how to handle it.</p>
<p><strong>Collinearity – when X-variables are associated (correlated)
with eachother</strong></p>
<p>Let’s start with a question for the class. We often say that
“<em>correlation does not equal causation</em>”. More specifically, you
might run a regression between X and Y and find a relationship, but that
does not mean that X causes Y.</p>
<p><strong>Q:</strong> Why not? Discuss this a bit.</p>
<p>Let’s consider an example. There was a study that came out a few
years ago that generated some interest from the press. This study
suggested that: for each 1 additional hour of TV that you watch per day,
it increases the risk of mortality by 11%.</p>
<p>Let’s think about this for a minute. Do you think the actual act of
watching TV increases the risk of mortality? How would that happen? Is
the TV going to fall out of the wall and potentially injure you? No,
probably not. Watching TV itself is not going to cause this
relationship.</p>
<p>What’s the problem here? Well, there are a lot of other things that
are <strong>correlated</strong> to watching TV that may have their own
effect on the Y variable (mortality).</p>
<p><strong>Confounding factors – an X-variable that is correlated with
the X-variable of interest that has its own effect on Y</strong></p>
<p>What is correlated with watching TV?</p>
<ul>
<li>You aren’t being active (e.g., exercising)!</li>
<li>You might be snacking, and your snacks might not be something
healthy. You probably aren’t eating carrots if you are watching TV. I
like eating chocolate covered pretzels!</li>
<li>You aren’t working, which might relate to your socioeconomic
status.</li>
<li>You may be less likely to have healthcare.</li>
<li>Et cetera, et cetera, et cetera.</li>
</ul>
<p>These features might all be correlated to how much an individual
watches TV, and all might have their own effects on mortality.</p>
<p><strong>Q:</strong> Here’s the key idea If we really want to know
what the true effect of TV watching on mortality is, what would would we
do?</p>
<p><strong>A MANIPULATIVE EXPERIMENT</strong>. In an ideal world, we
would do a manipulative experiment, which is the only true way to show
causation.</p>
<ul>
<li>Option 1: We would try to control for all of the above factors:
everyone has the same job, goes to the gym the same amount of time, no
smoking, etc. Let’s assume this class is our sample! We would assign
half of the class to watch TV for 1 hour a day, while the other half of
the class watches for 10 hours per day.</li>
<li>Option 2: We wouldn’t try to control for any of those factors, but
we would randomly assign for 1 vs. 10 hours of TV per day. This is a
lower-quality experiment, because all of those other factors may
<em>swamp out</em> our ability to detect an effect of watching TV. We
are not controlling for anything, so all of these other factors may
create <em>noise</em>.
<ul>
<li>But, we could collect data on those factors, and include those data
in the models, and then we could avoid the swamping issue.</li>
<li>Of course, this experimental design is super impractical… There’s no
way we could get our sample to watch TV for 10 hours a day – or probably
even 1 hour per day in many cases.</li>
</ul></li>
</ul>
<p>So what do we do? Researchers often resort to performing an
<strong>observational study</strong>, where theyn collect data on all of
these other features that might also influence the response variable.
Diet, exercise, socioeconomic status, smoking, alcohol, and maybe a
whole slew of other things that we might hypothesize also influence the
response variable (mortality). We could then put all those variables
into the model and <em>statistically control</em> for those other
variables. When we include correlated variables in a model, it
statistically controls for them – and we’ll demonstrate this today.</p>
<p>We may not be able to show causation, because there may be some other
confounding variable(s) in the world that we did not measure and collect
data.</p>
<p><strong>Important takehome message:</strong> When you are conducting
studies and experiments for your thesis or your careeer, you need to
<em>think carefully</em> about what are the potential confounding
factors – the other variables – that may be correlated to the X-variable
you are interested in and that might influence your results.</p>
<ul>
<li>This is less of a concern in a well-designed manipulative study,
where random assignment of treatment removes collinearity. For example,
if I randomly assign to the class who watches 1 vs. 10 hours of TV, then
there won’t be any collinearity between exercise and TV watching.
Presumably people that go the gym often will be randomly assigned to 10
hours of TV, and people who never go to the gym will be made to watch 1
hour of TV. This random assignment removes potential collinearity
between these variables.</li>
<li>Again, manipulative experiments are the strongest approach to
inference in science, but can be impractical and many of us resort to
observational studies.</li>
</ul>
<p>My goal with this lecture today is to demonstrate to you all that: if
you have confounding variables and you don’t include them in your
model/analysis, the estimates from your analysis will be biased
(incorrect)!</p>
<p><strong>If confounding variables are not included in model/analysis
–&gt; biased (incorrect) estimates (<span
class="math inline">\(\beta\)</span>s)</strong></p>
<p>People kind of understand this… but I want you to really understand
it. If there are variables that are correlated to your X-variable of
interest and you don’t collect those data and include them, then your
estimates are biased. This is scary!! That’s why this is one of the more
important topics we will cover this semester.</p>
<p>Let’s see how this works.</p>
</div>
<div id="biased-estimates-without-confounding-variables"
class="section level2">
<h2>Biased estimates without confounding variables</h2>
<p>Let’s reconsider an example that we examined in the last class.</p>
<ul>
<li><strong>Y = size</strong></li>
<li><strong><span class="math inline">\(X_1\)</span> = age</strong></li>
<li><strong><span class="math inline">\(X_2\)</span> = sex</strong></li>
</ul>
<p>However, this time around when you were sampling the animals of
interest, for some reason you had a hard time collecting data on young
females.</p>
<p><img src="lecture_13_files/figure-html/example_1-1.png" width="432" /></p>
<p>And you also had a really hard time catching old males…</p>
<p><img src="lecture_13_files/figure-html/example_2-1.png" width="432" /></p>
<p>Let’s say I run this model:</p>
<p><strong><span class="math inline">\(Y = \beta_0 + \beta_1
Age\)</span></strong></p>
<p>Sex is not included in this model… but Sex is correlated with age!
Our sample only includes old females and young males. (A T-test
comparing age between females and males would be different.)</p>
<p><strong>Q:</strong> If I ran a regression, where would the line
go?</p>
<p><img src="lecture_13_files/figure-html/example_3-1.png" width="432" /></p>
<p>For these data, it would be ~horizontal across the data. This is a
biased estimate. This estimate incorrectly estimates the effect of age
on size of these animals.</p>
<p>However, if we run this model:</p>
<p><strong><span class="math inline">\(Y = \beta_0 + \beta_1 Age +
\beta_2 Sex\)</span></strong></p>
<p>We get two lines! And these lines will look like:</p>
<p><img src="lecture_13_files/figure-html/example_4-1.png" width="432" /></p>
<p>Now, we get <em>unbiased</em> estimates of the effect of Age on Size
for each Sex.</p>
<p>If we leave out Sex in the first model, some of that Sex effect is
being transmitted to the effect of Age.</p>
<ul>
<li>Females are smaller than males, and it’s biasing that effect on
Size.</li>
<li>The difference in size is transmitted to the effect of Age.</li>
</ul>
<p>The same thing might be happening in the TV-effect on mortality
study. There were many other variables that were correlated to watching
TV that were really influencing mortality. Their effects, which were
left out of the statistical model, were being transmitted to the effect
of watching TV.</p>
<p><strong>Q:</strong> Does that make sense? Questions?</p>
<p>Reiterating the important point: if we leave confounding variables
out of our model, we get biased estimates of the variables that we do
leave in the model.</p>
<ul>
<li>So, we want to collect data on confounding variables and include
them in the model.</li>
<li>And we want to do this even if they are not statistically
significant. We’ll talk about model building later in the class and when
you should/should not remove variables from models.</li>
</ul>
<p>In general though, we want to leave variables in to statistically
control for confounding effects.</p>
<div id="tradeoff" class="section level3">
<h3>Tradeoff</h3>
<p><strong>If confounding variables <em>are</em> included in model –&gt;
Variance Inflation</strong></p>
<p>When we run a model with multiple X-variables, the computer package
is trying to figure out which of the two variables are having an effect
on Y. But they are correlated to eachother, which makes it tough for the
software to figure out out which X-variable is having what effect. And
the more strongly they are associated with eachother, it becomes more
difficult for the software to disentangle these effects.</p>
<p>A consequence of this is that more variables cause greater
uncertainty in the effects (<span
class="math inline">\(\beta\)</span>s). Increased uncertainty is
reflected in:</p>
<ul>
<li><strong>increased SE</strong></li>
<li><strong>increased CI</strong></li>
<li><strong>increased P-values</strong></li>
</ul>
<p>This can lead to problematic results.</p>
<p>I mentioned during the last class that, generally, if we add a
variable to a model, the p-values for the variables in a model go down.
<u>However, collinearity is the one exception to that rule!</u> And in
fact, it’s a red flag.</p>
<p>My recommendation is that, before you even start your analysis, you
need to know what the collinearity is between your X-variables! Plot
your X-variables, run some linear models (e.g., between Age and Sex),
and learn about what the collinearity is.</p>
<ul>
<li>Some people fail to do this and problems occur. For example, they
might build a model where Age was significant… and then they add Sex,
and Age is no longer significant…</li>
<li>If the P-value goes up a lot when you add a variable to a model,
this is a symptom of collinearity! This is happening because of
collinearity between X-variables.</li>
</ul>
</div>
</div>
<div id="symptoms-of-collinearity" class="section level2">
<h2>Symptoms of collinearity</h2>
<ol style="list-style-type: decimal">
<li>Collinearity between independent variables</li>
</ol>
<ul>
<li>High <span class="math inline">\(r^2\)</span> values</li>
<li>Statistically-significant relationships between X-variables</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>High variance inflation factors (VIF) of variables in model</li>
<li>Variables significant in simple regression, but not in
multi-variable regression</li>
<li>Individual variables not significant in multi-variable regression
model, but the overall multi-variable regression model is
significant</li>
<li>Large changes in coefficient estimates between full and reduced
models</li>
<li>Large SE in multi-variable regresion models, despite high power</li>
</ol>
<p>Breaking this list down a bit…</p>
<ol style="list-style-type: decimal">
<li><p>The easiest way to know if you have collinearity is to examine
fit a linear model between your two independent variables and see if
they have <strong>high <span class="math inline">\(r^2\)</span>
values</strong> or if there is a <strong>statistically-significant
relationship</strong> between them.</p></li>
<li><p>High variance inflation factors (VIF) of variables in model. Next
class I will show you how to examine VIF for variables in a
model.</p></li>
<li><p>As I said before, if you have significant variables in a simple
linear regression, but then you add another variable(s) to create a
multi-variable regression and the individual variable(s) is no longer
significant, then you have collinearity.</p></li>
<li><p>If you build a multi-variable model where (i) none of the
individuals variables are significant, but (ii) the overall whole model
is significant (the p-value at the bottom of the model0), then you have
collinearity. The whole model is significant, but none of the individual
variables is significant.</p></li>
</ol>
<ul>
<li>How does that happen? All of the X-variables are explaining a lot of
the Y-variable, but the software is having a hard-time figure out which
X-variable is explaining the response. And the collinearity causes the
p-values for individual variables to go up.</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li><p>Generally, when you add variables to a model, the betas
<em>should not change too much</em>. What do I mean by ‘too much’? E.g.,
when you have a simple model with beta and CI, and then you add another
X-variable to the model to create a multi-variable model, then the new
estimate for your beta should be within the 95% CI of the original
estimate. If it moves to be outside of the 95% CI of the original
estimate, that’s a considerable change and is a symptom of collinearity
of the X-variables.</p></li>
<li><p>What if you have 25,000 samples, and you have huge standard
errors. Why are the SE so big, when we have such a large sample size?
Collinearity in the system can increase SE, increase CI, and decrease
p-values.</p></li>
</ol>
<p><strong>Questions?</strong></p>
</div>
<div id="simulation-exercise-1" class="section level2">
<h2>Simulation Exercise 1</h2>
<p>When confronted with collinearity, a common approach taken by many
folks in our field is to <u>take variables out of the model</u>.
However, this has a consequence that they do not understand – it biases
our estimates! – so this is not the approach I recommend.</p>
<p>I’m going to show you a simulation exercise that demonstrates:</p>
<ol style="list-style-type: decimal">
<li>Collinearity influences parameter <strong>estimates</strong>,
<strong>uncertainty</strong>, and <strong>p-values</strong> when
confounding variables are not included in the model, and</li>
<li>When all the necessary confounding variables are included,
collinearity increases uncertainty and p-values but <u>does not bias
estimates</u>.</li>
</ol>
<div id="truth" class="section level3">
<h3>Truth</h3>
<p><span class="math inline">\(\beta_1 = 3\)</span> <span
class="math inline">\(\beta_2 = 3\)</span> <span class="math inline">\(z
= U[0.5,20]\)</span> – an estimate of how much correlation
(collinearity) there is between <span class="math inline">\(X_1\)</span>
and <span class="math inline">\(X_2\)</span></p>
<p>For the simulation exercise, I simulated 1,000 datasets with varying
degrees of collinearity (correlation) between two X-variables. Here is
truth:</p>
<ul>
<li>Simulations: <span class="math inline">\(n = 1,000\)</span></li>
<li><span class="math inline">\(y = 10 + 3X_1 + 3X_2 + \epsilon \sim
N(0, 2)\)</span></li>
<li><span class="math inline">\(X_1 = U[0,10]\)</span></li>
<li><span class="math inline">\(X_2 = X_1 + N(0, z)\)</span></li>
<li>For each simulation, I used a different value of <em>z</em> from a
uniform distribution: <span class="math inline">\(z = U[0.5,
20]\)</span>.</li>
</ul>
<p>Low values of <em>z</em> (e.g., <em>z</em> = 0.5) caused high
collinearity between the X-variables, because only a little bit of noise
gets added to <span class="math inline">\(X_1\)</span> to calculate
<span class="math inline">\(X_2\)</span>. Alternatively, high values of
<em>z</em> (e.g., <em>z</em> = 20) had low collinearity, because large
noise was added to <span class="math inline">\(X_1\)</span> to calculate
<span class="math inline">\(X_2\)</span>. In other words, the low values
of <em>z</em> caused very little noise (and high collinearity) between
<span class="math inline">\(X_1\)</span> and <span
class="math inline">\(X_2\)</span>, while the high values caused great
noise (and low collinearity) between the variables.</p>
<p>For each simulation, I did a few things:</p>
<ul>
<li>Fit a simple model (<span class="math inline">\(Y ~ X_1\)</span>)
and measured the estimate, SE, and p-value for <span
class="math inline">\(\beta1\)</span></li>
<li>Fit a multi-variable model (<span class="math inline">\(Y ~ X_1 +
X_2\)</span>) that included both of the collinear, confounding
variables, and measured the effect, SE, and p-value for <span
class="math inline">\(\beta1\)</span>.</li>
</ul>
<p>Most importantly, I measured how collinearity between <span
class="math inline">\(X_1\)</span> and <span
class="math inline">\(X_2\)</span> (i.e., <span
class="math inline">\(r^2\)</span>) influenced the the Variance
Inflation Factor from the multi-variable model.</p>
<p>I’m going to show a number of graphs, and for all of these graphs the
X-axis is a measure of collinearity (correlation; <span
class="math inline">\(r^2\)</span>) between <span
class="math inline">\(X_1\)</span> and <span
class="math inline">\(X_2\)</span>. If it’s collinearity equals 1, then
the variables are perfectly correlated, whereas low values show weak/no
correlation.</p>
<p>The first graph I am going to show you describes how the
<strong>Variance Inflation Factor</strong> score is influenced by
collinearity. The <strong>Variance Inflation Factor</strong> is a metric
used to diagnose whether there may be collinearity between X-variables
in a multiple-variable model.</p>
<p><strong>Variance Inflation Factor (VIF) – the amount (in
<em>times</em>) that the variance (<span
class="math inline">\(SE^2\)</span>) in the <span
class="math inline">\(\beta\)</span> increases due to
collinearity</strong></p>
<ul>
<li>E.g., if VIF = 4, then the variance is 4 times what it would be if
you didn’t have collinearity – and the SE would be doubled (<span
class="math inline">\(SE = \sqrt4 = 2\)</span>).</li>
</ul>
<p>First, let’s see how the VIF is influenced by collinearity:</p>
<p><img src="lecture_13_files/figure-html/simulation_2-1.png" width="768" /></p>
<p>This graph shows how VIF scores increase with increasing
collinearity.</p>
<p>Some people say that if the VIF &gt; 10, you need to worry about
collinearity. In this case, the SE would be triple what it would
otherwise be without collinearity. This happens when <span
class="math inline">\(r^2 = 0.9\)</span>, and the two X-variables are
almost perfectly correlated. This is too high, in my opinion.</p>
<p>Rule-of-thumb: <strong>VIF &gt; 2, recognize that you have
collinearity in your data.</strong></p>
<p>VIF = 2 ~ <span class="math inline">\(r^2\)</span> = 0.6 – so another
rule of thumb is that an <span class="math inline">\(r^2 &gt;
0.6\)</span> may also indicate collinearity.</p>
</div>
<div id="simple-regression-model-y-sim-x_1" class="section level3">
<h3>Simple regression model: <span class="math inline">\(Y \sim
X_1\)</span></h3>
<p>Now, let’s consider results from the simple model (<span
class="math inline">\(Y \sim X_1\)</span>) across the simulations.</p>
<p>The most important thing I want to emphasize to you all is in this
first graph. How is the <strong>beta for <span
class="math inline">\(X_1\)</span></strong> influenced by collinearity
in the simple model?</p>
<p><img src="lecture_13_files/figure-html/simulation_3-1.png" width="768" /></p>
<p>This graph shows, again, the <span class="math inline">\(r^2\)</span>
between <span class="math inline">\(X_1\)</span> and <span
class="math inline">\(X_2\)</span> (how correlated they are) on the
X-axis, and the Y-axis shows the estimate for the beta for <span
class="math inline">\(X_1\)</span>. If there is no collinearity, the
estimate is 3 – which is truth! Bottom-left-hand side of the graph.</p>
<p>However, even with a very little bit of collinearity (<span
class="math inline">\(r^2 &lt; 0.2\)</span>), the parameter estimate is
biased! As collinearity increases, the parameter estimate jumps pretty
quickly up to 6!</p>
<p><strong>Q:</strong> Why 6? 6 = 3 + 3. Because the two X-variables are
correlated, the estimation model starts to assign the effect of <span
class="math inline">\(X_2\)</span> onto <span
class="math inline">\(X_1\)</span>.</p>
<p>Not including the confounding variable introduces bias in our
estimates and negatively influences our inference.</p>
<p>Now, often in this class we ask: how do different approaches
influence betas, uncertainty, and p-values. Next we might ask ourselves:
how is <strong>standard error (SE)</strong> influenced by collinearity
in the simple model?</p>
<p><img src="lecture_13_files/figure-html/simulation_4-1.png" width="768" /></p>
<p>This is where it gets a little scary.</p>
<p>Not only is the parameter estimate biased, the standard error gets
smaller and smaller. As collinearity increases, your estimates are
getting more and more precise…</p>
<p>How are <strong>p-values</strong> influenced by collinearity in the
simple model?</p>
<p><img src="lecture_13_files/figure-html/simulation_5-1.png" width="768" /></p>
<p>As uncertainty gets smaller and samller, our p-values also get
smaller and smaller, because the effect of collinearity gets bigger and
bigger…</p>
<p>So, collinearity is <strong>really dangerous</strong>! If you have
collinearity in your data, and you leave out some confounding variable,
you are going to get biased estimates, and it’s going to seem like it’s
a really precise estimate with a clear result. Unless…</p>
<p><strong>Q:</strong> What if the effect of <span
class="math inline">\(X_2\)</span> was -3? The two effects might cancel
eachother out, and our estimate would be zero, with a small SE, and
<em>false confidence that there is no effect</em>.</p>
<p>If the effect of the unaccounted confounding variable, <span
class="math inline">\(X_2\)</span>, has an effect in the opposite
direction, the collinear effects may partially or wholly cancel
eachother out, and you may fail to observe any true effect of X on
Y.</p>
</div>
<div id="multi-variable-model-y-sim-x_1-x_2" class="section level3">
<h3>Multi-variable model: <span class="math inline">\(Y \sim X_1 +
X_2\)</span></h3>
<p>Now, let’s consider results from the multi-variable model (<span
class="math inline">\(Y \sim X_1 + X_2\)</span>) across the simulations.
This is the model where we have <em>statistically controlled</em> for
that collinearity.</p>
<p>Let’s start here by asking: how does collinearity influence the
<strong>beta for <span class="math inline">\(X_1\)</span></strong> in
the multi-variable model?</p>
<p><img src="lecture_13_files/figure-html/simulation_6-1.png" width="768" /></p>
<p>On the Y-axis, we have the coefficient estimate for <span
class="math inline">\(\beta_1\)</span>, and the X-axis is again the
correlation between the two X-variables.</p>
<p>The estimate is good, right around 3 (truth)!! But as collinearity
increases, the estimate can be more variable. So by including the
confounding variables, your estimate is good, but the bad news is that
due to the collinearity, you have less certainty in your estimate. But
at least the estimate is unbiased, which is the most important
thing.</p>
<p>Remember the rule of thumb about <span
class="math inline">\(r^2\)</span> indicating collinearity? Previously I
said that when <span class="math inline">\(r^2 = 0.6\)</span>,
collinearity should be on your radar. In this graph, we can see that
simulations with collinearity greater than 0.6 start to have parameter
estimates that are more variable. However, the mean is still centered on
~3 (Truth) after this rule-of-thumb. So no matter how much collinearity
there is, on average we will continue to get an unbiased estimate of the
effect of X-variables in the model. In statistics, any one estimate may
be bad – but on average, for this multi-variable model, the estimates
will be unbiased.</p>
<p>Another thing that we can see from this graph is the <strong>variance
inflation</strong> itself! As collinearity increases along the X-axis,
the cone of simulation results gets bigger, because there is more and
more uncertainty in what the parameter estimate really is.</p>
<p><img src="lecture_13_files/figure-html/simulation_7-1.png" width="768" /></p>
<p>And we see this here as well with the SE estimates. As collinearity
increases, SE of the estimate increases, again suggesting increasing
uncertainty due to collinearity.</p>
<p><img src="lecture_13_files/figure-html/simulation_8-1.png" width="768" /></p>
<p>And again, as uncertainty increases, p-values increase as well.</p>
<p>But hopefully the takehome message is clear. Failing to account for
confounding variables in a model will bias estimates. Accounting for
those confounding variables will produce unbiased estimates, although
the uncertainty may increase.</p>
</div>
<div id="confounding-variables" class="section level3">
<h3>Confounding variables</h3>
<p><strong>Confounding variable – a variable that will bias results if
you leave it out</strong>. There are two key features for confounding
variables:</p>
<ul>
<li>Correlated with another X-variable</li>
<li>Has it’s own effect on Y</li>
</ul>
<p>To avoid negative effects of confounding variables, I recommend:</p>
<ol style="list-style-type: decimal">
<li><p>Sample in a manner that eliminates collinearity. Collinearity may
be real, or it may be due to sampling artifact. E.g., in the first
example above, collinearity was introduced due to non-random sampling of
the different sexes. Collect your data in a way that eliminates
collinearity.</p></li>
<li><p>Use multi-variable regression. If you have collinear X-variables,
it may cause your SE to be large, but <u>your estimates will be
unbiased</u>. If you fail to include confounding variables, then
estimates will be biased.</p></li>
<li><p>Include confounding variables, even if they are
non-significant.</p></li>
</ol>
<p>Scenarios: two collinear variables (Age and Sex), and you run the
full model. VIF is really high (e.g., 10). SE and CI are very inflated,
but everything is still significant. Do you do anything? No.</p>
<ul>
<li>Your estimates are unbiased, because you ran the full model.</li>
<li>Your CI are large, but still statistically significant.</li>
<li>You have identified that you have collinearity, which caused the
large CI. But the betas are unbiased.</li>
</ul>
<p>But sometimes you run the analysis but X-variables are not
significant. What do you do then?</p>
<ol start="4" style="list-style-type: decimal">
<li>Get more data! This decreases SE and VIF.</li>
</ol>
<p>Most problems in statistics can be improved by getting more data.
Increasing N decreases SE, CI, and P-values… If none of your data are
significant due to collinearity, maybe you just need to collect more
data.</p>
<ol start="5" style="list-style-type: decimal">
<li>There are statistical techniques that can eliminate collinearity.
One of those is Principal Component’s Analysis, which we may talk about
in this class. Popular analysis. What this analysis does is it creates
<em>new variables</em> that are orthogonal – which means they have zero
collinearity.</li>
</ol>
</div>
<div id="redundant-variables" class="section level3">
<h3>Redundant variables</h3>
<p>If collinearity between X-variables results in biased estimates, why
do so many people throw out their variables in favor of reduced models
that will suffer from collinearity and thus have biased estimates?</p>
<p>Well, you can do this <u><em>sometimes</em></u> – primarily when your
collinear X-variables <u>don’t have an effect on Y</u>.</p>
<p>These are sometimes called <strong>redundant variables – collinear
X-variables that don’t have an effect on the Y-variable</strong>.</p>
<ol style="list-style-type: decimal">
<li>Correlated to another X-variable, but</li>
<li>Do not have an effect on Y</li>
</ol>
<p>They are similar to confounding variables by being correlated to
another X-variable, but the key difference is that they do not have an
effect on Y.</p>
<ul>
<li>If they don’t really have an effect (i.e., <span
class="math inline">\(\beta\)</span> = 0), there is no problem when you
take them out. Leaving out a redundant variable does not cause bias,
because it does not have an effect to add onto another variable included
in the model!</li>
<li>If you leave out a confounding variable, it assigns it’s effect to
the other collinear variable you left in. In the case of redundant
variables, leaving them out assigns the effect to the other X-variable –
but that effect was zero!</li>
<li>If you include a redundant variable, it will cause <strong>variance
inflation</strong>.</li>
</ul>
<p>In truth, there are no confounding or redundant variables, but rather
it’s a continuum depending on the <span
class="math inline">\(\beta\)</span>.</p>
<ul>
<li>If the <span class="math inline">\(\beta \neq 0\)</span>, it’s a
confounding variable.</li>
<li>If the <span class="math inline">\(\beta = 0\)</span>, it’s a
redundant variable.</li>
</ul>
<p>You don’t need to leave in redundant variables, and leaving them in
will generate variance inflation.</p>
<p>What’s an example of a redundant variable? Watching TV and it’s
effect on mortality. If you leave this in your analysis, you will
measure a <span class="math inline">\(\beta = 0\)</span> after you
include all of the other important things that truly do influence
mortality. And by leaving it in the model you will increase SE, CI, and
p-values of all of the other important variables.</p>
</div>
</div>
<div id="redundant-vs.-confounding-variables" class="section level2">
<h2>Redundant vs. Confounding Variables</h2>
<p>Like I said before, many scientists tend to throw out correlated
X-variables because they are afraid of ‘collinearity’, and they want to
reduce the model to avoid variance inflation.</p>
<p>This is fine – <u>if you are 100% certain that those variables are
redundant</u>.</p>
<p>If they are confounding variables, they must be included – because
removing them will cause biased betas, reduced uncertainty, and small
p-values.</p>
<p>So, how do we know whether variables are <strong>redundant</strong>
or <strong>confounding</strong> variables?</p>
<p><strong>Redundant vs. Confounding Variable</strong></p>
<div id="logic" class="section level3">
<h3>1. Logic!</h3>
<p>Put some thought into your variables. Statistics is a thinking
exercise and should involve critical thinking. Is it likely that your
X-variable has it’s own effect on Y? If so, include it.</p>
<p><img src="pic_gopher_tortoise.jpg" style="width:50.0%" /></p>
<p>Example: Gopher Tortoises. The gopher tortoise is a burrow-digging
turtle species in the southeastern United States that is a species of
conservation concern. Reproduction is important for populations to grow
and persist, and researchers are interested in what causes reproduction
in tortoise populations. Male tortoises are known to compete with
eachother for access to females, and they often get in wrestling matches
for access to females. Males sometimes have large shell extensions under
their neck called ‘gular scutes’ that are used to flip other males
during male-male combat. People have wondered what are the factors that
influence reproductive success of tortoises: age, body size, or gular
scute size? These factors are all correlated: older tortoises get larger
bodies and may grow larger gular scutes. However, there is likely
variation in growth rates such that sometimes younger tortoises may be
larger and/or have larger gular scutes than older tortoises. The
question is: are these variables redundant or confounding variables?</p>
<p>There may be an argument that they all of these variables are
measures of age, and shell size and gular scute size are redundant
variables. Alternatively, it could be that they each have their own
effect, where age, size, and gular scute length have different effects
on reproductive success. Maybe: (1) older tortoise are more experienced
at winning wrestling matches, (2) larger animals are better at winning
wrestling matches, and (3) animals with larger gular scutes are better
at winning wrestling matches or provides a signal of fitness to females.
So each one could have a different mechanism, such that you might have
an instance where a young, small animal but with big gular scutes
performs well at reproducing.</p>
<p>So, think about mechanisms for how different X-variables might
influence Y. If those mechanisms are different, then they may have
different effects on Y, and thus may be <strong>confounding
variables</strong>.</p>
<p>A helpful way to do this is to build a <u>conceptual model</u>
describing hypotheses for how all variables might be related in your
system and then use that model to guide what variables you might want to
measure. I recommend this to most students and collaborators I work
with!</p>
</div>
<div id="what-happens-between-full-and-reduced-models"
class="section level3">
<h3>2. <strong>What happens between ‘full’ and ‘reduced’
models?</strong></h3>
<p>A full model has all of the X-variables, whereas a reduced model has
a subset of X-variables. How does <u>reducing</u> the model influence
the betas and p-values? If you reduce the model and the betas change a
lot, this is an indication of collinearity.</p>
<ul>
<li><strong>Confounding variable – removing it changes the betas for the
other variables</strong></li>
<li><strong>Redundant variable – removing it does not change other
betas</strong></li>
</ul>
<p>(‘Change’ is a subjective term, but our usual rule of thumb would be
that it caused effect to move outside of the original confidence
limits.)</p>
<p>Conversely, we can think about this in terms of <u>adding</u>
variables to a model.</p>
<p><strong>When adding other variables, redundant <span
class="math inline">\(\beta\)</span>s tend to go to zero</strong></p>
<p>A good example of this is the ‘watching TV study’! It looks like
watching TV has an effect when it’s all by itself in the model. But when
you add the confounding variables that truly have an effect on Y, the
effect of TV will probably go to zero, because it is a redundant
variable. So, when you add other variables to a model, the beta for the
redundant variable will tend to go to zero.</p>
<p>In other words, if a variable had an effect but then you added in a
bunch of other variables and the effect disappeared – then it probably
never had an effect to begin with, it was just a redundant variable.</p>
<p><strong>When adding other variables, confounding <span
class="math inline">\(\beta\)</span>s will still be
‘substantial’</strong>. They may not still be statistically significant
because we have added more variables and contributed to variance
inflation. However, if we look at the beta and it still looks like a
biologically-significant effect, then it may be a confounding variable
that lost significance due to variance inflation.</p>
<p>In general, betas should not change when you add additional variables
to a model. If they do, you have collinearity.</p>
<p>This is a point in the class where things are starting to get pretty
nuanced and we need to be pretty strategic and careful about how we
approach our work.</p>
</div>
<div id="r2-between-variables" class="section level3">
<h3>3. <strong><span class="math inline">\(r^2\)</span> between
variables</strong></h3>
<p>This is not a rule, but rather a guideline. If you examine the
correlation between two X-variables and see an extremely high <span
class="math inline">\(r^2\)</span> value, it’s most likely that those
variables are redundant. Very, very strong collinearity tends to imply
redundancy.</p>
<p>Rule-of-thumb: high <span class="math inline">\(r^2\)</span> tends to
suggest redundancy.</p>
</div>
<div id="not-sure-treat-as-confounding." class="section level3">
<h3>4. <strong>Not sure? Treat as confounding.</strong></h3>
<p>If you are not sure, treat it as confounding.</p>
<p><strong>Q:</strong> Why?</p>
<p>Because then you will include it in your model, account for its
statistical effect, if one exists. If it’s actually a redundant
variable, then there is no effect! Your parameter estimates will be
unbiased and the only consequence will be a little bit of variance
inflation and reduced uncertainty. This is much better than leaving out
an important, confounding variable and having consequences on your
betas!</p>
<p><u>General takeaway here</u>: if adding variable(s) causes the beta
of a variable to change to zero, then it’s probably a redundant
variable, and this should be pretty obvious.</p>
</div>
</div>
<div id="simulation-exercise-2" class="section level2">
<h2>Simulation Exercise 2</h2>
<p>This simulation exercise is similar as before, but now only <span
class="math inline">\(X_1\)</span> has an effect, and <span
class="math inline">\(X_2\)</span> is a redundant variable.</p>
<p>Let’s look at how correlation between <span
class="math inline">\(X_1\)</span> and <span
class="math inline">\(X_2\)</span> influences the Variance Inflation
Factor.</p>
<p><img src="lecture_13_files/figure-html/sim_exercise_2_2-1.png" width="768" /></p>
<p>The relationship between collinearity and Variance Inflation does not
change if it’s a confounding or redundant variable. Adding a redundant
variable will still cause Variance Inflation, as I mentioned before.</p>
<div id="simple-regression-model-y-sim-x_1-1" class="section level3">
<h3>Simple regression model: <span class="math inline">\(Y \sim
X_1\)</span></h3>
<p>Now, we see that the existence of a redundant variable does not
influence our estimation of <span class="math inline">\(\beta_1\)</span>
using a simple linear model! This is good.</p>
<p><img src="lecture_13_files/figure-html/sim_exercise_2_3-1.png" width="768" /></p>
<p>There is some variation in the parameter estimates due to random
chance, but the estimates are unbiased along the gradation of
correlation with the redundant, collinear variable.</p>
<p><img src="lecture_13_files/figure-html/sim_exercise_2_4-1.png" width="768" /></p>
<p>SE estimates are also unbiased and do not change due to the existence
of the redundant variable.</p>
<p><img src="lecture_13_files/figure-html/sim_exercise_2_5-1.png" width="768" /></p>
<p>Nor are p-values influenced!</p>
<p>This simulation supports my previous suggestion that: redundant
variables do not influence estimation, error, or p-values in our simple
linear model.</p>
</div>
<div id="simple-linear-model-w-redundant-var-y-sim-x_2"
class="section level3">
<h3>Simple linear model w/ redundant var: <span class="math inline">\(Y
\sim X_2\)</span></h3>
<p>What if I now run a simple linear model but with <span
class="math inline">\(X_2\)</span> as the only X-variable? Remember,
<span class="math inline">\(X_2\)</span> is collinear with <span
class="math inline">\(X_1\)</span> – <u>but does not have an effect on
Y</u>.</p>
<p><img src="lecture_13_files/figure-html/sim_exercise_2_6-1.png" width="768" /></p>
<p>So this is interesting. When <span class="math inline">\(X_2\)</span>
has low correlation with <span class="math inline">\(X_1\)</span>, the
effect is measured as zero – because the two variables are not
collinear, and <span class="math inline">\(X_2\)</span> truly does not
have an effect. However, when correlation between <span
class="math inline">\(X_2\)</span> and <span
class="math inline">\(X_1\)</span> increases, all of the sudden the true
effect of <span class="math inline">\(X_1\)</span> (the variable not
included in the model), is now assigned to <span
class="math inline">\(X_2\)</span>! The stronger the relationship
between X1 and X2, the more the effect of X1 is assigned to X2.</p>
<p>This is what is happening in the TV-mortality study. A redundant,
collinear variable is being assigned the effect(s) of other true,
confounding variables.</p>
</div>
<div id="multi-variable-model-y-sim-x_1-x_2-1" class="section level3">
<h3>Multi-variable model: <span class="math inline">\(Y \sim X_1 +
X_2\)</span></h3>
<p>Now, what happens with the multi-variable model?</p>
<p>Estimates of <span class="math inline">\(X_1\)</span>:</p>
<p><img src="lecture_13_files/figure-html/sim_exercise_2_7-1.png" width="768" /></p>
<p>Our estimate of <span class="math inline">\(X_1\)</span> is unbiased
and close to truth! But the variance increases due to the presence of
the collinear, redundant variable in the model. The cone shape suggests
more uncertainty due to the variance inflation and the presence of the
redundant variable. With two collinear variables in the model, R has a
harder time figuring out which was has the effect.</p>
<p>Let’s see what it estimates for <span
class="math inline">\(X_2\)</span>:</p>
<p><img src="lecture_13_files/figure-html/sim_exercise_2_8-1.png" width="768" /></p>
<p>It estimates the effect of the redundant variable to be zero! As
collinearity increases, the estimate remains unbiased and correct at
around zero – although the variance inflation again causes less
certainty in the estimate, on average.</p>
</div>
</div>
<div id="practical-guidance-for-examining-and-dealing-with-collinearity"
class="section level2">
<h2>Practical guidance for examining and dealing with collinearity</h2>
<p>These lists mostly review what I have talked about during class so
far today - quite a bit of content! [<strong>Show these lists on the
projector</strong>]</p>
<p>Do you have collinearity in your data or system?</p>
<ol style="list-style-type: decimal">
<li>Be careful to identify potential confounding variables prior to data
collection. Use logic and try to identify all confounding variables and
measure these.</li>
<li>Calculate collinearity and VIF among independent variables – before
you start your analysis. High collinearity between X-variables tends to
imply redundancy.</li>
<li>Pay attention to how coefficient estimates and variable significance
change as variables are removed or added.</li>
</ol>
<p>Is a variable redundant or confounding?</p>
<ol style="list-style-type: decimal">
<li>Think! Use logic.</li>
<li>If there is extreme collinearity</li>
</ol>
<ul>
<li>Likely <strong>redundant</strong></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Large changes in coefficient estimates of both variables between
full and reduced models</li>
</ol>
<ul>
<li><strong>Confounding</strong></li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Large changes in coefficient estimates of one variable between full
and reduced models</li>
</ol>
<ul>
<li><strong>Redundant</strong> – full model estimate close to zero</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>Not sure whether it’s redundant or confounding? Assume confounding
&amp; include it.</li>
</ol>
<ul>
<li>Multi-variable regression also produces unbiased estimates (on
average) regardless of the type of collinearity</li>
</ul>
<p>What to do with <strong>redundant variables</strong>?</p>
<ol style="list-style-type: decimal">
<li>Determine which variable best explains the response using P-values
from regression and changes in coefficient estimates with variable
addition and removal</li>
<li>Do not include redundant variable in final model (to reduce
VIF)</li>
<li>Try a variable reduction technique (e.g., PCA)</li>
</ol>
<p>What to do with <strong>confounding variables</strong>?</p>
<ol style="list-style-type: decimal">
<li>Sample in a manner that eliminates collinearity, which can be due to
real collinearity or sampling artifact</li>
<li>Use multiple regression; may hvae large SE if collinearity is
strong</li>
<li>Include confounding variables, even if non-significant.</li>
<li>Get more data! Decrease SE due to variance inflation.</li>
</ol>
</div>
<div id="analysis-in-r" class="section level2">
<h2>Analysis in R</h2>
<p>Let’s examine some data I simulated in R. The dataset is located <a
href="lecture_13_dataset1.csv">here</a>.</p>
<p>These data are similar to the Age, Sex, and Size data we simulated
for last class. There is no collinearity between Age and Sex. However,
now there are two additional X-variables, MotherSize and FatherSize, and
both of those variables are a function of Age. All of these variables
influence size, so they are all examples of <u>confounding
variables</u>.</p>
<pre class="r"><code>par(mar=c(4,4,0,0))

# Read in the data
datum &lt;- read.csv(&quot;lecture_13_dataset1.csv&quot;)

# Examine the data
head(datum, 10)</code></pre>
<pre><code>##         Age    Sex Male MotherSize FatherSize      Size
## 1  3.588198 Female    0 -1.1584838   4.375937  9.725305
## 2  8.094746 Female    0  3.0661995   8.863788 18.078616
## 3  4.680792 Female    0 -0.3620782   5.012995 11.676238
## 4  8.947157 Female    0  5.3157589   7.938780 20.144040
## 5  9.464206 Female    0  4.2384346   9.344753 22.191753
## 6  1.410008 Female    0 -2.0735209   1.129613  5.855379
## 7  5.752949 Female    0 -0.7958034   6.315939 14.598091
## 8  9.031771 Female    0  4.6163851   8.659333 20.707404
## 9  5.962915 Female    0  1.0867694   6.939889 14.458792
## 10 5.109533 Female    0  0.3254742   4.734952 11.450063</code></pre>
<pre class="r"><code># Plot the data
plot(Size ~ Age, data = datum)</code></pre>
<p><img src="lecture_13_files/figure-html/analysis_1-1.png" width="432" /></p>
<p>It’s useful to write Truth up on the white board so that we can refer
back to that while running the analysis in R…</p>
<p>Truth:</p>
<ul>
<li>Age = 1.5</li>
<li>Sex = 2.5</li>
<li>MotherSize = 0.2</li>
<li>FatherSize = 0.2</li>
<li><span class="math inline">\(\sigma = 1.2\)</span></li>
</ul>
<p>We want to start by testing if there is collinearity among our data.
The easiest way to do this is by running linear models among our
X-variables.</p>
<pre class="r"><code># Run lm() for different pairs of X-variables
results &lt;- lm(Age ~ Sex, data = datum)
summary(results)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Age ~ Sex, data = datum)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.459 -2.293 -0.285  2.430  4.571 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   5.6808     0.3635  15.627   &lt;2e-16 ***
## SexMale      -0.3876     0.5141  -0.754    0.453    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.571 on 98 degrees of freedom
## Multiple R-squared:  0.005766,   Adjusted R-squared:  -0.004379 
## F-statistic: 0.5683 on 1 and 98 DF,  p-value: 0.4527</code></pre>
<p>The p-value is high (non-significant) and <span
class="math inline">\(r^2\)</span> value is low, suggesting there is no
collinearity. (And there isn’t, because I didn’t simulate there to be
any.)</p>
<pre class="r"><code># Run lm() for different pairs of X-variables
results &lt;- lm(Age ~ MotherSize, data = datum)
summary(results)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Age ~ MotherSize, data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.25851 -0.51424  0.02135  0.63884  2.19596 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  5.10454    0.09270   55.07   &lt;2e-16 ***
## MotherSize   0.88276    0.03387   26.07   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9153 on 98 degrees of freedom
## Multiple R-squared:  0.874,  Adjusted R-squared:  0.8727 
## F-statistic: 679.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><span class="math inline">\(r^2\)</span> is now 0.874 – definitely
collinearity here! (And we know this to be the case.)</p>
<p>And we can do this for other pair of X-variables as well, as much as
we want or need to.</p>
<p>However, an alternative way to do this is to create a ‘correlation
matrix’ using the function ‘cor()’, which calculates the correlation
among all pairs of continuous X-variables your provide to the
function.</p>
<ul>
<li>‘cor()’ won’t work for our dataframe, because it requires all
continuous variables and Sex is a categorical variable. So drop Sex and
Size.</li>
<li>This will run a simple ‘lm()’ between all pairs of X-variables and
calculate <em>r</em> value for each pair (measure of correlation)</li>
</ul>
<pre class="r"><code># Create new dataframe without categorical Sex
datum2 &lt;- subset(datum, select = -Sex)
datum2 &lt;- subset(datum2, select = -Size)

# Examine new subsetted data
head(datum2)</code></pre>
<pre><code>##        Age Male MotherSize FatherSize
## 1 3.588198    0 -1.1584838   4.375937
## 2 8.094746    0  3.0661995   8.863788
## 3 4.680792    0 -0.3620782   5.012995
## 4 8.947157    0  5.3157589   7.938780
## 5 9.464206    0  4.2384346   9.344753
## 6 1.410008    0 -2.0735209   1.129613</code></pre>
<pre class="r"><code># Correlation matrix - runs a simple &#39;lm()&#39; between each pair of X-variables
cor(datum2)</code></pre>
<pre><code>##                    Age        Male MotherSize  FatherSize
## Age         1.00000000 -0.07593267  0.9348535  0.93823222
## Male       -0.07593267  1.00000000 -0.1457588 -0.08094226
## MotherSize  0.93485348 -0.14575882  1.0000000  0.86631881
## FatherSize  0.93823222 -0.08094226  0.8663188  1.00000000</code></pre>
<p>[Squaring this would produce the usual <span
class="math inline">\(r^2\)</span> metric we are familiar with.]</p>
<p>This is a quick-and-easy way to assess correlation between
X-variables.</p>
<p>Another way is to run a full linear model with all of the X-variables
and ask for the Variance Inflation Factors! The VIF function exists in
the package ‘car’, which you will have to download to your computer one
time. It has a lot of ‘dependencies’ (other necessary R packages) that
will have to be downloaded…</p>
<pre class="r"><code># Download &#39;car&#39; by uncommenting this next line and running it through your console
# install.packages(&quot;car&quot;, dependencies=TRUE)</code></pre>
<p>Now we can fit a linear model with all X-variables and then run the
VIF function.</p>
<pre class="r"><code># Full linear model with all X-variables
results &lt;- lm(Size ~ Age + Sex + MotherSize + FatherSize, data = datum)

# Use the VIF function from &#39;car&#39;
# using &#39;car::vif()&#39; will tell R to call the function &#39;vif()&#39; from the package &#39;car&#39; specifically
car::vif(results)</code></pre>
<pre><code>##        Age        Sex MotherSize FatherSize 
##  17.062688   1.055190   8.381097   8.437139</code></pre>
<p>These are the Variance Inflation Factors for each X-variable. These
effects describe how much (in <em>times</em>) the variance of each
variable increased due to collinearity. So, the variance of Age was 17
times increased what it would have been in the absence of
collinearity…</p>
<pre class="r"><code># Square-root this to get VIF in terms of Standard Error
car::vif(results)^0.5</code></pre>
<pre><code>##        Age        Sex MotherSize FatherSize 
##   4.130701   1.027225   2.895012   2.904675</code></pre>
<p>SE of Age is 4 times larger than what it would have been without
collinearity. Sex is not influenced (times ~1), but the SE and 95% CI
are 4 or 3 times larger than what would happen without collinearity.</p>
<p>Note: the VIF is the variance inflation due to collinearity of all
other variables combined. So collinearity with multiple other
X-variables can contribute to variance inflation of each variable.</p>
<pre class="r"><code># Simple linear model of Size ~ Age
results &lt;- lm(Size ~ Age, data = datum)
summary(results)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Size ~ Age, data = datum)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.9934 -1.1775  0.0039  0.9656  2.9169 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.55544    0.32958   13.82   &lt;2e-16 ***
## Age          1.86126    0.05446   34.17   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.39 on 98 degrees of freedom
## Multiple R-squared:  0.9226, Adjusted R-squared:  0.9218 
## F-statistic:  1168 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>This estimate is wrong – not close to truth – and the confidence
intervals are small.The effect size and confidence intervals were
misleading because we did not account for confounding variables!</p>
<pre class="r"><code># Simple linear model of Size ~ MotherSize
results &lt;- lm(Size ~ MotherSize, data = datum)
summary(results)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Size ~ MotherSize, data = datum)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -5.032 -1.776  0.026  1.599  4.577 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 14.06203    0.22994   61.15   &lt;2e-16 ***
## MotherSize   1.62990    0.08401   19.40   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.27 on 98 degrees of freedom
## Multiple R-squared:  0.7934, Adjusted R-squared:  0.7913 
## F-statistic: 376.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Again, same problem here.</p>
<pre class="r"><code># Simple linear model of Size ~ MotherSize
results &lt;- lm(Size ~ Age + MotherSize, data = datum)
summary(results)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Size ~ Age + MotherSize, data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.98655 -1.17599 -0.02344  0.89925  3.01724 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   4.0338     0.7976   5.058 2.01e-06 ***
## Age           1.9646     0.1538  12.775  &lt; 2e-16 ***
## MotherSize   -0.1043     0.1452  -0.719    0.474    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.393 on 97 degrees of freedom
## Multiple R-squared:  0.923,  Adjusted R-squared:  0.9214 
## F-statistic: 581.4 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Truth for Age is 1.5, so this is closer – but the 95% CI around this
estimate of age still do not overlap truth. And MotherSize is not
significant, which it should be.</p>
<p>Last, the full model with all of the confounding variables:</p>
<pre class="r"><code># Simple linear model of Size ~ MotherSize
results &lt;- lm(Size ~ Age + Sex + MotherSize + FatherSize, data = datum)
summary(results)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Size ~ Age + Sex + MotherSize + FatherSize, data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.59760 -0.49615 -0.06507  0.56799  1.66825 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.04095    0.45985   8.788 6.43e-14 ***
## Age          1.49060    0.12975  11.488  &lt; 2e-16 ***
## SexMale      2.29112    0.16470  13.911  &lt; 2e-16 ***
## MotherSize   0.16766    0.08587   1.952  0.05382 .  
## FatherSize   0.24175    0.08668   2.789  0.00639 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8017 on 95 degrees of freedom
## Multiple R-squared:  0.975,  Adjusted R-squared:  0.974 
## F-statistic: 927.6 on 4 and 95 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We now get unbiased estimates that overlap with truth, and all
estimates are ~significant. MotherSize is narrowly insigificant, but
it’s CI overlap truth. And the p-value for the whole model is highly
significant.</p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<ul>
<li>If you add variables to a model, p-values should go down due to
swamping.</li>
<li>Multivariable models are generally good because they eliminate
issues related to swamping &amp; collinearity.</li>
</ul>
<p><br></p>
</div>
<div id="truth-1" class="section level2">
<h2>Truth</h2>
<pre class="r"><code>### Lecture 13: code to simulate collinear data and to demonstrate how failing to account for collinearity influences estimates and uncertainty

## Simulation Exercise 1

# Set the seed for reproducibility
set.seed(123)

# Number of simulations
s &lt;- 1000

# Empty vectors to save results from each simulation
simple_beta1 &lt;- numeric(s)
simple_se1 &lt;- numeric(s)
simple_p1 &lt;- numeric(s)
multi_beta1 &lt;- numeric(s)
multi_se1 &lt;- numeric(s)
multi_p1 &lt;- numeric(s)
r2 &lt;- numeric(s)
vif &lt;- numeric(s)

# x2 = x1 + error(0, z), where
# z can range from 0.5 (highly correlated to x1) to 20 (not correlated at ~all)
z_values &lt;- seq(0.5, 20, length.out = s)

# Loop through each simulation replicate to measure everything
for (i in 1:s){
  
  # Number of datapoints per simulation
  n &lt;- 100

  # x1
  x1 &lt;- runif(n, 0, 10) # Random, uniform variable; only simulated once

  # error for y; only simulated once
  error &lt;- rnorm(n, 0, 2)
  
  # X2 for each simulation
  x2 &lt;- x1 + rnorm(n, 0, z_values[i])
  
  # Y for each
  y &lt;- 10 + 3 * x1 + 3 * x2 + error

  # Simple model
  results1 &lt;- lm(y ~ x1)
  simple_beta1[i] &lt;- summary(results1)$coefficients[2,1]
  simple_se1[i] &lt;- summary(results1)$coefficients[2,2]
  simple_p1[i] &lt;- summary(results1)$coefficients[2,4]

  # Multi-variable model
  results2 &lt;- lm(y ~ x1 + x2)
  multi_beta1[i] &lt;- summary(results2)$coefficients[2,1]
  multi_se1[i] &lt;- summary(results2)$coefficients[2,2]
  multi_p1[i] &lt;- summary(results2)$coefficients[2,4]

  # R^2 between X-variables
  model_x12 &lt;- lm(x2 ~ x1)
  r2[i] &lt;- summary(model_x12)$r.squared
  
  # VIF for multi-variable model
  vif[i] &lt;- car::vif(results2)[&quot;x1&quot;]
}

## Simulation Exercise 2

# Set seed
set.seed(123)

# Number of simulations
s &lt;- 1000

# Empty vectors to save results from each simulation
simple_beta1 &lt;- numeric(s)
simple_se1 &lt;- numeric(s)
simple_p1 &lt;- numeric(s)
simple_beta2 &lt;- numeric(s)
multi_beta1 &lt;- numeric(s)
multi_beta2 &lt;- numeric(s)
multi_se1 &lt;- numeric(s)
multi_p1 &lt;- numeric(s)
r2 &lt;- numeric(s)
vif &lt;- numeric(s)

# x2 = x1 + error(0, z), where
# z can range from 0.5 (highly correlated to x1) to 20 (not correlated at ~all)
z_values &lt;- seq(0.5, 20, length.out = s)

# Loop through each simulation replicate to measure everything
for (i in 1:s){
  
  # Number of datapoints per simulation
  n &lt;- 100

  # x1
  x1 &lt;- runif(n, 0, 10) # Random, uniform variable; only simulated once

  # error for y; only simulated once
  error &lt;- rnorm(n, 0, 2)
  
  # X2 for each simulation
  x2 &lt;- x1 + rnorm(n, 0, z_values[i])
  
  # Y for each
  y &lt;- 10 + 3 * x1 + error

  # Simple model of X1
  results1 &lt;- lm(y ~ x1)
  simple_beta1[i] &lt;- summary(results1)$coefficients[2,1]
  simple_se1[i] &lt;- summary(results1)$coefficients[2,2]
  simple_p1[i] &lt;- summary(results1)$coefficients[2,4]
  
  # Simple model of X2
  results2 &lt;- lm(y ~ x2)
  simple_beta2[i] &lt;- summary(results2)$coefficients[2,1]

  # Multi-variable model
  results3 &lt;- lm(y ~ x1 + x2)
  multi_beta1[i] &lt;- summary(results3)$coefficients[2,1]
  multi_beta2[i] &lt;- summary(results3)$coefficients[3,1]
  multi_se1[i] &lt;- summary(results3)$coefficients[2,2]
  multi_p1[i] &lt;- summary(results3)$coefficients[2,4]

  # R^2 between X-variables
  model_x12 &lt;- lm(x2 ~ x1)
  r2[i] &lt;- summary(model_x12)$r.squared
  
  # VIF for multi-variable model
  vif[i] &lt;- car::vif(results3)[&quot;x1&quot;]
}

## Data for analysis in class
# Set the seed for reproducibility &amp; set graphing parameter
set.seed(123)

# This is similar to the Age, Sex, and Size data we simulated for last class.
# There is no collinearity between Age and Sex.

# X-variables
n &lt;- 50
Sex &lt;- c(rep(&quot;Female&quot;, n), rep(&quot;Male&quot;, n))
Age &lt;- runif(n * 2, 1, 10)
dummy &lt;- data.frame(model.matrix(~ Sex - 1))
colnames(dummy) &lt;- c(&quot;Female&quot;, &quot;Male&quot;)

# However, there are now two additional X-variables: MotherSize and FatherSize
# And both of those variables are a function of Age
MotherSize &lt;- rnorm(n * 2, Age - 5, 1)
FatherSize &lt;- rnorm(n * 2, Age, 1)

# All of these variables influence size, so they are all examples of confounding variables.

# Simulate error
Error &lt;- rnorm(n * 2, 0, 1.2)

# Predict Y
Response &lt;- 4 + 1.5 * Age + 2.5 * dummy$Male + 0.2 * MotherSize + 0.2 * FatherSize + Error

# Dataframe
datum &lt;- data.frame(Age = Age, Sex = Sex, Male = dummy$Male, MotherSize = MotherSize, FatherSize = FatherSize, Size = Response)

# Save the data
write.csv(datum, &quot;lecture_13_dataset1.csv&quot;, row.names = FALSE)</code></pre>
</div>
<div id="footnote" class="section level2">
<h2>Footnote</h2>
<p>TV-effect on mortality study: Wijndaele et al. 2011 <em>International
Journal of Epidemiology</em>.</p>
<p><a href="lecture_14.html">–go to next lecture–</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
