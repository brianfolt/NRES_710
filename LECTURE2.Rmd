---
title: "Sampling uncertainty"
author: "NRES 710"
date: "Fall 2022"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

```

```{r echo=FALSE}

##  NRES 710, Lecture 2                               
##    University of Nevada, Reno                       
##    Sampling distributions                                      

```

## Statistics: inference from a sample

![](ylf.jpg){width=40%}

Consider the following example:

Yellow-Legged frog example:    
*Population*: all yellow-legged frogs in ponds in the central Sierra Nevada     
*Parameter*: mean body size (SVL) of adult yellow-legged frogs in all ponds in the central Sierra Nevada    
*Sample*: As many frogs are captured and measured as possible in 10 ponds randomly sampled from the central Sierra Nevada   
*Statistic*: Sample mean    

Even more generally, the goal of statistics (even when not performing a test) is to infer something about a population from a sample. In the yellow-legged frog example below, there is a “true” mean body size of frogs in ponds in the central Sierra Nevada. We just don’t know what it is! After collecting the data, statistics might help use to say something about the mean body size in the population -- both about what we know AND what we don't know! 

All of statistics is about dealing with uncertainty. We don't need statistics if we have complete certainty!

We assume that our sample mean (and many other summary statistics derived from a sample) is representative of the population. How? Why? Because of the *Central Limit Theorem*. 

### The Central Limit Theorem

The Central Limit Theorem (CLT) says that if you have a sample with a reasonably large number of observations, and each observation is randomly sampled, then the sample mean will be similar to the actual population mean: the mean of ALL yellow-legged frogs in ponds in the central Sierra Nevada (how similar? that's where the t-distribution comes in- we'll get there). And as the sample size gets bigger, the sample mean will become more representative of the true mean (it will converge on the true mean as sample size approaches infinity). 

The concept of **regression to the mean** is a natural consequence of the Central Limit Theorem!

[In-class R demo: regression to the mean]

This is useful, but that's not all. 

The CLT is the magic wand of statistics. It does enormous amounts of work for us. Why?

The CLT also implies that the distribution of sample means collected from repeated sampling is approximately normally distributed even if the underlying data themselves are not normally distributed. 

Did you ever wonder why the normal (Gaussian) distribution is so common in statistics? It's because of the CLT- many summary statistics derived from a sample are expected to have a sampling distribution that is *approximately* normally distributed (based on the CLT)!

![](clt1.png)


### Example

How does this work? Let’s use the yellow-legged frog example. 

Let’s say that we could measure ALL the frogs in ALL the ponds in CA. What would that look like? 

Let’s simulate it using a log-normal distribution that is strongly right skewed (positively skewed), suggesting that there are a lot of frogs out there that are relatively small-bodied, and a few that are giants! NOTE: this is not necessarily biologically realistic, but it makes a point.  

First, let's set the population/parameter (the truth about which we hope to make inference but can never know in reality)

**Q** if we could measure the entire population of interest, do we even need statistics???


```{r}

#### ALL FROGS IN CA

allfrogs.bodysize <- rlnorm(10000,1.5,0.4)        # statistical 'population'
hist(allfrogs.bodysize,main="",xlab="SVL (mm)")   # plot out histogram

truemean_SVL <- mean(allfrogs.bodysize)           # the 'parameter'
truemean_SVL 

```


Now let's take a sample!

```{r}

mysample <- sample(allfrogs.bodysize,10)    # take sample of size 10 (10 frogs measured)
mean(mysample)   # compute the sample mean


```


And another, this time with n=20

```{r}

mysample <- sample(allfrogs.bodysize,20)    # take sample of size 20 (20 frogs measured)
mean(mysample)   # compute the sample mean


```

Since sampling is random, sampling will produce a different result every time.

To get a better picture of the sampling variance, let's sample many times!



```{r}

lotsofsamples <- list()

for(s in 1:5000){
  lotsofsamples[[paste0("sample",s)]] <- sample(allfrogs.bodysize,30)    # take sample of size 30 (20 frogs measured)
}

lotsofsamples$sample1
lotsofsamples$sample99
lotsofsamples$sample732

```

Now we can compute the sample means and the sampling variance for the summary statistic (mean body size)

```{r}

samplemeans <- sapply(lotsofsamples,mean)

hist(samplemeans,xlab="mean body size (n=30)")

```


Interesting- does this look skewed to you? Doesn't it look like a normal distribution??

It's the CLT at work!!


One way to think about this is just that there are more ways of getting a sample mean near the real mean than to get one far away from the sample mean. It doesn't matter what the raw distribution is. Of all the samples you could get, there are very few that are all at one end of the distribution. There are a lot more possible random samples that span the full distribution of values, from low to high. Take the average af all those values, low and high, and you get something in the middle. The normal distribution is humped right in the middle, because of the tendency for low and high observations to 'average out' within a sample. 

Craps example (rolling a pair of dice). There are many more ways to get 7 than to get 2!   

This works with coin flips too!

Here's the sampling distribution for the number of heads out of a single coin flip:

```{r}

hist(rbinom(10000,1,.5),xlab="N heads out of 1")

```


Now let's build up sample size and see how the sampling distribution changes. 


```{r}
par(mfrow=c(3,2))
for(i in seq(2,12,2)){
   hist(rbinom(10000,i,.5),main=paste0("sample size = ",i),xlab=sprintf("N heads out of %s",i)) 
}

```

And with really big sample size:

```{r}

hist(rbinom(10000,1000,.5),xlab="N heads out of 1")

```


## Sampling distributions

Pretty much all of classical null hypothesis testing (NHT) works like this: 

1. We compute a summary statistic from our data. This statistic is usually accompanied by a well-described *sampling distribution* that describes the distribution of summary statistics you would expect to observe under the null hypothesis. 
2. Using this known sampling distribution, we inquire whether or not our summary statistic (computed from our sample data) could have been a result of random sampling error in the null universe. 
3. If not (if $p\le\alpha$) then we are sufficiently surprised to reject the null hypothesis. Otherwise we fail to reject the null...

Sampling distributions that are commonly used include:

- t distribution (sampling distribution for the t statistic under the null hypothesis)
- Chi-squared distribution (sampling distribution for the Chi-squared statistic under the null hypothesis)
- F distribution (sampling distribution for the F statistic under the null hypothesis- used in ANOVA and regression)


## Replication and pseudoreplication

Let’s explore replication, and what it truly means. 

Make sure you read this [highly influential monograph on 'Pseudoreplication' by Stuart Hurlburt](hurlburt.pdf).  

And here's [one more paper for good measure, by Davies and Gray (2015)](daviesandgray.pdf): this is a counter to the Hurlburt article.

Clearly one of the main take-aways is: science is messy!!!

Why does science require replication?

In general, the scientific project is to discover generalizable truths. 

Q: How do we know if a result is true, and not a result of just random noise?     
    A: We use a large enough sample size so that we can convince ourselves that random noise (sampling variation) could not cause the result!
 
Q: How do we know if a result is general, and not a result of just some localized or specific phenomenon?     
    A: We draw our sample randomly from the entire population so that our sample is truly representative!
    
### Find the pseudoreplication!

*Population*: All little brown bats across the USA    
*Parameter*: Infection rate of white nose syndrome infection in cave hibernacula    
*Sample*: Two cave hibernacula in New York state.    
*Statistic*: Infection rate among sampled bats    

*Population*: All humans    
*Parameter*: Effectiveness of a coronavirus vaccine    
*Sample*: 10,000 humans sampled in Sweden    
*Statistic*: Infection rate in Uppsala (control) vs infection rate in Helsingborg (treatment)    

*Population*: all yellow-legged frogs in ponds in the central Sierra Nevada     
*Parameter*: mean body size (SVL) of adult yellow-legged frogs in all ponds in the central Sierra Nevada    
*Sample*: 3,000 frogs sampled from a single pond in the central Sierra Nevada    
*Statistic*: Sample mean


The takeaway: you can only convince yourself of the generality of a result if the sample is representative of the population. In experimental design, one way to try to ensure generality is to sample randomly from the population of interest. 

One part of the scientific endeavor is to poke at other people's research to see if it stands up to scrutiny. Even more so, we poke at in our own research. If we can convince ourselves of the truth of our results and conclusions, only then can we feel comfortable sharing the results with the scientific community. That's not out of meanness or masochism, it's out of a search for truth and generality!  

But the search for the truth is messy. In environmental science, we pseudoreplicate all the time - by necessity. For practical reasons our observations are not always completely independent from one another. 

Are we ever truly replicating perfectly and sampling sufficiently from our generalized target of inference? In many cases we are not. But that shouldn't stop us from trying to find truth- we just need to proceed with caution! 

## Assumption: all data points are independent!

Nearly all of the classical statistical analyses and tests we will go over in this class make a very important assumption -- that all data points are independent samples drawn from the population of interest. Unfortunately, truly independent data points are far from the norm in ecology and environmental science! When data points are not independent, the information content of the sample (relative to the population of interest) is reduced. Does this make sense? When we treat non-independent data points as independent we are committing pseudoreplication!

Pseudoreplication, when left 'untreated' (i.e., subjected to statistical analyses that account for sources of non-independence), can result in using statistical methods inappropriately -- mistakenly assuming that you have more information than you actually have. 

### Demo: non-independence and sampling distributions

Let's see what happens to sampling distributions with and without pseudoreplication. We are interested in testing whether mountain yellow-legged frogs in the cascade range tend to be heavier than frogs in the sierra nevada (which we know to be 1.5 g from previous studies). We take a sample of 100 individuals: 50 from each of 2 ponds. 

We assume that frogs from the two mountain ranges are the same size on average (null hypothesis is true) and that the distribution of sizes is Gaussian. However, individuals from the same pond tend to be more similar to one another than individuals sampled from other ponds. Here is the scenario: 

```{r}

# pseudoreplication demonstration

meansize.allfrogs <- 1.5    # population mean 
sdsize.allfrogs <- 0.5     # population sd
sdsize.amongpond <- 0.44   # standard deviation among ponds 

nponds <- 5000   # total number of ponds in the population
nfrogs.perpond <- 1000    # 1000 frogs in each pond


```

So we can now generate the mean sizes for all ponds in the population and all frogs within each pond 


```{r}

pondmeans <- rnorm(nponds,meansize.allfrogs,sdsize.amongpond)
 # hist(pondmeans)
allfrogs <- sapply(pondmeans, function(t) rnorm(nfrogs.perpond,t,sqrt(sdsize.allfrogs^2-sdsize.amongpond^2)) )
rownames(allfrogs) <- paste0("frog",1:(nfrogs.perpond))
colnames(allfrogs) <- paste0("pond",1:nponds)

  # confirm that population mean and standard deviation are as specified
sd(allfrogs)
mean(allfrogs)

```

Okay, now we have a population of frogs. Now we need to sample from this population!! Remember that for practical reasons we are only able to sample from 2 ponds. 

```{r}

nponds.sampled <- 2
nsamp.perpond <- 50 

ponds.sampled <- sample(1:nponds,nponds.sampled)
frogs.sampled <- replicate(nponds.sampled,sample(1:nfrogs.perpond,nsamp.perpond))

thissamp <- sapply(1:nponds.sampled,function(t) allfrogs[frogs.sampled[,t],ponds.sampled[t]])
rownames(thissamp) <- paste0("frog",1:(nsamp.perpond))
colnames(thissamp) <- paste0("pond",1:nponds.sampled)

head(thissamp)

```

Okay, now that we have collected our sample, let's run a t-test to see if the mean in our sample is different from 1.5

```{r}

test <- t.test(as.vector(thissamp),mu=1.5,alternative="greater")
test

```

Okay, we have now collected one sample from this hypothetical scenario. To generate a sampling distribution we need to generate many more samples. Let's do that now! Here we will run both a sampling scheme where we only sample 2 ponds and another sampling scheme where we sample 100 frogs randomly from the entire population. 

```{r}

means <- numeric(1000)
means.ind <- numeric(1000)

ttest <- numeric(1000)
ttest.ind <- numeric(1000)

for(scenario in 1:1000){
  ponds.sampled <- sample(1:nponds,nponds.sampled)
  frogs.sampled <- replicate(nponds.sampled,sample(1:nfrogs.perpond,nsamp.perpond))
  
  thissamp <- sapply(1:nponds.sampled,function(t) allfrogs[frogs.sampled[,t],ponds.sampled[t]])
  thissamp.ind <- matrix(sample(allfrogs,nsamp.perpond*nponds.sampled),ncol=nponds.sampled)
  
  means[scenario] <- mean(thissamp)
  means.ind[scenario] <- mean(thissamp.ind)
  
  test <- t.test(as.vector(thissamp),mu=1.5,alternative="greater")
  test.ind <- t.test(as.vector(thissamp.ind),mu=1.5,alternative="greater")
  
  ttest[scenario] <- test$p.value
  ttest.ind[scenario] <- test.ind$p.value
  
}



```

Let's look at the sampling distribution of the mean difference between male and female body mass in the null universe. Here we compare the sampling distribution for the pseudoreplicated design vs the sampling distribution for the case with 100 true independent replicates. 

```{r}

layout(matrix(1:2,nrow=1))
hist(means,xlim=c(0,3))
hist(means.ind,xlim=c(0,3))

length(which(ttest<0.05))/1000

length(which(ttest.ind<0.05))/1000

```

In the pseudoreplicated design, we have a ca. 40% chance of incorrectly rejecting the null hypothesis with nominal alpha = 0.05! With independent samples the rate is around 0.05 as it should be. 

Try running this script with less egregious pseudoreplication- e.g., with 5 ponds with 20 frogs each. How much does this improve the result??

NOTE: in the above example, the only reason pseudoreplication is a problem is that we are incorrectly assuming that the sampling distribution of our test statistic resembles the histogram on the right (independent samples). If we used the sampling distribution on the left (which correctly accounts for pseudoreplication) there would be no issue -- except that we would need to see much more extreme results in order to reject our null hypothesis! 


## Summary statistics (calculated from sample data)

We often summarize our samples by their centers (e.g., average) and possibly their spread (dispersion)    

### "Center" statistics: means, medians, geometric mean      

Sample Mean (arithmetic mean) = sum of all sampled values divided by the sample size     
Sample Median (midway point) = 50% quantile. Order the values and select the value at the center.          
Sample Geometric mean: product of numbers taken to the nth root. For two numbers, 3 and 4, you’d have the sq root of 3*4 = 3.46         

### Data spread, or dispersion: 

Sample Standard Deviation – sigma ($\sigma$) for population standard deviation, *s* for the sample standard deviation.        

Standard deviations and variances are calculated differently depending on whether we are computing these quantities for an entire population of interest vs a sample drawn from a larger population!     
The variance of a sample ($s^2$) or the variance of the population parameter ($\sigma^2$) can be computed as the square of the standard deviation. 

The variance represents the average squared difference from the mean. The standard deviation represents the square root of the average squared difference from the mean.

Standard deviation is much more commonly reported than variance because it is in the same units/scale as the original measurements.   

Coefficient of variation (cv) is the standard deviation represented as a fraction of the mean. The cv only has meaning for *ratio data*.        

#### Std deviation calculation example

For a population:  $\sigma = \sqrt{\sum_{n=1}^{i}{\frac{(x_i-\mu)^2}{N}}}$         
For a sample (estimating population variance from a sample):  $s = \sqrt{\sum_{n=1}^{i}{\frac{(x_i-\bar{x})^2}{(N-1)}}}$

For example: compute the variance of 5 numbers: 4, 3, 5, 5, 2

$\mu = (4+3+5+5+2)/5 = 3.8$

(4-3.8)^2 = 0.2^2 = 0.04       
(3-3.8)^2  = 0.64       
(5-3.8)^2 = 1.44     
(5-3.8)^2 = 1.44     
(2-3.8)^2 = 3.24     

Sum these  = 6.8     
Divide by 5  = $\sigma$ = 6.8/5 = 1.36        

For sample sd (summary statistic for population variance from a sample):    

$\bar{x}  = (4+3+5+5+2)/5 = 3.8$   

(4-3.8)^2 = 0.2^2 = 0.04       
(3-3.8)^2  = 0.64       
(5-3.8)^2 = 1.44     
(5-3.8)^2 = 1.44     
(2-3.8)^2 = 3.24      

Sum these  = 6.8      
Divide by 4  = $s$ = 6.8/4 = 1.7     


So the population sd is 1.36 whereas the sample sd is 1.7.    


#### Aside: degrees of freedom

OK, so why the different estimate of dispersion for population vs. sample? 

Which is larger? Which are we less confident in?   

This has to do with a concept called *degrees of freedom*. 

Sigma is known. Since you have the entire population measured, you can compute a measure of dispersion for the population and that measure is perfect. It is not an estimate, it is a perfect point value that is known with certainty and that represents the square root of the average squared difference from the mean. No bias, perfect precision.     

The sample standard deviation $s$, on the other hand, is an imperfect estimate of dispersion for a much larger population! 

**Q** would you expect the sample standard deviation using the first formula (pop stdev) to be biased? Why or why not?? In what direction would it be biased? 

NOTE: the formula for sample stdev *uses the sample mean*. Therefore, the sample data have already been used once to compute the standard deviation. If the true population mean was different than the sample mean, what would happen to the estimate of stdev if we replaced the sample mean with the true mean? Is it possible that the sample mean is not equal to the true mean? (of course it is!!)    

NOTE ALSO: if you know that four of the five sampled values are 4, 3, 5, and 2 AND we know that the sample mean is 3.8, we know that the final sampled data point MUST be 5. There is no *freedom* there- the data point has to be 5. Therefore, even though we have 5 data points, we have only 4 *degrees of freedom* since the sample mean is included in the formula for the estimate for sample stdev. That is, we have four independent pieces of information that we can use for computing standard deviation (we 'spent' one degree of freedom already to compute the sample mean!).  

By dividing the sum of squared deviations from the sample mean by 4 instead of 5, we are *unbiasing* the estimate of dispersion so that it is a better estimate of the population standard deviation.

## Sampling variance (variance among hypothetical sample summary statistics)

NOTE: sampling variance is NOT the same thing as the variance of a sample!

Review: we collect a random sample from a population. We want to know something about the population, but any summary statistic we compute from the sample (e.g., mean, median, variance, stdev, whatever!) will be an imperfect estimate of the true population parameter. So what do we do? How can we truly make inference about the population??

First, we need to know something about the sampling variance. We know the summary statistic will be different for every sample we collect, but *how different, really???*. How surprised should be be to get a sample statistic as or more extreme than a particular observation under the null hypothesis? What does the sample really tell us about the population of interest??

Statisticians have used probability calculus to work out the sampling distributions (often called sampling variance) for some common summary statistics. For example, the sample mean (super common summary statistic) has a theoretical sampling distribution that follows the t-distribution (N-1 degrees of freedom for a single sample) and is centered on a hypothetical value (e.g., zero under many null hypotheses, or the sample mean when you are generating a confidence interval) with dispersion described by the standard error of the mean. Let's dissect this one!  

### Standard error of the mean

Standard error of the mean = sample std deviation divided by the square root of the sample size

$se = \frac{s}{\sqrt{N}}$

The standard error of the mean is used to help us describe the sampling distribution for the sample mean (the expected dispersion of sample means if you collected thousands of new samples and computed the mean).

```{r}

#######
# Sampling distribution: the sample mean

mysample <- c(4.1,3.5,3.7,6.6,8.0,5.4,7.3,4.4)
mysample
n <- length(mysample)    # sample size
sample.mean <- mean(mysample)  # sample mean
sample.stdev <- sd(mysample)   # sample standard deviation (r uses denominator of n-1 by default!)
std.error <- sample.stdev/sqrt(n) 

std.error 

```

Now we have all the information we need to compute the sampling distribution for the sample mean. 

Our sample mean is 5.375. But if we collected different samples of size n=8, we would get different values - even if the true population mean was 5.375. What does this distribution of values look like?

```{r}

sampdist <- function(x){dt((x-sample.mean)/std.error,n-1)}
curve(sampdist,0,11,ylab="probability density",xlab="value",main="sampling distribution for the sample mean!")
abline(v=sample.mean,col="green",lwd=3)
confint <- c(sample.mean+std.error*qt(0.025,n-1),sample.mean+std.error*qt(0.975,n-1))
abline(v=confint,col="blue",lty=2)

```

The vertical blue lines indicate the *confidence interval* around the mean with the *confidence level* set at 95%. We will talk about confidence intervals a lot more, but just know that about 95% of such confidence intervals generated from random samples should include the true mean. The confidence interval helps us visualize what might happen if we repeated our sampling over and over and over- how might our summary statistic change?

Note the use of the *t distribution* in the above code block. The t distribution here represents the sampling variation that you would expect to observe around the sample mean (or an arbitrary hypothesized value) if you collected many many samples of the same size as yours (see more details below) from an unknown population. The t distribution is expressed in units of standard errors (number of standard errors above or below your hypothetical true mean). 

Just to cement this concept, let's compare the distribution above (based on a t distribution) with a brute-force simulation method!



```{r}

#######
# Sampling distribution: the sample mean #2 (brute force simulation version)

mysample <- c(4.1,1.5,3.7,6.6,8.0,4.5,5.3,4.4)
mysample
n <- length(mysample)    # sample size
sample.mean <- mean(mysample)  # sample mean
sample.stdev <- sd(mysample)   # sample standard deviation (r uses denominator of n-1 by default!)

simulated.samples <- list()
for(s in 1:10000){
  sd1 <- sqrt(sum((sample(mysample,length(mysample)-1,replace = T)-sample.mean)^2)/(length(mysample)-2))  # account for unknown standard deviation
  simulated.samples[[paste0("sample ",s)]] <- rnorm(n,sample.mean,sd1)
}
sampling.distribution <- sapply(simulated.samples,mean)

plot(density(sampling.distribution),xlim=c(0,11),ylab="probability density",xlab="value",main="sampling distribution for the sample mean!",lwd=2)    # plot the brute-force sampling distribution
hist(sampling.distribution,add=T,freq=F)
par(new=T)
curve(sampdist,0,11,xlim=c(0,11),xaxt="n",yaxt="n",xlab="",ylab="",col="red",lwd=2)  # official sampling distribution
abline(v=sample.mean,col="green",lwd=3)



```



Not a bad match right? Obviously it's easier and faster (and more accurate) to use the t distribution to approximate the sampling distribution, but I hope this helps to cement the concept!! 

NOTE: the t distribution accounts for uncertainty about the true population variance as well as the true population mean, which is why I did not just use the sample variance in the code block above (if I had, you would see that the t distribution had 'heavier tails' than the brute force distribution because it accounts for uncertainty in the sample variance and the sample mean) 




## Probability distributions- the basics (and how to use them in R)

### Discrete vs. continuous
In *discrete distributions*, each outcome (value that could be sampled under this probability distribution) has a specific *probability mass* (like the probability of flipping a coin 10 times and getting 4 heads). For example, let's consider the Poisson distribution:  

```{r}

# Probability distributions ---------------------

# Discrete probability distributions 

mean <- 5
rpois(10,mean)    # note: the random numbers sampled from this distribution have no decimal component

             # plot discrete probabilities of getting particular outcomes!
xvals <- seq(0,15,1)
probs <- dpois(xvals,lambda=mean)
names(probs) <- xvals
               
barplot(probs,ylab="Probability Mass",main="Poisson distribution (discrete)")

barplot(cumsum(probs),ylab="Cumulative Probability",main="Poisson distribution (discrete)")   # cumulative distribution

sum(probs)   # just to make sure it sums to 1!  Does it???

```

In *continuous distributions*, each possible value/quantity that could be randomly sampled is associated with a *probability density*, $f(x)$, not probability mass $Prob(x)$. This is because the probability of getting any particular value in a continuous distribution is effectively zero. This arises from the problem of precision. The sum of the probability distribution must be 1 (there is only 100% of probability to go around). In a continuous distribution, there are an infinite number of possible values of x. So any individual probability is always divided by infinity, which makes it zero. Therefore we have to talk about probability density, unless we want to specify a particular range of values – we can’t calculate $Prob(x = 5)$, but we can calculate $Prob(4 < x < 6)$ or $Prob(x > 5)$. The probability density is defined as the probability of getting a value within an infinitesimally small range of a particular value, divided by that infinitesimally small interval. No worries if you don't understand that - you can just think of probability density as the relative likelihood of sampling one value versus another. Let's consider the beta distribution: 

```{r}

# continuous distributions

shape1 = 0.5
shape2 = 0.5

rbeta(10,shape1,shape2)   # generate 10 random numbers from a continuous distribution

curve(dbeta(x,shape1,shape2),ylab="probability density",xlab="possibilities")   # probability density function (PDF)

curve(pbeta(x,shape1,shape2),ylab="cumulative probability",xlab="possibilities")   # cumulative distribution

integrate(f=dbeta,lower=0,upper=1,shape1=shape1,shape2=shape2)    # just to make sure it integrates to 1!!

```


### Parameters and parametric statistics

**Parameters** are the arguments used to describe probability distributions - they describe the exact shape and location of the distribution. Different distribution families are associated with different parameters. For example, the normal distribution is described by 2 parameters: mean and stdev. The Poisson distribution has only one parameter (the mean). The binomial distribution is described by 2 parameters: size (number of trials) and prob (success probability for each trial).  

*Parametric statistics* require assuming certain things about distributions & parameters, while *nonparametric statistics* do not require these assumptions. NOTE: nonparametric statistics still usually make the assumption that observations are independent!

**Q** Given the CLT makes many classical tests robust to non-normal data distributions, why do we need non-parametric statistics? 

### Probability distributions in R

#### Random number generators

**Random number generators** are functions for generating random values from a specified probability distribution (e.g., 'rnorm', 'rpois', 'rt')

```{r}

# random number generators

rnorm(10)    # generate 10 random numbers from a standard normal distribution
rnorm(5,25,5)  # generate 5 random numbers from a normal distribution with mean=25 and sd=5
rpois(8,18)  # generate 8 random numbers from a poisson distribution with mean=18

```

#### Probability density functions

Continuous distributions are associated with **PDFs**, or probability density functions (e.g., 'dnorm','dt','dgamma'). These functions give you the probability density (relative probability) of any particular value/quantity that could be randomly sampled under this distribution.  

```{r}

## probability density function example 

curve(dt(x,8),-4,4,xlab="possibilities",ylab='relative probability (prob density)')

```
#### Probability mass functions

For discrete distributions, **PMFs** - Probability mass functions (e.g., 'dpois','dbinom') -- give you the exact probability of obtaining any particular value that could be sampled under this distribution. 

```{r}

## probability mass function example 

x <- barplot(sapply(0:10,function(t) dpois(t,2)),xlab="possibilities",ylab='probability')
axis(1,at=x,labels=0:10)

```

#### Cumulative distribution function
For continuous AND discrete probability distributions, **CDFs** - Cumulative distribution functions (e.g., 'pnorm','pt','pchisq','pbinom','ppois') give you the probability of obtaining a value less than or equal to any particular value that could be sampled under the distribution.

```{r}

## cumulative distribution function  

    # for continuous distribution
curve(pt(x,df=8),-4,4,xlab="possibilities",ylab='cumulative probability')

    # for discrete distribution
x <- barplot(sapply(0:10,function(t) ppois(t,2)),xlab="possibilities",ylab='cumulative probability')
axis(1,at=x,labels=0:10)

```

#### Quantile functions

**Quantile functions** - inverse of CDF- give you the values below which a specific percent of random samples should fall (e.g., 'qnorm','qt','qpois','qchisq'). The quantile function is the inverse of the cumulative distribution function.

```{r}

## quantile function  

    # for continuous distribution
curve(qt(x,df=8),0,1,xlab="cumulative probability",ylab='quantile')

    # for discrete distribution
curve(qpois(x,4),0,1,xlab="cumulative probability",ylab='quantile')

```

### Moments

**Moments** are important descriptors of a distribution. The collection of all the moments (of all orders, from 0 to infinity) uniquely determines the shape of the distribution.  

* The zeroth central moment ($\int \left ( x-\mu  \right )^{0}Prob(x)\partial x$) is the total probability (i.e. one),  
* The first central moment ($\int \left ( x-\mu  \right )^{1}Prob(x)\partial x$) is $\mu - \mu = 0$.   
* The second central moment ($\int \left ( x-\mu  \right )^{2}Prob(x)\partial x$) is the variance.    
* The third central moment ($\int \left ( \left (x-\mu   \right )/\sigma  \right )^{3}Prob(x)\partial x$) is the skewness.     
* The fourth central moment is the kurtosis. 

### Some common probability distributions  
Here are some common probability distributions. Pay particular attention to the type of *process* described by each distribution. The key to using these distributions to represent random variables is to figure out which statistical process best matches the ecological process you’re studying, then use that distribution. e.g., am I counting independent, random events occurring in a fixed window of time or space (like sampling barnacles in quadrats on an intertidal bench)? Then the distribution of their occurrence is likely to follow a Poisson or Negative Binomial distribution.


#### Binomial
```{r}

# Binomial

size <- 10
prob <- 0.3
rbinom(10,size,prob)

xvals <- seq(0,size,1)
probs <- dbinom(xvals,size,prob)
names(probs) <- xvals
               
barplot(probs,ylab="Probability",main="Binomial distribution")

barplot(cumsum(probs),ylab="Cumulative Probability",main="Binomial distribution")   # cumulative distribution

sum(probs)   # just to make sure it sums to 1!  Does it???

```


#### Normal
```{r}

# Gaussian (normal)

mean = 7.1
stdev = 1.9

rnorm(10,mean,stdev)

curve(dnorm(x,mean,stdev),0,15)   # probability density

curve(pnorm(x,mean,stdev),0,15)   # cumulative distribution

integrate(f=dnorm,lower=-Inf,upper=Inf,mean=mean,sd=stdev)    # just to make sure it integrates to 1!!


```


#### t distribution

```{r}

# t distribution

df = 6

rt(10,df)     # random numbers from the t distribution

curve(dt(x,df),-4,4)   # probability density

curve(pt(x,df),-4,4)   # cumulative distribution

integrate(f=dt,lower=-Inf,upper=Inf,df=df)    # just to make sure it integrates to 1!!


```



#### Chi-squared distribution


```{r}

# Chi-squared distribution

df = 6

rchisq(10,df)     # random numbers from the chi squared distribution

curve(dchisq(x,df),0,15)   # probability density

curve(pchisq(x,df),0,15)   # cumulative distribution

integrate(f=dchisq,lower=0,upper=Inf,df=df)    # just to make sure it integrates to 1!!


```



#### Exercise (optional):

Visualize (in R) the following distributions as above: Gamma, Exponential, Lognormal, Negative Binomial.




[--go to next lecture--](LECTURE3.html) 

















