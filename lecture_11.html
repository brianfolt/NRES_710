<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 710" />


<title>Analyze as Continuous or Categorical…?</title>

<script src="site_libs/header-attrs-2.27/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 710</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Syllabus</a>
</li>
<li>
  <a href="schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="lecture_1.html">Syllabus and the Purpose of Statistics</a>
    </li>
    <li>
      <a href="lecture_2.html">P-value Discussion and Intro to R</a>
    </li>
    <li>
      <a href="lecture_3.html">Sampling Uncertainty</a>
    </li>
    <li>
      <a href="lecture_4.html">Linear Regression</a>
    </li>
    <li>
      <a href="lecture_5.html">Linear Regression - results</a>
    </li>
    <li>
      <a href="lecture_6.html">Linear Regression - assumptions</a>
    </li>
    <li>
      <a href="lecture_7.html">Linear Regression - predictions</a>
    </li>
    <li>
      <a href="lecture_8.html">Analysis of Categorical Data - 2 groups</a>
    </li>
    <li>
      <a href="lecture_9.html">Analysis of Categorical Data - &gt;2 groups</a>
    </li>
    <li>
      <a href="lecture_10.html">Analysis of Categorical Data - posthoc tests</a>
    </li>
    <li>
      <a href="lecture_11.html">Analysis with Continuous or Categorical X?</a>
    </li>
    <li>
      <a href="lecture_12.html">Multi-variable Modeling</a>
    </li>
    <li>
      <a href="lecture_13.html">Multi-variable Modeling - collinearity</a>
    </li>
    <li>
      <a href="lecture_14.html">Multi-variable Modeling - interactions</a>
    </li>
    <li>
      <a href="lecture_15.html">Generalized Linear Models</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="exercise_1.html">Exercise 1 - Data Summary Functions</a>
    </li>
    <li>
      <a href="exercise_2.html">Exercise 2 - Linear Regression</a>
    </li>
    <li>
      <a href="exercise_3.html">Exercise 3 - Linear Regression (cont.)</a>
    </li>
    <li>
      <a href="exercise_4.html">Exercise 4 - Analysis of Categorical Data</a>
    </li>
    <li>
      <a href="exercise_5.html">Exercise 5 - Multi-variable Analysis</a>
    </li>
    <li>
      <a href="exercise_6.html">Exercise 6 - TBD</a>
    </li>
    <li>
      <a href="exercise_7.html">Exercise 7 - TBD</a>
    </li>
    <li>
      <a href="exercise_8.html">Exercise 8 - TBD</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    More Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Links.html">Links</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Analyze as Continuous or Categorical…?</h1>
<h4 class="author">NRES 710</h4>
<h4 class="date">Last compiled: 2024-08-20</h4>

</div>


<div id="review" class="section level2">
<h2>Review</h2>
<p>Up to this point we have been working with the linear model and we
have explored situations where the X-variable is continuous or
categorical. We can analyze both of those instances using ‘lm()’, and
the only difference is how we report the results.</p>
<ul>
<li>“For each unit increase, we observed a […] change…”</li>
<li>“We found that group 1 was [beta] larger than group 2…”</li>
</ul>
<p>Today we are going to talk about a special case that can be useful in
statistics: when you have an X-variable that can be treated as either
continuous or categorical.</p>
</div>
<div id="continuous-or-categorical-x" class="section level2">
<h2>Continuous or categorical X</h2>
<p><strong>X-variable – categorical <em>or</em> continuous</strong>.
Today we will discuss an example where:</p>
<ul>
<li><strong>Y-variable – biomass (continuous)</strong></li>
<li><strong>X-variable – number of cows (continuous or
categorical)</strong></li>
</ul>
<p>This is a hypothetical example where we have a bunch of different
fields where we are measuring biomass at the end of the experiment. At
the start of the experiment, we put a bunch of <strong>American
Bison</strong> (<em>Bison bison</em>) out into the field – any given
field could receive between 1–5 bison. Our research question of interest
is what is the effect of a large native herbivore, bison, on ecosystem
productivity, which we measure with biomass?</p>
<p>Our X-variable is a <strong>number</strong>, so it could be treated
as <strong>continuous</strong>. But also because we have multiple fields
with that same number of cows in it, we could treat it as
<strong>categorical</strong>.</p>
<p>To illustrate this, let’s consider a few examples:</p>
<p><img src="lecture_11_files/figure-html/example1-1.png" width="864" /></p>
<p>Here our X-variable is definitely continuous, and the correct and
most robust approach to testing for the effect of sunlight on the
response variable is using linear regression (<strong>left
panel</strong>).</p>
<p>However, some people who haven’t taken this class might be more
comfortable with an ANOVA-style approach to analysis, and they may be
tempted to <em>bin</em> their X-data to place it into ~3 groups to
facilitate ANOVA (<strong>right panel</strong>). Split the continuous
measure into ‘low’, ‘medium’ and ‘high’ sunlight categories. This causes
you to lose a lot of information! And then it creates a more complicated
and less clear analysis that provides no real advantage.</p>
<p>In this case, you should always treat this as continuous.</p>
<p>Alternatively, another example is where your X-variable is a ‘text’
variable, such as where you have categorical variable with four factors:
urban, forest, ag, wetland.</p>
<p><img src="lecture_11_files/figure-html/example2-1.png" width="864" /></p>
<p>For this example, the variable is words that cannot be translated
into a numeric scale. You have to treat this as a categorical
variable.</p>
<div id="replicated-regression" class="section level3">
<h3>Replicated regression</h3>
<p>Let’s revisit the bison example I mentioned previously. We want to
know: what is the effect of bison on ecosystem productivity?</p>
<p><img src="pic_bison.png" style="width:50.0%" /></p>
<p>Photo: Ted Naninga (iNaturalist)</p>
<p>For this example, we have a bunch of different fields where we put a
bunch of American Bison (<em>Bison bison</em>) out into the fields, and
any individual field could receive between 1–5 bison. Our data might
look something like this:</p>
<p><img src="lecture_11_files/figure-html/stripplot-1.png" width="672" /></p>
<p>We measure the biomass in replicate areas with different numbers of
bison. Some call this ‘<strong>replicated regression</strong>’ (e.g.,
Cottingham et al. 2008). We could analyze this two ways:</p>
<ol style="list-style-type: decimal">
<li>As a continuous X-variable</li>
<li>As a categorical X-variable, with:</li>
</ol>
<p>Technically, this is not truly continuous, as you cannot have 1.5
bison in an area. Another X-variable, like fertilizer, could be
continuous in this way. But if we ignore that technicality, these data
look continuous and we could easily fit a regression line to the
data.</p>
<p>Alternatively, we have a bunch of samples collected at each number of
the X-variable. We could very easily treat this as a categorical
variable – and no binning would be necessary to do so; it’s already set
up as a categorical variable. So the second way to analyze these data
would be as a categorical variable.</p>
<p>So with these two options in mind, how should we handle this
variable? Continuous or categorical?</p>
<p>What would our linear model look like in both of these instances?</p>
<p><strong><span class="math inline">\(Y = \beta_0 + \beta_1 Bison +
\epsilon \sim N(0, \sigma)\)</span></strong></p>
<p>That’s it! Our beta is our slope; with each one bison increase, the
slope tells us how much more biomass we would get.</p>
<p>What if we treat the X as categorical?</p>
<p><strong><span class="math inline">\(Y = \beta_0 + \beta_1 Bison1 +
\beta_2 Bison2 + \beta_3 Bison3 + \beta_4 Bison4 + \epsilon \sim N(0,
\sigma)\)</span></strong></p>
<p>In this case, our betas are the differences between groups. Beta1 is
the difference between 0 and 1 bison, beta2 is the difference between 0
and 2 bison, so on so forth. If we want to know the differences between
other pairs, we have to change the reference.</p>
<p><strong>Q:</strong> So let’s pose the question to you all: which of
these approaches do you want to do? And why?</p>
<p>Truthfully, there is no right or wrong answer. You can do either
one.</p>
<p>But let’s draw up a list on the board weighing the pros and cons of
both options.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Continuous X
</th>
<th style="text-align:left;">
Categorical X
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<strong>Simple model - 1 beta</strong>
</td>
<td style="text-align:left;">
Complicated - 3 betas
</td>
</tr>
<tr>
<td style="text-align:left;">
<strong>No worries about inflating Type I error</strong>
</td>
<td style="text-align:left;">
Post-hoc test
</td>
</tr>
<tr>
<td style="text-align:left;">
<strong>1 results sentence</strong>
</td>
<td style="text-align:left;">
10 results sentences
</td>
</tr>
<tr>
<td style="text-align:left;">
<strong>Interpolation/extrapolation</strong>
</td>
<td style="text-align:left;">
No predictions
</td>
</tr>
<tr>
<td style="text-align:left;">
Assumption of linearity
</td>
<td style="text-align:left;">
<strong>No assumption of linearity</strong>
</td>
</tr>
<tr>
<td style="text-align:left;">
<strong>More power</strong>*
</td>
<td style="text-align:left;">
Less power*
</td>
</tr>
</tbody>
</table>
<p>This last reason is the <strong>most important reason</strong> why
you might want to use a continuous X variable. Since it only has 1 beta
(effect), it will have <strong>more power</strong>! More power means you
are more likely to have lower P-values and thus correctly reject the
null hypothesis, when appropriate.</p>
<p>Why is that?</p>
<p>Everytime you add a beta to your model, it adds another <em>degree of
freedom</em> to your model. This causes the model to eat up one of your
samples and thus functionally decreases your sample size. And we’ve
already talked at great length about how sample size influences
p-values. By decreasing our sample size, we are less likely to recover a
low p-value and correctly reject the null.</p>
<p>So, in the bison example, if you use this more complicated model, you
have reduced your sample size by three. And because of that, it has less
power.</p>
<p>Now, there is a caveat… hence the ‘asterisk’ above.</p>
<p>*If, and only if, the assumption of linearity is met.</p>
<p>If the assumption of linearity is not met, then we might have more
power with the more complicated model.</p>
</div>
<div id="example" class="section level3">
<h3>Example</h3>
<p>What if our data looked like this:</p>
<p><img src="lecture_11_files/figure-html/stripplot-2-1.png" width="432" /></p>
<p>Maybe this could happen if there was some sort of <em>intermediate
disturbance</em> that benefited biomass production.</p>
<p><strong>Q:</strong> If you ran an ANOVA, would you detect a
significant difference between these categorical groups?
<strong>YES</strong></p>
<p><strong>Q:</strong> If you ran a regression, would you detect a
significant difference between these categorical groups?
<strong>NO</strong></p>
<p><img src="lecture_11_files/figure-html/stripplot-3-1.png" width="432" /></p>
<p>This happens because the assumption of linearity is very clearly
violated. There is significant variation among these data, but we are
trying to fit a linear model – a straight line – to data that do not
have a linear relationship.</p>
<p>In this case, it would be much better to consider X as a categorical
variable, which will give us more Power to detect a significant
relationship.</p>
</div>
<div id="what-does-this-all-mean" class="section level3">
<h3>What does this all mean?</h3>
<p>If we go back to our list, it’s pretty clear that treating X as a
continuous variable is better in <em>many</em> ways – unless the
assumption of linearity is not met. If the assumption of linearity is
not met, then we:</p>
<ul>
<li>no longer describe it with a simple linear equation</li>
<li>lose power</li>
<li>cannot make predictions because we fit a line to data rather than
the necessary curve</li>
</ul>
<p>These advantages fall apart if we don’t have linear data.</p>
<p><strong>Recommendation: Always treat X-variable as continuous, unless
treating as categorical ‘significantly improves the fit of the model to
the data’ – which will only happen if the relationship is
non-linear</strong></p>
<p>This very nicely fits in with the principle of parsimony – Occam’s
razor.</p>
<p><strong>Occam’s razor – the world is simple, and it’s best to use
simple explanations to describe the world unless there is significant
evidence that more complexity is needed.</strong></p>
<ul>
<li>Simple ideas are best, unless you can disprove the simple model in
favor of a more complicated model.</li>
</ul>
<p>What do we mean by ‘significantly improve the fit’?</p>
<p>There is an idea in statistics (truism, axiom) that whenever you make
a model more complex it will always improve the fit.</p>
<ul>
<li>With two points, adding a linear beta will connect them.</li>
<li>With three points, adding a quadratic beta will connect them.</li>
<li>With four points, adding a cubed-term will connect them.</li>
</ul>
<p>So given that we can always improve a fit of a curve to points by
increasing the complexity of the model, we want to demonstrate that it
is a <strong>significant improvement of fit</strong>. We want to show
that this variable isn’t just improving fit, it is improving fit more
than would be expected if the variable didn’t have any explanatory value
whatsoever.</p>
</div>
<div id="testing-model-fit" class="section level3">
<h3>Testing model fit</h3>
<p>A simple way that I was taught to do this is using an <strong>‘F-drop
test’</strong>. However, there is some confusion about what this is
really called and names vary widely. Some other names include:</p>
<ul>
<li><strong>Extra sum of squares F-test</strong></li>
<li><strong>Type III sum of squares test</strong> (e.g., SAS)</li>
<li><strong>Partial likelihood ratio test</strong></li>
</ul>
<p>I prefer ‘F-drop test’ because this is how I was taught and it is an
easy and simple term.</p>
<p>No matter what you call it, you are comparing a more complex model to
a more simple model to test whether that more complex model is a
significant improvement to the fit in the data.</p>
<p>In other words, we are comparing our simple model:</p>
<p><strong><span class="math inline">\(Y = \beta_0 + \beta_1 Bison +
\epsilon \sim N(0, \sigma)\)</span></strong></p>
<p>To the complex model:</p>
<p><strong><span class="math inline">\(Y = \beta_0 + \beta_1 Bison1 +
\beta_2 Bison2 + \beta_3 Bison3 + \beta_4 Bison4 + \epsilon \sim N(0,
\sigma)\)</span></strong></p>
<p>And asking ourselves: do we need these extra betas?</p>
<p>Another way of thinking about it is that we are testing the null
hypothesis that these extra betas are all equal to zero:</p>
<p><span class="math inline">\(H_0: \beta_2 + \beta_3 + \beta_4 =
0\)</span></p>
<p>You might be scratching your head and thinking: the meanings of <span
class="math inline">\(\beta_1\)</span> are different between the two
models… That’s okay, the meanings of these two betas can be different.
The question here is whether we need additional betas beyond that first
one to improve the fit of the model.</p>
<p>Final thought: you will only reject the null when the more
complicated model is a better fit to the data. When that happens, what
does that mean about the relationship between X and Y?</p>
<p>Our data are significantly nonlinear! By comparing these two models,
we can use this as a way to test for non-linearity.</p>
</div>
<div id="summary" class="section level3">
<h3>Summary</h3>
<p>We are talking about a continuous X variable that could also be
treated as a categorical variable – either way is appropriate.
Generally, the recommendation is treat it as continuous, unless it is a
significant improvement in the fit of the data to treat it as
categorical. We can evaluate this using an ‘F-drop’ test.</p>
</div>
</div>
<div id="comparing-model-fit-in-r" class="section level2">
<h2>Comparing model fit in R</h2>
<p>Let’s briefly examine truth. We can peak at the code I used to
simulate it.</p>
<p>Here is the <a href="lecture_11_dataset1.csv">dataset(s) of
interest</a></p>
<pre class="r"><code>### Code for F-Drop tests

# Read in the data
datum &lt;- read.csv(&quot;lecture_11_dataset1.csv&quot;)
head(datum)</code></pre>
<pre><code>##   Bison  Biomass
## 1     0 6.117610
## 2     0 5.834632
## 3     0 5.844188
## 4     0 4.848827
## 5     0 5.914562
## 6     0 6.070139</code></pre>
<pre class="r"><code># Plot the data
plot(Biomass ~ Bison, data = datum)</code></pre>
<p><img src="lecture_11_files/figure-html/example-analysis-1.png" width="432" /></p>
<pre class="r"><code># In this case, R saws numbers in the &#39;Bison&#39; column and treated the variable as continuous

# Plot the data with X as a categorical variable
plot(Biomass ~ as.factor(Bison), data = datum)</code></pre>
<p><img src="lecture_11_files/figure-html/example-analysis-2.png" width="432" /></p>
<pre class="r"><code># Use the as.factor function to get R to treat variable as a categorical variable
# We could have manually created categorical variable or dummy-coded variables in Excel, but
# using as.factor is much easier.
# We could also also create a new variable and save it in our &#39;datum&#39;

# Run a regression, where we treat Bison as continuous
results &lt;- lm(Biomass ~ Bison, data = datum)
summary(results)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Biomass ~ Bison, data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.40751 -0.35372  0.08017  0.31745  1.50813 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  5.82066    0.15962   36.47  &lt; 2e-16 ***
## Bison       -0.78992    0.06517  -12.12 1.26e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5829 on 38 degrees of freedom
## Multiple R-squared:  0.7945, Adjusted R-squared:  0.7891 
## F-statistic: 146.9 on 1 and 38 DF,  p-value: 1.255e-14</code></pre>
<pre class="r"><code># Bison is continuous, because R saw numbers for that variable
# We know this because we only got 1 beta from the analysis

# Residuals plot
plot(residuals(results) ~ datum$Bison)</code></pre>
<p><img src="lecture_11_files/figure-html/example-analysis-3.png" width="432" /></p>
<pre class="r"><code># Error in data appear normally distributed, etc.

# Run an ANOVA - treat Bison as categorical
results2 &lt;- lm(Biomass ~ as.factor(Bison), data = datum)
# use the as.factor function to get R to treat &#39;Bison&#39; as continuous
summary(results2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Biomass ~ as.factor(Bison), data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.34640 -0.42548  0.05945  0.31859  1.56923 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         5.6720     0.2105  26.942  &lt; 2e-16 ***
## as.factor(Bison)1  -0.5065     0.2977  -1.701 0.097761 .  
## as.factor(Bison)2  -1.3055     0.2977  -4.385 0.000101 ***
## as.factor(Bison)3  -2.2822     0.2977  -7.665 5.40e-09 ***
## as.factor(Bison)4  -3.0618     0.2977 -10.284 4.06e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5955 on 35 degrees of freedom
## Multiple R-squared:  0.8025, Adjusted R-squared:  0.7799 
## F-statistic: 35.55 on 4 and 35 DF,  p-value: 7.096e-12</code></pre>
<pre class="r"><code># Conduct the f-drop test
anova(results2, results)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Biomass ~ as.factor(Bison)
## Model 2: Biomass ~ Bison
##   Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)
## 1     35 12.410                          
## 2     38 12.909 -3  -0.49888 0.469 0.7058</code></pre>
<pre class="r"><code># The two models we have are the arguments
# Usually, you should list the more complicated model first
# Note: the two models can&#39;t have the same number of parameters
# A significant p-value means the more complex model is a significant improvement in fit
# A non-significant p-value means the simpler model is adequate
# Note: RSS (which is the same thing as SSE) is always lower in the more complex model</code></pre>
<p>The F-drop tests compares the residual sum of squares (RSS) of the
two models and asks: how much does the sum of squares change when we add
all those variables? Does it change a lot, such that there is a
significant improvement in fit? Or did it not really decrease that much
error?</p>
<p>In this case, it didn’t change the error that much, and the test did
not recover a significant improvement in model fit by the more complex
model. Therefore, the more complicated model does not significantly
improve the fit to the data.</p>
<div id="f-drop-rules" class="section level3">
<h3>F-drop rules:</h3>
<p><strong>If P &lt; 0.05, the complex model is a significant
improvement (better) in fit.</strong></p>
<p><strong>If P &gt; 0.05, the complex model is <em>not</em> a
significant improvement in fit; the simpler model is
adequate.</strong></p>
<p>This does not mean the simpler model is <em>better</em>; it is
<strong>adequate</strong>. The simpler model will never be better, as
the more complex model will always be better due to the extra
parameters. The question is: is the complex model significantly better,
or is the simple model adequate.</p>
<p>As always, the limitations of p-values apply here: sample sizes,
effect sizes, etc.</p>
<p>Now, let’s consider another scenario. What if our data really looked
like the second example from above:</p>
<pre class="r"><code># Read in the data
datum &lt;- read.csv(&quot;lecture_11_dataset2.csv&quot;)
head(datum)</code></pre>
<pre><code>##   Bison  Biomass
## 1     0 1.764855
## 2     0 1.775757
## 3     0 1.727619
## 4     0 2.096225
## 5     0 2.148394
## 6     1 3.005565</code></pre>
<pre class="r"><code># Plot the data
plot(Biomass ~ Bison, data = datum)</code></pre>
<p><img src="lecture_11_files/figure-html/stripplot-4-1.png" width="432" /></p>
<p>We know that fitting a line to this data is not a good fit, and we
also know that a categorical approach would also fit this data pretty
well.</p>
<p>But what about an in-between approach, where we fit one line but it
is non-linear?</p>
</div>
<div id="quadratics" class="section level3">
<h3>Quadratics</h3>
<p>One of the easiest ways to fit a non-linear line to your data is
using a <strong>quadratic</strong> approach.</p>
<ul>
<li>A bit of a caveat: a quadratic may not be a good mechanistic
explanation, because there are very few things in ecology that follow
quadratic equations and they are hard to interpret. But, they can be
good descriptions of your data.</li>
</ul>
<p><strong><span class="math inline">\(Y = \beta_0 + \beta_1 Bison +
\beta_2 Bison^2 + \epsilon \sim N(0, \sigma)\)</span></strong></p>
<p>Notice that this curve has the advantage of <strong>no assumption of
linearity</strong> and also doesn’t use nearly as many betas as treating
X as categorical. In those ways, it is a good compromise between the two
options we have discussed so far.</p>
<pre class="r"><code># BONUS: How to fit a quadratic curve to data when x is continuous

# Fit a linear model
results &lt;- lm(Biomass ~ Bison, data = datum)

#Fit a quadratic curve to data when x is continuous
results3 &lt;- lm(Biomass ~ Bison + I(Bison^2), data = datum)
anova(results3, results)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Biomass ~ Bison + I(Bison^2)
## Model 2: Biomass ~ Bison
##   Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1     22  1.7485                                  
## 2     23 16.5003 -1   -14.752 185.61 3.349e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The quadratic model is the more complicated model here, because it
has more parameters than the linear model.</p>
<p>The quadratic model is a <strong>significant improvement</strong> in
model fit compared to the simple linear model.</p>
<p>We can use this approach to compare any two models, as long as one
model is more complicated than the other. We cannot compare models that
have the same number of effects (betas).</p>
<p><strong>Important point:</strong> when comparing two models, we need
to make sure the datasets are exactly the same. If you use different
datasets for two models, you can’t compare those two models using this
approach and other approaches we will explore in the semester.</p>
</div>
<div id="marginal-p-values" class="section level3">
<h3>Marginal p-values</h3>
<p>So far in this class, we have always calculated p-values the same
way: by comparing a model with that variable, to one without that
variable. But I haven’t explained how this works yet…</p>
<p>When we ran the F-drop test above, we compared two models:</p>
<p>Simple model: <strong><span class="math inline">\(Y = \beta_0 +
\beta_1 Bison + \epsilon \sim N(0, \sigma)\)</span></strong></p>
<p>More complex model: <strong><span class="math inline">\(Y = \beta_0 +
\beta_1 Bison + \beta_2 Bison^2 + \epsilon \sim N(0,
\sigma)\)</span></strong></p>
<p>This test evaluates the significance of the additional quadratic
beta. Is this <span class="math inline">\(\beta_2\)</span> significantly
different from zero?</p>
<p>We can also do an F-drop test comparing these two models:</p>
<p>Complex model: <strong><span class="math inline">\(Y = \beta_0 +
\beta_1 Bison + \epsilon \sim N(0, \sigma)\)</span></strong></p>
<p>Simple model: <strong><span class="math inline">\(Y = \beta_0 +
\epsilon \sim N(0, \sigma)\)</span></strong></p>
<p>An F-drop test evaluates whether <span
class="math inline">\(\beta_1\)</span> is significantly different from
zero.</p>
<p>In truth, this is what the p-value from our simple linear regression
analysis is evaluating!</p>
<pre class="r"><code># Read in dataset 1
datum &lt;- read.csv(&quot;lecture_11_dataset1.csv&quot;)

# Simple linear model
results &lt;- lm(Biomass ~ Bison, data = datum)
summary(results)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Biomass ~ Bison, data = datum)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.40751 -0.35372  0.08017  0.31745  1.50813 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  5.82066    0.15962   36.47  &lt; 2e-16 ***
## Bison       -0.78992    0.06517  -12.12 1.26e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5829 on 38 degrees of freedom
## Multiple R-squared:  0.7945, Adjusted R-squared:  0.7891 
## F-statistic: 146.9 on 1 and 38 DF,  p-value: 1.255e-14</code></pre>
<p>This p-value is called a <strong>marginal p-value</strong> and is
asking whether or how much that additional parameter marginally improves
the fit to the data beyond the simpler model without that variable.
(This is also called Type III Sum of Squares). It is essentially an
F-drop test that is built into our linear model.</p>
<p>So, anytime you need a p-value, if R gives it to you, it is most
likely giving you a marginal p-value.</p>
<p>Next class, we will explore more situations around this topic.</p>
<p><br></p>
</div>
</div>
<div id="truth" class="section level2">
<h2>Truth</h2>
<p>Here is code to simulate the data we analyzed in this lecture.</p>
<pre class="r"><code>### Lecture 11: code to simulate data for F-drop tests 

# Set the seed for reproducibility
set.seed(111)

## First dataset
# X variable
Bison &lt;- c(rep(0, 8), rep(1, 8), rep(2, 8), rep(3, 8), rep(4, 8))

# Error
error &lt;- rnorm(length(Bison), 0, 0.5)

# Y variable
Biomass &lt;- 6 - 0.8*Bison + error

# Create dataframe
datum &lt;- data.frame(Bison, Biomass)

# Save the CSV file
write.csv(datum, &quot;lecture_11_dataset1.csv&quot;, row.names = FALSE)

## Second dataset
# X variable
n &lt;- 25
x &lt;- c(rep(0, 5), rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 5))

# Error
error &lt;- rnorm(n, 0, 0.2)

# Dummy code x
dummy &lt;- data.frame(x = as.factor(x))
dummy &lt;- data.frame(model.matrix(~ dummy$x - 1, data=dummy))
colnames(dummy) &lt;- c(&quot;Zero&quot;, &quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;)

# Simulate continuous y-variable data
y &lt;- 2 + dummy$One * 1 + dummy$Two * 2 + dummy$Three * 1 + dummy$Four * 0 + error

# Create dataframe
datum &lt;- data.frame(Bison = x, Biomass = y)

# Save the CSV file
write.csv(datum, &quot;lecture_11_dataset2.csv&quot;, row.names = FALSE)</code></pre>
<p><a href="lecture_12.html">–go to next lecture–</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
