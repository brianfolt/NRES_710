<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 710" />


<title>GLM and GLMM</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 710</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 710</a>
    </li>
    <li>
      <a href="LECTURE1.html">Basic Concepts</a>
    </li>
    <li>
      <a href="LECTURE2.html">Sampling uncertainty</a>
    </li>
    <li>
      <a href="LECTURE3.html">Taxonomy of common statistics</a>
    </li>
    <li>
      <a href="LECTURE4.html">t-test and z-test</a>
    </li>
    <li>
      <a href="LECTURE5.html">chi-squared tests</a>
    </li>
    <li>
      <a href="LECTURE6.html">Linear Regression</a>
    </li>
    <li>
      <a href="LECTURE7.html">ANOVA</a>
    </li>
    <li>
      <a href="LECTURE8.html">GLM and GLMM</a>
    </li>
    <li>
      <a href="LECTURE9.html">Next steps</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="EXERCISE1.html">Exercise- data summary functions</a>
    </li>
    <li>
      <a href="EXERCISE2.html">Exercise- t-tests</a>
    </li>
    <li>
      <a href="EXERCISE3.html">Exercise- simple linear regression</a>
    </li>
    <li>
      <a href="EXERCISE4.html">Exercise- multiple linear regression</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    More Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Links.html">Links</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final Projects</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Datasets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Data1.dat">Data1</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">GLM and GLMM</h1>
<h4 class="author">NRES 710</h4>
<h4 class="date">Fall 2022</h4>

</div>


<div id="download-the-r-code-for-this-lecture" class="section level2">
<h2>Download the R code for this lecture!</h2>
<p>To follow along with the R-based lessons and demos, <a
href="LECTURE8.R">right (or command) click on this link and save the
script to your working directory</a></p>
</div>
<div id="overview-generalized-linear-models" class="section level2">
<h2>Overview: Generalized Linear Models</h2>
<p>Generalized Linear Models (GLM) are not generally covered in ‘intro’
stats classes, but they are so flexible and so common in ecology and
environmental science that you really need to know how to work with
these models!</p>
<p>The real data sets that we deal with as ecologists and environmental
scientists tend to violate some key assumptions of classical linear
regression or ANOVA. In particular, residuals are often non-normal and
variance is not equal (heteroskedastic) across the range of
predictions.</p>
<p>GLMs allow us to model response variables that are not amenable to
classical linear regression – but uses a model structure that closely
resembles linear regression. Pretty much everything about running a GLM
feels like linear regression. The primary function for running GLM
models (<code>glm()</code>) even looks very similar to the regression
function <code>lm()</code>.</p>
<p>GLMs are <em>parametric</em> analyses – it’s just that (1) we don’t
need to assume our response variable is normally distributed and (2) we
don’t need to assume the relationship between the response variable and
the predictor variable(s) is linear on the scale of the untransformed
response variable. Let’s look into each of these in more detail:</p>
<div id="alternative-error-distributions" class="section level3">
<h3>Alternative error distributions</h3>
<p>So we don’t need to assume the response variable is normally
distributed – but we do need to assume it is distributed according to
some known probability distribution – and we need to specify what
distribution we ARE assuming. We can assume that the response process is
Poisson distributed, or gamma distributed, or any of a host of other
distributions (GLMs are typically limited to the set of distributions
known as the “exponential family”). But we, the modelers, have to
specify which distribution to use!</p>
</div>
<div id="link-functions" class="section level3">
<h3>Link functions</h3>
<p>So we don’t need to assume the relationship between the mean of the
response variable and the predictor variable(s) is linear on the scale
of the untransformed response variable – but we DO assume that the
hypothesized relationship is linear on some transformation of the
response variable – and we need to specify what transformed version of
the response process we wish to assume linearity for. This is called the
‘link function’.</p>
<p>In the general case, a GLM can be described by the following
pseudo-equation:</p>
<p><span class="math inline">\(f(\mu)=\beta_0+\beta_1\cdot x_1+\beta_2
\cdot x_2 \ldots\)</span></p>
<p>The left side of this equation is the mean of the response variable,
transformed according to the specified link function. The right side of
this equation is called the <strong>linear predictor</strong> and
describes how the (transformed) mean response varies as a function of
the predictor variable(s).</p>
<p>To complete the picture, we represent the error distribution
(inherent variability among observations) according to whatever error
process we specified: e.g., a binomial distribution, Poisson
distribution, or a number of other possibilities.</p>
</div>
<div id="logistic-regression" class="section level3">
<h3>Logistic regression</h3>
<p>For example, we might have a <em>binary</em> response variable and a
<em>continuous</em> predictor variable. Making the assumption of
linearity would not necessarily make sense in this case.</p>
<p>Let’s first make up an example:</p>
<pre class="r"><code># logistic regression ----------------------

## made up data for glm #1 (logistic regression)

predictor &lt;- runif(100,0,50)
response &lt;- rbinom(100,1, plogis(-5 + 0.26*predictor) )

plot(response~predictor,ylim=c(-2,2))
abline(lm(response~predictor),col=&quot;red&quot;)   # overlay regression line</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>layout(matrix(1:4,nrow=2,byrow=2))
plot(lm(response~predictor))</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<p>Note that the predicted mean response at high values of the predictor
exceed 1- which is impossible for a binary response. In fact, the mean
(expected) value for a binary response is the same thing as the
“probability of success (frequency of obtaining a value of 1)” – and
probabilities are constrained between 0 and 1. Clearly we can’t make the
assumption of linearity on the un-transformed binary response.
Furthermore, the other diagnostic plots also do not look so hot…</p>
<p>One transformation that makes sense for a regression with a binary
response variable is the <strong>logit transformation</strong>. The
logit transformation is commonly used to take probabilities (which are
constrained between 0 and 1) and transform them to
<em>unconstrained</em> values that can vary between -Inf and Inf.</p>
<p>This way, the linear predictor (which is inherently unconstrained and
can therefore theoretically vary between -Inf and Inf) can always be
interpretable as a probability!</p>
<p>For example, take the following probabilities:</p>
<pre class="r"><code>probs &lt;- runif(10)
probs</code></pre>
<pre><code>##  [1] 0.10691064 0.55847640 0.44510033 0.08945630 0.42134245 0.21089711
##  [7] 0.69728466 0.29543409 0.55251127 0.04963902</code></pre>
<p>Here’s what happens if we apply the logit transformation:</p>
<p><span class="math inline">\(logit(p) =
log(\frac{p}{(1-p)})\)</span></p>
<pre class="r"><code>data.frame(
  p = probs,
  logit.p=log(probs/(1-probs))
)</code></pre>
<pre><code>##             p    logit.p
## 1  0.10691064 -2.1226933
## 2  0.55847640  0.2349809
## 3  0.44510033 -0.2204876
## 4  0.08945630 -2.3202917
## 5  0.42134245 -0.3172649
## 6  0.21089711 -1.3195264
## 7  0.69728466  0.8344008
## 8  0.29543409 -0.8691361
## 9  0.55251127  0.2108225
## 10 0.04963902 -2.9520648</code></pre>
<p>If our response variable is binary and we want to assume that the
mean response (on some transformed scale) is linearly dependent on our
predictor variable, the logit transformation is a good candidate for our
link function, because this way the mean response will never go below
zero or above one.</p>
<p>So instead of:</p>
<p><span class="math inline">\(\bar{y} = \beta_0 + \beta_1\cdot
x\)</span></p>
<p>We can use the <strong>logit link function</strong> and assume
instead that:</p>
<p><span class="math inline">\(logit(\bar{y}) = \beta_0 + \beta_1\cdot
x\)</span></p>
<p>If we solve for y, this equation becomes:</p>
<p><span class="math inline">\(\bar{y} = \frac{e^{\beta_0 + \beta_1\cdot
x}}{1+e^{\beta_0 + \beta_1\cdot x}}\)</span></p>
<p>This is what we do when we conduct a <em>logistic regression</em>!
Specifically, in a logistic regression we generally assume the
following:</p>
<p><strong>Response distribution</strong>: response variable is
binomially distributed. Often (but not necessarily), this takes the form
of a specific binomial distribution with size=1 (can only be zero or
one; also known as a Bernoulli distribution).</p>
<p><strong>Link function</strong>: the mean response (binomial
probability) is a linear function of the predictor variable(s) on the
logit scale.</p>
<p>We use a binomial response distribution because our response variable
is analogous to a coin flip. If you flip one coin you can get only a
zero or a one, just like the response variable. The binomial
distribution matches the response variable, so it is an appropriate
distribution to assume!</p>
<pre class="r"><code>  ## conduct logistic regression:

model &lt;- glm(response~predictor,family=binomial(link=&quot;logit&quot;))    # logistic regression in R
summary(model)   # summary looks similar to ordinary linear regression!</code></pre>
<pre><code>## 
## Call:
## glm(formula = response ~ predictor, family = binomial(link = &quot;logit&quot;))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4670  -0.1771   0.0593   0.2770   1.9181  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -5.93999    1.42090  -4.180 2.91e-05 ***
## predictor    0.26975    0.05919   4.557 5.19e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 132.813  on 99  degrees of freedom
## Residual deviance:  43.443  on 98  degrees of freedom
## AIC: 47.443
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<pre class="r"><code>newdat &lt;- data.frame(        # make predictions for plotting regression line and approx conf bounds
  predictor = seq(0,50,1)
)

mypred &lt;- predict(model,type=&quot;response&quot;,se.fit=T,newdata = newdat)

plot(response~predictor)
lines(newdat$predictor,mypred$fit,col=&quot;blue&quot;)
lines(newdat$predictor,mypred$fit+2*mypred$se.fit,col=&quot;blue&quot;,lty=2)
lines(newdat$predictor,mypred$fit-2*mypred$se.fit,col=&quot;blue&quot;,lty=2)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Note that the relationship between the response and predictor looks
non-linear. But this is not the same thing as non-linear regression. GLM
is a type of linear model for a reason. It’s just that the relationship
is assumed to be linear on the logit scale. Here is another
visualization of the same exact model:</p>
<pre class="r"><code>par(mfcol=c(1,2))

mypred &lt;- predict(model,type=&quot;link&quot;,se.fit=T,newdata = newdat)

plot(newdat$predictor,mypred$fit,col=&quot;blue&quot;,type=&quot;l&quot;,ylab=&quot;mean response (logit scale)&quot;,xlab=&quot;predictor&quot;)
lines(newdat$predictor,mypred$fit+2*mypred$se.fit,col=&quot;blue&quot;,lty=2)
lines(newdat$predictor,mypred$fit-2*mypred$se.fit,col=&quot;blue&quot;,lty=2)


mypred &lt;- predict(model,type=&quot;response&quot;,se.fit=T,newdata = newdat)

plot(newdat$predictor,mypred$fit,col=&quot;blue&quot;,type=&quot;l&quot;,ylab=&quot;mean response&quot;,xlab=&quot;predictor&quot;)
lines(newdat$predictor,mypred$fit+2*mypred$se.fit,col=&quot;blue&quot;,lty=2)
lines(newdat$predictor,mypred$fit-2*mypred$se.fit,col=&quot;blue&quot;,lty=2)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The standard regression diagnostic plots don’t work for logistic
regression. However, we can use deviance residuals, Pearson residuals or
quantile residuals to assess whether the key assumptions of GLM may be
violated. I generally prefer to use the quantile residuals, because they
(unlike deviance residuals) make sense for nearly any class of model
including logistic regression and even GLMM:</p>
<pre class="r"><code> # quantile residuals (GLM diagnostics)

qr &lt;- statmod::qresiduals(model)
qqnorm(qr)
abline(0,1)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>plot(qr~predict(model))</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
</div>
<div id="another-simple-example-poisson-count-regression"
class="section level3">
<h3>Another simple example (Poisson count regression)</h3>
<p>Sometimes our measured response is a <em>count</em> of something
(e.g., number of stems in a plot). In such cases, our response variable
cannot go below zero- and the response variable should ideally come from
a discrete distribution that only allows integers. The simplest way to
model this is:</p>
<p><strong>Response distribution:</strong> Poisson<br />
<strong>Link function:</strong> Natural logarithm</p>
<p>The Poisson distribution (with only one parameter) is the simplest
discrete probability distribution, and the (natural) log is the simplest
link function that maps a quantity with a lower bound of zero to a
quantity with a lower bound of -Inf. After all, a count cannot go below
zero, whereas a linear function of predictor variables can!</p>
<p>Let’s make up some count data:</p>
<pre class="r"><code># Count regression example

predictor = runif(30,-2,2)
response = rnbinom(30,mu=exp(3-0.5*predictor),size=2)     # make up data!

plot(response~predictor)
abline(lm(response~predictor))</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>par(mfrow=c(2,2))
plot(lm(response~predictor))</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<p>Here we see some potential issues with ordinary linear regression and
we might consider Poisson count regression instead!</p>
<pre class="r"><code>## try Poisson count regression model!

model &lt;- glm(response~predictor,family=poisson(link=&quot;log&quot;))
summary(model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = response ~ predictor, family = poisson(link = &quot;log&quot;))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -5.1637  -1.9085  -0.3191   2.0617   4.3448  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  2.86667    0.04955   57.86   &lt;2e-16 ***
## predictor   -0.75982    0.03677  -20.66   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 716.22  on 29  degrees of freedom
## Residual deviance: 186.76  on 28  degrees of freedom
## AIC: 322.16
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>plot(response~predictor)

newdat &lt;- data.frame(
  predictor = seq(-3,3,0.1)
)

mypred &lt;- predict(model,type=&quot;response&quot;,se.fit = T,newdata=newdat)

lines(newdat$predictor,mypred$fit,col=&quot;blue&quot;)
lines(newdat$predictor,mypred$fit+2*mypred$se.fit,col=&quot;blue&quot;,lty=2)
lines(newdat$predictor,mypred$fit-2*mypred$se.fit,col=&quot;blue&quot;,lty=2)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
</div>
<div id="diagnostic-testing-with-glm" class="section level2">
<h2>Diagnostic testing with GLM</h2>
<p>Obviously, the standard diagnostic plots don’t make much sense for
GLM– after all, they are testing assumptions that we are no longer
making! We don’t need to test for normality of residuals if we are
assuming our response variable is binomially distributed! We don’t need
to test for homogeneity of variance if our assumed probability
distribution is heteroskedastic!</p>
<p>The Poisson distribution, for example, does not have homogeneous
variance- in fact, the variance of the Poisson distribution is equal to
the mean. So the larger the expected value, the larger the variance!</p>
<p>However, we need some way of testing whether the distribution we
selected is a reasonable fit to our data. We could use so-called
deviance residuals here (which is the default in r) but I prefer to use
the quantile residual approach.</p>
<p>The DHARMa package in R not only implements the quantile residual
approach, but does so in a way that is applicable to nearly all models
you can fit in R!</p>
<p>NOTE: the DHARMa package also works for GLMM models (generalized
linear mixed-effects models- see below!)</p>
<pre class="r"><code>residuals(model)  # compute the deviance residuals for the poisson regression model</code></pre>
<pre><code>##           1           2           3           4           5           6 
##  1.41091663  0.02027447 -1.91687117 -1.88329487 -2.94412743  4.34481884 
##           7           8           9          10          11          12 
## -2.25788730 -1.15574121 -4.68016554  1.39524279 -0.06377005  0.89809385 
##          13          14          15          16          17          18 
##  2.62100502 -0.99064866  2.64203006  2.23928663 -1.16695579 -5.16374164 
##          19          20          21          22          23          24 
##  3.06452320 -0.83251497 -1.39097097 -2.82515688  2.24746686  2.13160046 
##          25          26          27          28          29          30 
##  1.85218827 -0.10777226 -0.53042214 -3.30217257  3.32555707 -3.96901409</code></pre>
<pre class="r"><code>summary(residuals(model))   # median should be near zero</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -5.1637 -1.9085 -0.3191 -0.2329  2.0617  4.3448</code></pre>
<pre class="r"><code>paste0(c(&quot;Null deviance: &quot;, &quot;Residual deviance: &quot;),     # null deviance should be much higher than residual deviance
       round(c(model$null.deviance, deviance(model)), 2))</code></pre>
<pre><code>## [1] &quot;Null deviance: 716.22&quot;     &quot;Residual deviance: 186.76&quot;</code></pre>
<pre class="r"><code>paste0(c(&quot;model df: &quot;, &quot;Residual deviance: &quot;),     # resid deviance should be close to residual df
       round(c(model$df.residual, deviance(model)), 2))</code></pre>
<pre><code>## [1] &quot;model df: 28&quot;              &quot;Residual deviance: 186.76&quot;</code></pre>
<p>So using the deviance residuals we’re starting to get a picture that
the Poisson distribution may not be a great fit. Let’s use the DHARMa
package now…</p>
<pre class="r"><code>library(DHARMa)

simresids &lt;- simulateResiduals(model,n=250,plot=T)   # clearly this is a bad fit!</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>plotResiduals(simresids,predictor)   # look for patterns across a predictor variable</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-11-2.png" width="672" /></p>
<pre class="r"><code>testResiduals(simresids)  # run tests on the residuals!</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-11-3.png" width="672" /><img src="LECTURE8_files/figure-html/unnamed-chunk-11-4.png" width="672" /></p>
<pre><code>## $uniformity
## 
##  Asymptotic one-sample Kolmogorov-Smirnov test
## 
## data:  simulationOutput$scaledResiduals
## D = 0.29942, p-value = 0.009224
## alternative hypothesis: two-sided
## 
## 
## $dispersion
## 
##  DHARMa nonparametric dispersion test via sd of residuals fitted vs.
##  simulated
## 
## data:  simulationOutput
## dispersion = 8.6128, p-value &lt; 2.2e-16
## alternative hypothesis: two.sided
## 
## 
## $outliers
## 
##  DHARMa bootstrapped outlier test
## 
## data:  simulationOutput
## outliers at both margin(s) = 6, observations = 30, p-value &lt; 2.2e-16
## alternative hypothesis: two.sided
##  percent confidence interval:
##  0.00000000 0.05083333
## sample estimates:
## outlier frequency (expected: 0.00933333333333333 ) 
##                                                0.2</code></pre>
<pre><code>## $uniformity
## 
##  Asymptotic one-sample Kolmogorov-Smirnov test
## 
## data:  simulationOutput$scaledResiduals
## D = 0.29942, p-value = 0.009224
## alternative hypothesis: two-sided
## 
## 
## $dispersion
## 
##  DHARMa nonparametric dispersion test via sd of residuals fitted vs.
##  simulated
## 
## data:  simulationOutput
## dispersion = 8.6128, p-value &lt; 2.2e-16
## alternative hypothesis: two.sided
## 
## 
## $outliers
## 
##  DHARMa bootstrapped outlier test
## 
## data:  simulationOutput
## outliers at both margin(s) = 6, observations = 30, p-value &lt; 2.2e-16
## alternative hypothesis: two.sided
##  percent confidence interval:
##  0.00000000 0.05083333
## sample estimates:
## outlier frequency (expected: 0.00933333333333333 ) 
##                                                0.2</code></pre>
<p>Okay so the DHARMa package diagnostics seem to indicate that the
Poisson regression was a poor fit to the data (you will find this is
usually true with Poisson regression). Let’s try running a negative
binomial regression instead!</p>
<pre class="r"><code>## try NegBinom count regression model!

library(MASS)

## NOTE: in reality you should use glm.nb because you don&#39;t know the additional parameter theta!
model &lt;- glm(response~predictor,family=negative.binomial(link=&quot;log&quot;,theta = 2))
summary(model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = response ~ predictor, family = negative.binomial(link = &quot;log&quot;, 
##     theta = 2))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.8241  -0.7926  -0.1538   0.4755   1.1283  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.87003    0.11157   25.73  &lt; 2e-16 ***
## predictor   -0.74449    0.09135   -8.15 7.15e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for Negative Binomial(2) family taken to be 0.6373751)
## 
##     Null deviance: 65.919  on 29  degrees of freedom
## Residual deviance: 21.200  on 28  degrees of freedom
## AIC: 220.46
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>model &lt;- glm.nb(response~predictor)

plot(response~predictor)

newdat &lt;- data.frame(
  predictor = seq(-3,3,0.1)
)

mypred &lt;- predict(model,type=&quot;response&quot;,se.fit = T,newdata=newdat)

lines(newdat$predictor,mypred$fit,col=&quot;blue&quot;)
lines(newdat$predictor,mypred$fit+2*mypred$se.fit,col=&quot;blue&quot;,lty=2)
lines(newdat$predictor,mypred$fit-2*mypred$se.fit,col=&quot;blue&quot;,lty=2)</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Now let’s check the model fit!</p>
<pre class="r"><code>simresids &lt;- simulateResiduals(model,n=250,plot=T)   # looks a lot better!</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<pre class="r"><code>testResiduals(simresids)  # run tests on the residuals!</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-13-2.png" width="672" /><img src="LECTURE8_files/figure-html/unnamed-chunk-13-3.png" width="672" /></p>
<pre><code>## $uniformity
## 
##  Exact one-sample Kolmogorov-Smirnov test
## 
## data:  simulationOutput$scaledResiduals
## D = 0.09726, p-value = 0.913
## alternative hypothesis: two-sided
## 
## 
## $dispersion
## 
##  DHARMa nonparametric dispersion test via sd of residuals fitted vs.
##  simulated
## 
## data:  simulationOutput
## dispersion = 0.6299, p-value = 0.552
## alternative hypothesis: two.sided
## 
## 
## $outliers
## 
##  DHARMa bootstrapped outlier test
## 
## data:  simulationOutput
## outliers at both margin(s) = 0, observations = 30, p-value = 1
## alternative hypothesis: two.sided
##  percent confidence interval:
##  0.00000000 0.03333333
## sample estimates:
## outlier frequency (expected: 0.007 ) 
##                                    0</code></pre>
<pre><code>## $uniformity
## 
##  Exact one-sample Kolmogorov-Smirnov test
## 
## data:  simulationOutput$scaledResiduals
## D = 0.09726, p-value = 0.913
## alternative hypothesis: two-sided
## 
## 
## $dispersion
## 
##  DHARMa nonparametric dispersion test via sd of residuals fitted vs.
##  simulated
## 
## data:  simulationOutput
## dispersion = 0.6299, p-value = 0.552
## alternative hypothesis: two.sided
## 
## 
## $outliers
## 
##  DHARMa bootstrapped outlier test
## 
## data:  simulationOutput
## outliers at both margin(s) = 0, observations = 30, p-value = 1
## alternative hypothesis: two.sided
##  percent confidence interval:
##  0.00000000 0.03333333
## sample estimates:
## outlier frequency (expected: 0.007 ) 
##                                    0</code></pre>
<p>And this time our count regression model fits the data well, and we
can report the results of this model with confidence!</p>
<p>NOTE: you should always run goodness-of-fit tests for any model you
fit to data. But it is especially important in the case of Poisson
regression, because in general most count data has much more variance
that a Poisson distribution allows. This can lead to inflated type 1
error and can be a huge problem. You should always be wary of reported
Poisson regression results with no goodness of fit checks!</p>
</div>
<div id="model-selection-with-aic" class="section level2">
<h2>Model selection with AIC</h2>
<p>Often we have multiple <strong>candidate models</strong> for
describing how our response variable relates to one or more of our
predictor variables. This is true for multiple linear regression and GLM
models, and mixed-effects models (see below) and non-linear regression
models.</p>
<p>Information-theoretic criteria like <strong>Akaike’s Information
Criterion</strong> provide a common currency that allows us to compare
and rank multiple models.</p>
<p>In general, the models with the lowest AIC are better than models
with higher AIC.</p>
<p>AIC is defined as:</p>
<p><span class="math inline">\(AIC = -2\cdot ln(Likelihood) + 2\cdot
k\)</span></p>
<p>Where <em>k</em> is the number of fitted parameters in the model and
<em>Likelihood</em> is the maximum likelihood of the data (probability
of the observed data set under the fitted model).</p>
<p>There is a commonly used correction for small sample size called
<em>AICc</em>:</p>
<p><span class="math inline">\(AICc = AIC +
\frac{2k^2+2k}{n-k-1}\)</span></p>
<p>Comparing multiple GLM models using AIC is relatively simple!</p>
<pre class="r"><code>######
# Make up data!

predictor1 = runif(30,-2,2)
predictor2 &lt;- runif(30,-100,100)
predictor3 &lt;- rnorm(30)   # useless predictor
response = rnbinom(30,mu=exp(3-0.5*predictor1+0.01*predictor2),size=2)


###
# fit a bunch of candidate models

model.pois.all &lt;- glm(response~predictor1+predictor2+predictor3,family=&quot;poisson&quot;)
model.nb.all &lt;- glm.nb(response~predictor1+predictor2+predictor3)
model.nb.1 &lt;- glm.nb(response~predictor1)
model.nb.12 &lt;- glm.nb(response~predictor1+predictor2)
model.nb.2 &lt;- glm.nb(response~predictor2)

cand.set &lt;- list(
  Poisson=model.pois.all,
  NegBin_allvars = model.nb.all,
  NegBin_pred1 = model.nb.2,
  NegBin_preds1and2 = model.nb.12,
  NegBin_pred2 = model.nb.2
)

### Make AIC table

AICtab &lt;- data.frame(
  ModelName = names(cand.set),
  LogLikelihood = sapply(cand.set,logLik),
  AIC = sapply(cand.set,AIC)
)

AICtab$DeltaAIC &lt;- abs(AICtab$AIC-min(AICtab$AIC))

AICtab[order(AICtab$DeltaAIC,decreasing = F),]</code></pre>
<pre><code>##                           ModelName LogLikelihood      AIC   DeltaAIC
## NegBin_preds1and2 NegBin_preds1and2     -121.2851 250.5703   0.000000
## NegBin_allvars       NegBin_allvars     -121.2836 252.5671   1.996863
## NegBin_pred1           NegBin_pred1     -131.0361 268.0723  17.501980
## NegBin_pred2           NegBin_pred2     -131.0361 268.0723  17.501980
## Poisson                     Poisson     -258.1650 524.3300 273.759717</code></pre>
<p>NOTE: for AIC model comparison to make sense, the response variable
must be exactly the same (note: you can’t compare different
transformations of the response variable) and the number of observations
must be exactly the same. Careful with missing data here!</p>
</div>
<div id="overview-mixed-effects-models" class="section level2">
<h2>Overview: Mixed-effects models</h2>
<p>Remember the assumption of independent observations? Every single one
of the models we have considered so far makes that assumption. If that
assumption is violated, we are committing pseudoreplication. If
pseudoreplication is relatively minor we may be able to ignore it for
modeling (but still reporting the potential issue). But in many cases
the issue is too large to ignore.</p>
<p>Mixed models allow us to build more realistic models that incorporate
some known potential sources of non-independence in our data.</p>
<p>The term <strong>mixed-effects models</strong> refers to the fact
that these models have two kinds of predictor variables: <em>fixed
effects</em> and <em>random effects</em>. The random effects are what
allow us to incorporate potential inter-dependencies among our
observations.</p>
<div id="is-it-a-fixed-effect-or-a-random-effect"
class="section level3">
<h3>Is it a fixed effect or a random effect?</h3>
<p>One point of confusion that quickly bubbles up around mixed-effects
models is the question of whether you should include a predictor
variable as a random or a fixed effect.</p>
<p>It might help to consider some typical random effects and some
typical fixed effects:</p>
<div id="fixed-effects" class="section level4">
<h4>Fixed effects</h4>
<p>Typical fixed effects include:</p>
<ul>
<li>Temperature (continuous fixed effect- e.g., linear regression)</li>
<li>Treatment (categorical fixed effect- e.g., ANOVA)</li>
<li>Interaction between temperature and treatment (interaction fixed
effect)</li>
</ul>
<p>Fixed effects are what we have been calling ‘predictor variables’ in
this class. They usually represent the variables that we want to relate
to our response variable. Ideally, our observations span the full range
of our fixed effects (predictor variables) such that we can use our data
to make inference about the relationship between our response and
predictor variables across the range of values that we might encounter
in our entire population of interest.</p>
<p>There is no assumption that our predictor variables are normally
distributed- in fact, the only assumption we make about the predictor
variables (usually) are that (1) they are measured with certainty and
(2) we ideally want them to be distributed evenly across the range of
values across which we want to make inference about our response
process.</p>
<p>For example, if we want to make inference about the relationship
between tree diameter and volume, we would like to take measurements of
trees that vary from the smallest-diameter trees we wish to make
inference about to the largest-diameter trees that we’d like to make
inference about. To ensure that our sample includes the full range of
diameters, we might specifically select trees that cover the entire
range of interest. That is, we could <strong>fix</strong> the set of
trees in our samples to include the full range of our predictor
variable. There is no rule stating that we have to select our predictor
variable levels from a random process. In fact, experimental design is
all about pre-determining our predictor variable levels!</p>
<p>The term <strong>fixed effect</strong> comes from experimental
design, where we literally arrange (fix) our observations into specific
treatment and control groups. For example, we might take 100 otherwise
interchangeable sapling trees and subject them randomly to different
treatments of <strong>fixed</strong> levels of some factor that might
influence growth (say nitrogen concentrations)- and then we can make
inference about the effect of nitrogen across the range of
concentrations that we determined.</p>
<p><strong>SIDE NOTE</strong>: we can (and often do) make inferences
about our response variable for levels of our predictor variables that
are outside the range of values in our data set. This is called
<strong>extrapolation</strong> and can be a dangerous practice because
we lack empirical support for this type of inference, especially when
the predictions are far outside the bounds of our data set.</p>
</div>
<div id="random-effects" class="section level4">
<h4>Random effects</h4>
<p>Typical Random effects include:</p>
<ul>
<li>Block ID (blocks are a small subset of the units about which you
want to make inference, multiple observations per block)</li>
<li>Site ID (sites are a small subset of the units about which you want
to make inference, multiple observations per site)</li>
<li>Year (study years are a small subset of the years about which you
want to make inference, multiple observations per year)</li>
<li>Individual (individuals studied are randomly selected from the
population of interest, each individual subjected to repeated
measurements)</li>
</ul>
<p>First of all, most random effects will be categorical.</p>
<p>Second, random effects are a random sample or otherwise a small
subset of the units about which you want to make inference.</p>
<p>Third, random effects variables must have multiple (usually &gt;3)
observations per level. If you only have one observation per random
effect level, it is not a random effect- it’s just a replicate
observation in an ordinary linear regression!</p>
</div>
</div>
<div id="mixed-effects-models-in-regression-notation"
class="section level3">
<h3>Mixed effects models in regression notation</h3>
<p>Before we run examples in R, let’s look at linear mixed-effects
regression models in regression notation.</p>
<p>The equation should look familiar- the only difference is that there
is more than one error term. Each random effect is now associated with a
new error term…</p>
<p>Let’s say we are fitting the following mixed-effects model:</p>
<p><strong>Response variable:</strong> Tortoise clutch size<br />
<strong>Predictor variables: fixed effects:</strong> Annual winter
precipitation, Annual spring temperature<br />
<strong>Predictor variables: random effects:</strong> Site, year</p>
<p>The simplest way to model random effects is to include them as
<strong>random intercepts</strong>– that is, the intercept term changes
randomly with each factor level.</p>
<p>In many cases, simply adding a random intercept term for each random
effect is appropriate for accounting for sources of non-independence in
your data set- but in many cases it is not sufficient. That is because
the slope (the relationship between your response and predictor– the
thing you are usually most interested in making inference about) can
itself vary depending on your random effect levels. This is called a
<strong>random slopes</strong> model.</p>
<p>Here is an equation to represent the simpler random-intercept
analysis:</p>
<p><span class="math inline">\(Clutch \space size = \beta_0 + \beta_1
\cdot precip_t + \beta_2 \cdot temp_t + \gamma_{site} + \gamma_{year} +
\epsilon_{obs}\)</span></p>
<p>Here we now have three sources of ‘error’: a random (normally
distributed) term for each site (<span
class="math inline">\(\gamma_{site}\)</span>), a random (normally
distributed) term for each year, and a normally distributed residual
error term (<span class="math inline">\(\epsilon_{obs}\)</span>).</p>
<p>If we want to include random slope and random intercept terms (which
is often the most appropriate model), the equation gets a bit more
complicated:</p>
<p><span class="math inline">\(Clutch \space size = \beta_0 + \beta_1
\cdot precip_t + \beta_2 \cdot temp_t + \gamma_{site} + \gamma_{year} +
\gamma_{site, \beta_1}\cdot precip_t + \gamma_{site, \beta_2}\cdot
temp_t + \epsilon_{obs}\)</span></p>
<p>Here we have two addition “error” terms that allow the slope terms
(beta1 and beta2) to vary randomly with each site and year.</p>
</div>
<div id="nested-random-effects" class="section level3">
<h3>Nested random effects</h3>
<p>You will often read descriptions of mixed-effects models saying
things like “Individual and Site were included as random effects, with
Individual nested within Site”.</p>
<p>Let’s imagine we randomly selected 10 sites and within each site we
capture a random sample of tortoises and we measure tortoise clutch size
for 4 consecutive years for each tortoise.</p>
<p>In this case, we have two potential random effects: site (random
subset of a much larger set of potential sites) and individual (random
subset of a much larger set of individuals within each site). Individual
#1 from Site #1 is obviously a different individual than Individual #1
from Site #2. So we can’t simply include a simple random effect term for
“Individual #1”. Since there are many “Individual #1”s, each “Individual
#1” must get its own random effect! In this case, Individual is nested
within Site!</p>
<p>Let’s contrast this with a non-nested random effect- let’s say site
and year.</p>
<p>In this case, Year “2014” in site #1 is the same as Year “2014” in
site #2. Therefore the random effect associated with year “2014” does
not differ depending on site. In this case, site and year are
independent, non-nested random effects!</p>
</div>
<div id="assumptions-of-mixed-effects-regression"
class="section level3">
<h3>Assumptions of mixed-effects regression</h3>
<p>The assumptions of mixed-effects regression are the same as in
classical linear regression (for mixed-effects regression models) or
generalized linear models (for generalized linear mixed models; GLMM).
The only additional assumption is this:</p>
<ul>
<li>All random effects are normally distributed!</li>
</ul>
<p>In some more complex models you might encounter models that assume
random effects take other distributions- but this is still rare to
see!</p>
</div>
<div id="example-mixed-effects-regression-in-r" class="section level3">
<h3>Example: mixed-effects regression in R</h3>
<p>The ‘workhorse’ package in R for fitting mixed-effects regression
models (and GLMM) is the ‘lme4’ package. However, there are some other
packages you should be aware of that can make your life easier. One such
package is “glmmTMB”- I have found this package has more flexibility and
tends to have less trouble fitting complex mixed-effects models.</p>
<p>We will use <a href="tundra2.csv">this dataset</a> as our example -
this is a data set on carbon balance in the tundra. This example is
taken from <a
href="https://bbolker.github.io/mixedmodels-misc/ecostats_chap.html">this
website</a>.</p>
<pre class="r"><code>#########
# TUNDRA EXAMPLE
#########

##### Read in the data

mc1 &lt;- read.csv(&quot;tundra2.csv&quot;,sep=&quot;,&quot;,na.strings=c(&quot;-&quot;,&quot;NA&quot;))

summary(mc1)</code></pre>
<pre><code>##        X              Year          Site               GS.NEE        
##  Min.   : 1.00   Min.   :1966   Length:82          Min.   :-153.000  
##  1st Qu.:21.25   1st Qu.:1993   Class :character   1st Qu.: -55.350  
##  Median :41.50   Median :1998   Mode  :character   Median : -20.867  
##  Mean   :41.50   Mean   :1998                      Mean   : -10.012  
##  3rd Qu.:61.75   3rd Qu.:2005                      3rd Qu.:   3.275  
##  Max.   :82.00   Max.   :2010                      Max.   : 390.000  
##        n             cYear         
##  Min.   :1.000   Min.   :-31.8698  
##  1st Qu.:1.000   1st Qu.: -4.8698  
##  Median :1.000   Median :  0.6302  
##  Mean   :1.829   Mean   : -0.1015  
##  3rd Qu.:2.750   3rd Qu.:  7.1302  
##  Max.   :6.000   Max.   : 12.1302</code></pre>
<pre class="r"><code>table(mc1$Year)  # some years have many observations</code></pre>
<pre><code>## 
##   1966   1970   1971   1972   1983   1984   1985   1987   1990   1991   1992 
##      1      1      1      1      1      1      1      1      4      5      2 
##   1993   1994   1995   1996   1997   1998   1999   2000   2001   2002 2002.5 
##      4      4      6      4      3      1      2      2      3      2      1 
##   2003 2003.5   2004 2004.5   2005   2006   2007   2008 2008.5   2009   2010 
##      2      1      3      1      4      5      4      4      1      3      3</code></pre>
<pre class="r"><code>table(mc1$Site)  # some sites have many observations</code></pre>
<pre><code>## 
##       Anajtyvuk River, AK               APL-133, AK               Atqasuk, AK 
##                         1                         3                         1 
##          Barrow Peninsula                Barrow, AK           Daring Lake, CA 
##                         1                        12                         7 
##             Halmer-Yu, RU          Happy Valley, AK                 Healy, AK 
##                         1                         5                         7 
##        Imnavait Creek, AK                Ivotuk, AK       Kytalyk Reserve, RU 
##                         4                         2                         1 
##           Lek Vorkuta, RU           Meade River, AK    Pituffik Peninsula, GL 
##                         2                         1                         2 
##           Prudhoe Bay, AK                Sagwon, AK       Samoylov Island, RU 
##                         7                         1                         1 
##                Talnik, RU Toolik &amp; Happy Valley, AK                Toolik, AK 
##                         1                         1                        12 
##                U-PAD, AK      West Dock &amp; U-PAD, AK            Zachenberg, GL 
##                         2                         1                         6</code></pre>
<pre class="r"><code>library(ggplot2)

## visualize net ecosystem exchange by year- varying by site

ggplot(mc1,aes(x=Year,y=GS.NEE,colour=Site))+geom_point()+
    geom_smooth(method=&quot;lm&quot;,alpha=0.3)+
    scale_y_continuous(limits=c(-150,400),oob=scales::squish)</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<pre><code>## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced</code></pre>
<pre><code>## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning
## -Inf

## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning
## -Inf

## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning
## -Inf

## Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning
## -Inf</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Okay now let’s fit a mixed-effects regression model with Net
Ecosystem Exchange (NEE) as the response variable and Year as the
fixed-effect (covariate). For our random effect we will have site- and
the intercept and trend (slope term) can vary with site. That is, we
have random intercept terms and random slope terms for each site.</p>
<p>Fitting a mixed model in ‘lme4’ (using the ‘lmer’ function) looks a
lot like fitting a linear model in ‘lm’.</p>
<pre class="r"><code>library(lme4)
cmod_lmer &lt;- lmer(GS.NEE ~ cYear + (1+cYear|Site),
                data=mc1, weights=n)</code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<p>A couple notes: first of all, the ‘weights’ term is there because
some observations are aggregated- that is, there are multiple
observations for a given site/year combination- those observations that
aggregate 3 observations get triple the weight of a site/year
combination with only one observation. (Technically, we are using an
inverse-variance weighting scheme!)</p>
<p>Secondly, note that year is treated as a fixed effect in this model.
That is- we are looking for a trend in our response variable over
time!</p>
<p>Third, note the warning of a ‘singular fit’. This is fairly common to
see, and doesn’t necessarily mean you can’t use the model. But it means
that lme4 struggled to fit the model and some of the parameters may have
very wide confidence bounds!</p>
<p>Let’s look at the model results using ‘summary’:</p>
<pre class="r"><code>summary(cmod_lmer)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: GS.NEE ~ cYear + (1 + cYear | Site)
##    Data: mc1
## Weights: n
## 
## REML criterion at convergence: 874.2
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.90221 -0.35038 -0.07972  0.30155  2.93141 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  Site     (Intercept)  116.05  10.773        
##           cYear         19.95   4.467   -1.00
##  Residual             3355.16  57.924        
## Number of obs: 82, groups:  Site, 24
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  -16.296      7.338  -2.221
## cYear         -3.745      1.341  -2.792
## 
## Correlation of Fixed Effects:
##       (Intr)
## cYear -0.417
## optimizer (nloptwrap) convergence code: 0 (OK)
## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<p>The results from the ‘summary’ function should look somewhat
familiar! But note that there are no p-values associated with your fixed
effects (regression coefficients)- just t-statistics! This is because it
is really hard to know what the degrees of freedom are for the test! Is
it the number of observations? Is it the number of sites?</p>
<p>Also note that the summary now includes a summary of the random
effects. One red flag here is that the random intercept term and random
slope terms are perfectly (negagively) correlated (Corr=-1). This is the
reason for the ‘singular’ warning in this case.</p>
<p>From the random effects summary we can see that site explains a
relatively small percent of the total variance among observations.</p>
<p>To fit the model using glmmTMB we can use the following code:</p>
<pre class="r"><code>library(glmmTMB)
cmod_glmmTMB &lt;- glmmTMB(GS.NEE ~ cYear + (1+cYear|Site),
                data=mc1,
                weights=n)</code></pre>
<pre><code>## Warning in fitTMB(TMBStruc): Model convergence problem; non-positive-definite
## Hessian matrix. See vignette(&#39;troubleshooting&#39;)</code></pre>
<pre class="r"><code>summary(cmod_glmmTMB)</code></pre>
<pre><code>##  Family: gaussian  ( identity )
## Formula:          GS.NEE ~ cYear + (1 + cYear | Site)
## Data: mc1
## Weights: n
## 
##      AIC      BIC   logLik deviance df.resid 
##       NA       NA       NA       NA       76 
## 
## Random effects:
## 
## Conditional model:
##  Groups   Name        Variance  Std.Dev. Corr  
##  Site     (Intercept) 3.841e-02  0.196         
##           cYear       3.148e+01  5.611   -0.79 
##  Residual             1.543e+03 39.280         
## Number of obs: 82, groups:  Site, 24
## 
## Dispersion estimate for gaussian family (sigma^2): 1.54e+03 
## 
## Conditional model:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -22.347      5.579  -4.005 6.19e-05 ***
## cYear         -3.706      1.416  -2.617  0.00887 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The summary looks similar- but one difference you see is that there
are p-values on the regression coefficients now.</p>
<p>Let’s perform some model diagnostics to test goodness-of-fit!</p>
<p>Since the standard linear regression assumptions apply, we could look
at our standard diagnostic plots:</p>
<pre class="r"><code>plot(cmod_lmer,type=c(&quot;p&quot;,&quot;smooth&quot;))</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code>plot(cmod_lmer,sqrt(abs(resid(.)))~fitted(.),
                  type=c(&quot;p&quot;,&quot;smooth&quot;),ylab=expression(sqrt(abs(resid))))</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-19-2.png" width="672" /></p>
<pre class="r"><code>plot(cmod_lmer,resid(.,type=&quot;pearson&quot;)~cYear,
                  type=c(&quot;p&quot;,&quot;smooth&quot;))</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-19-3.png" width="672" /></p>
<pre class="r"><code>qqnorm(residuals(cmod_lmer,type=&quot;pearson&quot;,scaled=T))</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-19-4.png" width="672" /></p>
<p>The diagnostics look, well, okay!. If you look into it more deeply,
you see that that ‘Toolik’ site is the one causing most of the
issues.</p>
<p>Okay let’s visualize the random effects (both intercept and random
slope terms):</p>
<pre class="r"><code>library(lattice)
dotplot(ranef(cmod_lmer,condVar=TRUE),
          lattice.options=list(layout=c(1,2)))</code></pre>
<pre><code>## $Site</code></pre>
<p><img src="LECTURE8_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>If we want to test to see if year is explaining any of the variance,
we can run an F-test (ANOVA):</p>
<pre class="r"><code>library(car)
Anova(cmod_lmer)</code></pre>
<pre><code>## Analysis of Deviance Table (Type II Wald chisquare tests)
## 
## Response: GS.NEE
##        Chisq Df Pr(&gt;Chisq)   
## cYear 7.7956  1   0.005237 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Note that if we had a categorical variable and wanted to run pairwise
comparisons, we could- using the ‘emmeans’ package (just like with
standard ANOVA).</p>
<p>If we want confidence intervals on our fixed effects, we can use the
‘confint’ function, just like in ordinary linear regression. Here we use
the Wald method (simplified version) and only extract confidence
intervals for the fixed effects.</p>
<p>I had to use ‘suppressWarnings’ to avoid lots of warning messages
here!</p>
<pre class="r"><code>confint(cmod_lmer,parm=&quot;beta_&quot;,method=&quot;Wald&quot;)</code></pre>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) -30.677776 -1.914152
## cYear        -6.374309 -1.116174</code></pre>
<p><a href="LECTURE9.html">–go to next lecture–</a></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
