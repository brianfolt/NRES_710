<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 710" />


<title>Linear Regression and ANOVA</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 45px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h2 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h3 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h4 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h5 {
  padding-top: 50px;
  margin-top: -50px;
}
.section h6 {
  padding-top: 50px;
  margin-top: -50px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 710</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 710</a>
    </li>
    <li>
      <a href="LECTURE1.html">Basic Concepts</a>
    </li>
    <li>
      <a href="LECTURE2.html">Analysis- t test</a>
    </li>
    <li>
      <a href="LECTURE3.html">Analysis- chi2 test</a>
    </li>
    <li>
      <a href="LECTURE4.html">Analysis- linear regression</a>
    </li>
    <li>
      <a href="LECTURE5.html">Analysis- ANOVA</a>
    </li>
    <li>
      <a href="LECTURE6.html">Analysis- Multivariate</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    More Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Links.html">Links</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final Projects</a>
    </li>
    <li>
      <a href="STUDYGUIDE.html">Study Guide</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Linear Regression and ANOVA</h1>
<h4 class="author">NRES 710</h4>
<h4 class="date">Fall 2020</h4>

</div>


<div id="download-the-r-code-for-this-lecture" class="section level2">
<h2>Download the R code for this lecture!</h2>
<p>To follow along with the R-based lessons and demos, <a href="LECTURE4.R">right (or command) click on this link and save the script to your working directory</a></p>
</div>
<div id="overview-linear-regression" class="section level2">
<h2>Overview: Linear Regression</h2>
<p>Classical linear regression involves testing for a relationship between a continuous response variable (dependent variable) and a continuous predictor variable (independent variable).</p>
<p>You can have multiple explanatory variables… hence you can have multiple linear regression. We will focus on <em>simple linear regression</em> here.</p>
<p>The null hypothesis is that there is no relationship between the response variable and the predictor variable in your population of interest. That is, observations with larger values for your predictor variable are not expected to be associated with larger or smaller values of the response variable on average.</p>
<div id="simple-example" class="section level3">
<h3>Simple example</h3>
<p>Imagine we are testing for a relationship between the brightness of artificial lighting at long stretch of beach (e.g., from hotels and other forms of development) and the total number of hatchling sea turtles per nest that successfully make it to the ocean.</p>
<p><em>Population</em>: All nests in this particular stretch of beach<br />
<em>Parameter(s)</em>: The mean number of hatchlings per nest that successfully travel from their nest to the ocean and how this changes as a function of the brightness of artificial lighting.<br />
<em>Sample</em>: All monitored nests<br />
<em>Statistic(s)</em>: Slope and intercept of the linear relationship between the measured response variable (number of successful ocean-arrivers per nest) and the predictor variable (brightness of artificial lighting)</p>
</div>
<div id="some-more-specifics" class="section level3">
<h3>Some more specifics!</h3>
<p>We assume there is some true model out there describing the expected (mean) value of our response variable <em>y</em> as a linear function of our predictor variable <em>x</em>:</p>
<p><span class="math inline">\(E(y)= \beta_0 + \beta_1\cdot x\)</span></p>
<p>To interpret this equation: the true mean of our response variable (<span class="math inline">\(E(y)\)</span>) is computed by taking the true intercept (<span class="math inline">\(\beta_0\)</span>) and adding the product of the true slope term (<span class="math inline">\(\beta_1\)</span>) and our predictor variable. This is just another way of saying that the expected value of the response variable is computed as a linear function of the predictor variable. <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_0\)</span> are both <em>parameters</em> that we wish to estimate!</p>
<p>To complete the thought, we also assume that there is some “noise” in the system. The “noise” term in regression and ANOVA is also known as the <em>residual error</em>. Specifically, we assume that the noise (residual error) is normally distributed with mean of zero and standard deviation of <span class="math inline">\(\sigma\)</span>.</p>
<p>Mathematically, we are assuming that our data/sample was generated from this process:</p>
<p><span class="math inline">\(y= \beta_0 + \beta_1\cdot x + \epsilon\)</span></p>
<p>OR:</p>
<p><span class="math inline">\(y= E(y) + \epsilon\)</span></p>
<p>WHERE:</p>
<p><span class="math inline">\(\epsilon \equiv Normal(0, \sigma)\)</span></p>
<p>This is actually the same assumption we made for a one-sample t-test!</p>
<p>For a t-test, we assume that there is a true population mean <span class="math inline">\(\mu\)</span> (equivalent to <span class="math inline">\(E(y)\)</span>) and that the true “noise” is normally distributed with standard deviation of <span class="math inline">\(\sigma\)</span>.</p>
<p>As with a t-test, where we can only approximate the true population mean by computing the sample mean, we can only approximate the linear relationship between our response and predictor variables:</p>
<p><span class="math inline">\(\bar{y} = \hat{B_0} + \hat{B_0}\cdot x\)</span></p>
<p>Just like any other statistical test, we assume that our observed linear relationship (defined by test statistics <span class="math inline">\(\hat{B_0}\)</span> and <span class="math inline">\(\hat{B_1}\)</span>) is just one of many such possible relationships that <em>could have been derived</em> from random sampling from our population of interest. If we collected a different sample, we would get a different linear relationship.</p>
<p>NOTE: in linear regression we are generally far more interested in the slope of the linear relationship (<span class="math inline">\(\hat{B_1}\)</span> rather than the intercept). So for now, we assume <span class="math inline">\(\hat{B_1}\)</span> (slope between response and predictor, computed from the sample) is the main test statistic of interest!</p>
<p>So.. what is the sampling distribution for our test statistic <span class="math inline">\(\hat{B_1}\)</span> under the null hypothesis in this case? Well, the answer is that it (when converted to units of standard error) is t-distributed! Let’s look into this a bit more.</p>
</div>
<div id="regression-and-t-tests--the-link" class="section level3">
<h3>Regression and t-tests- the link!</h3>
<p>Our discussion of t-tests actually rolls us straight into linear regression. Why? How?</p>
<p>In a one-sample t-test we are interested in estimating the true population mean, and we assume that our t-statistic (i.e., deviation of the sample mean from the null mean, in units of standard error) is t-distributed with degrees of freedom of one less than the sample size.</p>
<p>What is the hypothesis of a typical one-sample t-test? (<span class="math inline">\(\mu = 0\)</span> - that is, the true mean is zero!)</p>
<p>What is the hypothesis of a linear regression? (Slope = 0 - that is, the true relationship is zero).</p>
<p>So already we are seeing a bit of a similarity.</p>
<div id="t-test-recap" class="section level4">
<h4>t-test recap</h4>
<p>In a t-test we assume that the population mean is equal to the null mean and that the data are normally distributed. We could write this as (using regression notation):</p>
<p><span class="math inline">\(y = \beta_0 + \epsilon\)</span></p>
<p>WHERE:</p>
<p><span class="math inline">\(\epsilon \equiv Normal(0, \sigma)\)</span></p>
<p>In the above equation, <span class="math inline">\(\beta_0\)</span> represents the population mean under the null hypothesis.</p>
<p>We approximate our population mean using the sample mean <span class="math inline">\(\bar{\beta_0}\)</span> (formerly known as <span class="math inline">\(\bar{x}\)</span>) and we use the CLT and other statistical theories to show that the t-statistic:</p>
<p><span class="math inline">\(t = \frac{\bar{\beta_0}-\beta_0}{StdErr(\beta_0)}\)</span></p>
<p>Is t-distributed with df computed as the sample size minus the number of parameters estimated in the model (there is only one estimated parameter- the sample mean <span class="math inline">\(\bar{\beta_0}\)</span>).</p>
</div>
<div id="linear-regression-version" class="section level4">
<h4>linear regression version</h4>
<p>In linear regression we assume that the mean of our response is determined by two parameters- the intercept and the slope (linear relationship with the predictor variable). The null hypothesis (usually) is that the true mean is defined only by the intercept term and there is no relationship with the predictor variable (slope term is equal to zero).</p>
<p>The slope term of the linear regression can be computed as:</p>
<p><span class="math inline">\(\hat{\beta_1} = \frac{\sum_{i=1}^{n}{(x_i-\bar{x})(y_i-\bar{y})}}{\sum_{i-1}^{n}{(x_i-\bar{x})^2}}\)</span></p>
<p>And the intercept term can be computed as:</p>
<p><span class="math inline">\(\hat{\beta_0} = \bar{y} - \beta_1*\bar{x}\)</span></p>
<p>Simple linear regression models (that is, the slope and intercept terms above) are fitted using “least squares”. The best fit model minimizes the sum of the squared residuals. DRAW THIS OUT.</p>
<p>The standard error of the slope term (as opposed to the standard error of the mean) is computed as:</p>
<p><span class="math inline">\(std.err_{\hat{\beta_1}} = \sqrt{\frac{\frac{1}{n-2}\sum_{i=1}^n{\hat\epsilon_i^2}}{\sum_{i=1}^n{(x_i-\bar{x})^2}}}\)</span></p>
<p>Where the <span class="math inline">\(\hat\epsilon_i\)</span> refers to the residual errors.</p>
<p>We can then compute a t-statistic for the slope term (difference from the sample slope term and the null slope term in units of standard error):</p>
<p><span class="math inline">\(t=\frac{\hat{\beta_1}-\beta_{1null}}{std.err_{\hat{\beta_1}}}\)</span></p>
<p>Just like with the t-test, we assume that this t-statistic is t-distributed under the null hypothesis. This time the degrees of freedom is 2 less than the sample size (since computing the residual error requires computing two parameters: mean and slope).</p>
</div>
</div>
</div>
<div id="simple-linear-regression-examples" class="section level2">
<h2>Simple linear regression: examples</h2>
<p>Okay let’s consider the sea turtle example from the beginning of lecture:</p>
<p>Imagine we are testing for a relationship between the brightness of artificial lighting at long stretch of beach (e.g., from hotels and other forms of development) and the total number of hatchling sea turtles per nest that successfully make it to the ocean.</p>
<p><em>Population</em>: All nests in this particular stretch of beach<br />
<em>Parameter(s)</em>: The mean number of hatchlings per nest that successfully travel from their nest to the ocean and how this changes as a function of the brightness of artificial lighting.<br />
<em>Sample</em>: All monitored nests<br />
<em>Statistic(s)</em>: Slope and intercept of the linear relationship between the measured response variable (number of successful ocean-arrivers per nest) and the predictor variable (brightness of artificial lighting)</p>
<p>First we will simulate some data under a known process model:</p>
<pre class="r"><code>eggs.per.nest &lt;- 100
n.nests &lt;- 15
light &lt;- rnorm(n.nests,50,10)   # make up some light pollution values (predictor var)

probsucc &lt;- function(light){    # egg success as a function of light pollution
  plogis(1.5-0.01*light)
}

hatchlings.successful &lt;- rbinom(n.nests,eggs.per.nest,probsucc(light))   # determine number of successful eggs (response var)

#curve(probsucc,0,100)

plot(hatchlings.successful~light)  # plot the data</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Now that we have data, let’s run a linear regression!</p>
<pre class="r"><code>slope &lt;- sum((light-mean(light))*(hatchlings.successful-mean(hatchlings.successful)))/sum((light-mean(light))^2)
intercept &lt;- mean(hatchlings.successful) - slope*mean(light)

exp.successful &lt;- intercept+slope*light # expected number of eggs for each observation
residuals &lt;- hatchlings.successful-exp.successful

stderr &lt;- sqrt(((1/(n.nests-2))*sum(residuals^2))/(sum((light-mean(light))^2)))    # standard error

t.stat &lt;- (slope-0)/stderr    # t statistic

pval &lt;- 2*pt(t.stat,n.nests-2)    # p value


############
# use lm function instead (easy way!)

model &lt;- lm(hatchlings.successful~light)

summary(model)   # get the same t stat and p-value hopefully!</code></pre>
<pre><code>## 
## Call:
## lm(formula = hatchlings.successful ~ light)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -11.1861  -1.9268  -0.3609   2.0089   7.7926 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  91.4235     9.0816  10.067 1.67e-07 ***
## light        -0.3567     0.1839  -1.939   0.0745 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.147 on 13 degrees of freedom
## Multiple R-squared:  0.2244, Adjusted R-squared:  0.1647 
## F-statistic: 3.761 on 1 and 13 DF,  p-value: 0.07446</code></pre>
<pre class="r"><code>############
# plot regression line!

plot(hatchlings.successful~light)  # plot the data
abline(intercept,slope,col=&quot;blue&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>mod &lt;- lm(Volume~Girth,data=trees)
summary(mod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Volume ~ Girth, data = trees)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -8.065 -3.107  0.152  3.495  9.587 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***
## Girth         5.0659     0.2474   20.48  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.252 on 29 degrees of freedom
## Multiple R-squared:  0.9353, Adjusted R-squared:  0.9331 
## F-statistic: 419.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="assumptions-of-simple-linear-regression" class="section level2">
<h2>Assumptions of simple linear regression</h2>
<div id="normal-distribution" class="section level3">
<h3>Normal distribution</h3>
<p>Simple linear regression assumes that the model <strong>residuals</strong> are normally distributed.</p>
<p>Let’s quickly review what residuals are (see whiteboard exercise)</p>
<p>Let’s look at the residuals for our sea turtle example:</p>
<pre class="r"><code>my.intercept &lt;- model$coefficients[&quot;(Intercept)&quot;]
my.slope &lt;- model$coefficients[&quot;light&quot;]
expected.vals &lt;- my.intercept+my.slope*light 
my.residuals &lt;- hatchlings.successful-expected.vals
my.residuals</code></pre>
<pre><code>##  [1]   1.9184018  -1.3888319  -0.7763980   1.5561769  -2.4648016  -0.9236137
##  [7]  -0.3608579   7.7925980   7.0735228   5.9462566 -11.1860902   2.0994176
## [13]  -6.2096460  -3.7434128   0.6672784</code></pre>
<pre class="r"><code>### alternative way of getting residuals (best way!)

my.residuals2 &lt;- model$residuals

### alternative way using predict function

my.residuals3 &lt;- hatchlings.successful-predict(model)

### histogram of residuals

hist(my.residuals)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>### test for normality

qqnorm(my.residuals)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<pre class="r"><code>shapiro.test(my.residuals)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  my.residuals
## W = 0.95855, p-value = 0.6672</code></pre>
</div>
<div id="linear-model" class="section level3">
<h3>Linear model</h3>
<p>The true relationship between the response and predictor variables is linear!</p>
<p>Which one of the following plots violates this assumption?</p>
<pre class="r"><code>layout(matrix(1:4,nrow=2,byrow = T))
plot(anscombe$y1~anscombe$x1,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y2~anscombe$x2,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y3~anscombe$x3,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y4~anscombe$x4,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="independence-of-observations" class="section level3">
<h3>Independence of observations</h3>
<p>Of course! All classical analyses make this assumption!</p>
</div>
<div id="equal-variance-homoscedasticity-or-lack-of-heteroscedasticity" class="section level3">
<h3>Equal variance (homoscedasticity, or lack of heteroscedasticity)</h3>
<p>In simple linear regression, we assume that the model residuals are normally distributed– and that the spread of that distribution does not change as your predictor variable or response variable goes from small to large. That is, the residuals are <strong>homoskedastic</strong> – which means they have equal variance across the range of the predictor and response variables.</p>
<p>Let’s look at the sea turtle example:</p>
<pre class="r"><code>my.residuals &lt;- model$residuals

plot(my.residuals~light)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>plot(my.residuals~predict(model))</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<p>Do you see any evidence for <strong>heteroskedasticity</strong>? Heteroskedasticity means non-homogeneity of variance across the range of the predictor variable. Serious violations of the equal variance assumption may warrant a <strong>transformation</strong> of your data, or you may choose to use an alternative analysis like a <strong>generalized linear model</strong>.</p>
</div>
<div id="predictor-variable-is-known-with-certainty" class="section level3">
<h3>Predictor variable is known with certainty</h3>
<p>That is, the observed (e.g., measured) values for your predictor variable are correct and known with perfect precision. In regression, the randomness in linear regression is associated only with the response variable - and that randomness is normally distributed.</p>
</div>
<div id="diagnostic-plots" class="section level3">
<h3>Diagnostic plots</h3>
<p>Simple linear regression analyses are generally accompanied by ‘diagnostic plots’, which are intended to diagnose potential violations of key assumptions, or other potential pitfalls of regression.</p>
<p>When you use the ‘plot’ function in R to evalate a model generated with the ‘lm’ function, R returns four diagnostic plots:</p>
<pre class="r"><code>layout(matrix(1:4,2,byrow = T))
plot(model)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The first diagnostic plot (residuals vs fitted) lets you check for possible non-linear patterns. This plot should have no obvious pattern to it- it should look like random noise across the range of fitted values.</p>
<p>The second diagnostic plot (normal Q-Q) lets you check for normality of residuals. You already know how to do this.</p>
<p>The third diagnostic plot (scale-location) lets you check for homoskedascitiy. This plot should have no obvious pattern to it- it should look like random noise across the range of fitted values.</p>
<p>The fourth diagnostic plot (residuals vs leverage) lets you check to make sure your regression model isn’t driven by a few <strong>influential points</strong>. Look for points that fall far to the right of the other points- these are high leverage points. Also look specifically at the upper and lower right hand side of this figure- points in these regions (with large values for “Cook’s distance”) have the property that if they were removed from the analysis the results would change substantially.</p>
<p>These four figures, taken together, should be a guide to interpreting how robust your regression is. You need to be the scientist. Analyze these plots and make decisions about what you’re willing to accept.</p>
</div>
<div id="influential-points" class="section level3">
<h3>Influential points</h3>
<p>Which of the following plots has a highly influential point?</p>
<pre class="r"><code>layout(matrix(1:4,nrow=2,byrow = T))
plot(anscombe$y1~anscombe$x1,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y2~anscombe$x2,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y3~anscombe$x3,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)
plot(anscombe$y4~anscombe$x4,ylab=&quot;response&quot;,xlab=&quot;predictor&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Can you try to draw a regression line if this point was removed?</p>
</div>
</div>
<div id="a-deeper-exploration" class="section level2">
<h2>A deeper exploration</h2>
<div id="r-squared" class="section level3">
<h3>R-squared</h3>
<p>The coefficient of determination, also known as R-squared, is a statistic that is commonly used to indicate the performance of a regression model. Specifically, R-squared tells you how much of the total variance in your response variable is explained by your predictor variable.</p>
<p>R-squared can be computed as:</p>
<p><span class="math inline">\(R_2 = 1-\frac{SS_{res}}{SS_{tot}}\)</span></p>
<p>where <span class="math inline">\(SS_{tot} = \sum(y_i-\bar{y})^2\)</span> and <span class="math inline">\(SS_{res} = \sum(y_i-y_{pred})^2\)</span>.</p>
<p>The maximum value for R-squared is 1- values close to 1 indicate a very “good” model!</p>
</div>
<div id="regression-outcomes" class="section level3">
<h3>Regression outcomes</h3>
<p>Let’s explore some possible outcomes of linear regression:</p>
<p>Try to come up with scenarios (with plots) for each of the following:</p>
<ol style="list-style-type: decimal">
<li><p>Non-significant p, high R2</p></li>
<li><p>Significant p/ low R2</p></li>
<li><p>Significant p/high R2</p></li>
<li><p>Non-significant p, low R2</p></li>
</ol>
<p><a href="LECTURE5.html">–go to next lecture–</a></p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
