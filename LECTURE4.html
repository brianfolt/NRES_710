<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="NRES 710" />


<title>t-tests and z-tests</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 710</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="schedule.html">Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 710</a>
    </li>
    <li>
      <a href="LECTURE1.html">Basic Concepts</a>
    </li>
    <li>
      <a href="LECTURE2.html">Sampling uncertainty</a>
    </li>
    <li>
      <a href="LECTURE3.html">Taxonomy of common statistics</a>
    </li>
    <li>
      <a href="LECTURE4.html">t-test and z-test</a>
    </li>
    <li>
      <a href="LECTURE5.html">chi-squared tests</a>
    </li>
    <li>
      <a href="LECTURE6.html">Linear Regression</a>
    </li>
    <li>
      <a href="LECTURE7.html">ANOVA</a>
    </li>
    <li>
      <a href="LECTURE8.html">GLM and GLMM</a>
    </li>
    <li>
      <a href="LECTURE9.html">Next steps</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="EXERCISE1.html">Exercise- data summary functions</a>
    </li>
    <li>
      <a href="EXERCISE2.html">Exercise- t-tests</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    More Resources
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Links.html">Links</a>
    </li>
    <li>
      <a href="FINALPROJ.html">Final Projects</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Datasets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Data1.dat">Data1</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">t-tests and z-tests</h1>
<h4 class="author">NRES 710</h4>
<h4 class="date">Fall 2022</h4>

</div>


<div id="download-the-r-code-for-this-lecture" class="section level2">
<h2>Download the R code for this lecture!</h2>
<p>To follow along with the R-based lessons and demos, <a
href="LECTURE4.R">right (or command) click on this link and save the
script to your working directory</a></p>
<p>Considering the “field guide” from the last lecture, we are talking
about the t-test, which is appropriate when we have a quantitative
response variable and either (1) no predictor variable (one sample
t-test) or (2) a binary categorical predictor variable (two sample
t-test).</p>
</div>
<div id="the-t-statistic-signal-to-noise-ratio-for-the-sample-mean"
class="section level2">
<h2>The t statistic: signal to noise ratio for the sample mean</h2>
<p>The <em>t statistic</em> is a useful summary statistic that we can
calculate from a sample.</p>
<p>The <em>t distribution</em> is the theoretical sampling distribution
for the t-statistic under the null hypothesis. It describes the
distribution of “false signals” that can arise from random sampling
error.</p>
<p>Mathematically, <em>t</em> is the ratio of the difference between the
sample mean and the hypothesized population mean (<span
class="math inline">\(\mu_0\)</span>), in units of standard error:</p>
<p><span class="math inline">\(t =
\frac{(\bar{x}-\mu_0)}{s.e.}\)</span></p>
<p><strong>Q</strong> what part of the above fraction (numerator or
denominator) represents the signal? Which represents the “noise”?</p>
<p>Let’s assume that the null hypothesis is true (as we often do), and
that our null hypothesis is that the sample mean is equal to zero.</p>
<p>This is almost easier to conceptualize if we imagine a
<em>paired</em> design in which, say, a set of individuals are monitored
at two points in time: before a treatment and after.</p>
<p>Let’s imagine we are researching the effectiveness of a weight loss
drug. We sample 25 patients and weigh them before they start taking the
drug and then weigh all of them 1 month after they start taking the
drug. Never mind for now that this isn’t the best drug trial design!</p>
<p>So for each patient we compute the change in weight between the first
and second measurements. In the null universe, the drug is not effective
and we would expect the change to be no greater than expected by random
chance. Our task is to convince ourselves that the results (mean
observed weight loss across all patients) are meaningful.</p>
<p>Remember:</p>
<ul>
<li>the t statistic is the difference between the sample mean and some
hypothesized population mean, standardized in units of standard
error.<br />
</li>
<li>in classical null hypothesis testing, the t-statistic represents the
difference (in standard errors) between your sample mean and the
hypothetical true mean (e.g., under the null hypothesis).</li>
<li>the t-distribution is the sampling distribution for the
t-statistic.</li>
</ul>
<p>Here is a worked example, in R:</p>
<pre class="r"><code>## Paired t-test example -----------------

weightloss.data &lt;- c(-10.4,-11.6,3.9,1.5,-0.3,-3.5 -10.0,-6.7,-6.1,-2.4,-6.0,2.3,0.1,-4.1,-3.2, -11.3,-3.2,-9.3,-7.5,-5.7,-0.1,0.0,-9.8,1.0,-11.9)
hist(weightloss.data,breaks=7)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>mean.weightloss &lt;- mean(weightloss.data)
null.weightloss &lt;- 0
stdev.weightloss &lt;- sd(weightloss.data)
sample.size &lt;- length(weightloss.data)
std.error &lt;- stdev.weightloss/sqrt(sample.size)

t.statistic &lt;- (mean.weightloss-null.weightloss)/std.error
t.statistic</code></pre>
<pre><code>## [1] -4.544623</code></pre>
<pre class="r"><code>curve(dt(x,sample.size-1),-5.5,2)
abline(v=t.statistic,col=&quot;green&quot;,lwd=3)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<pre class="r"><code>t.crit &lt;- qt(0.05,sample.size-1)    # &#39;critical value&#39; of the t statistic- you can reject the null if your value is more extreme than this!

p=pt(t.statistic,sample.size-1)    # p value
p    # this is the p value</code></pre>
<pre><code>## [1] 7.241049e-05</code></pre>
<pre class="r"><code>options(scipen=100)   # if you don&#39;t like scientific notation!
p     </code></pre>
<pre><code>## [1] 0.00007241049</code></pre>
<pre class="r"><code># Alternative: use R&#39;s built in t test

t.test(weightloss.data,alternative = &quot;less&quot;)   # should get the same p-value!</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  weightloss.data
## t = -4.5446, df = 23, p-value = 0.00007241
## alternative hypothesis: true mean is less than 0
## 95 percent confidence interval:
##       -Inf -2.966463
## sample estimates:
## mean of x 
##   -4.7625</code></pre>
<p><strong>Q</strong> Does a significant p-value here mean that everyone
who takes the drug will lose weight? If not, what does it mean?</p>
<p>NOTE: as the df gets large, the t distribution approximates a
<em>standard normal</em> distribution.</p>
<p>Fun fact: William Sealy Gosset – a student of Pearson, worked at
Guinness brewery. He was hired to work out a way to determine the
quality of stout. He published anonymously as ‘Student’ because Guinness
prevented its employees from publishing. That’s why we call it the
‘Student’s t-test’!</p>
<div id="aside-difference-between-t-and-normal-distribution"
class="section level4">
<h4>Aside: difference between t and normal distribution</h4>
<p>You can use this code to explore the difference between the
t-distribution and the standard normal distribution:</p>
<pre class="r"><code># difference between t and standard normal

library(ggplot2)
base = ggplot() + xlim(-5,5) + geom_function(aes(col=&quot;normal (z)&quot;), fun=dnorm, lwd=2) + labs(y=&quot;density&quot;,x=&quot;potential fake signal&quot;)

deg_free &lt;- 1
base + geom_function(aes(col=sprintf(&quot;t, df = %s&quot;,deg_free)), fun=dt, args=list(df=deg_free), lwd=2,alpha=0.5)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code># base r plotting alternative!:

# curve(dnorm,-5,5,ylab=&quot;density&quot;,xlab=&quot;fake signal&quot;)
# curve(dt(x,df=1),add=T,col=&quot;green&quot;)</code></pre>
</div>
<div id="z-tests" class="section level3">
<h3>z-tests</h3>
<p>The z-test is a simpler version of the t-test for when sample size is
large enough so that we don’t really need the t distribution anymore
(n&gt;50) or if the population dispersion is known (in which case we can
assume the sample mean is normally distributed, following the CLT).</p>
<p>The z statistic is:</p>
<p><span class="math inline">\(Z =
\frac{(\bar{X}-\mu_0)}{s.e.}\)</span></p>
<p>As you can see, this looks very much like the t statistic. In fact it
is exactly the same!</p>
<p>The difference is that we are now assuming that the sampling
variation for this statistic is normally distributed instead of
t-distributed, so we now call it the z statistic!</p>
<div id="example-z-test-golden-state-warriors-height"
class="section level4">
<h4>Example z-test: Golden State Warriors height</h4>
<p>Tall basketball players are often successful. The Golden State
Warriors are often successful.</p>
<p>How tall are the Golden State Warriors (GSW) relative to other
basketball players in the NBA?</p>
<p>We hypothesize that GSW players are taller than the NBA average! The
null hypothesis is that they are essentially randomly sampled from the
general pool of NBA players.</p>
<p>What is the Population? Parameter? Sample? Statistic?</p>
<p>The GSW are 15 players with an average height of 6’8” (80”). A large
survey of NBA players suggests they are, on average, 6’7” tall (79”)
with a standard deviation of 4 inches.</p>
<p>NOTE: the z-test is only appropriate here because we know the
population standard deviation. If this is the case, we know under the
CLT that the sample means should follow a normal distribution (not a
t-distribution).</p>
<p>The reason the t-distribution differs from the normal distribution is
that we also are usually estimating the standard deviation from our
sample. The population standard deviation is therefore not known with
certainty most of the time!.</p>
<p>NOTE: Dr. Sullivan (NRES) obtained those values from research into
the height of NBA players.</p>
<p>Here, the mean (mu) and the standard deviation (sigma) are coming
from the population (all NBA players) instead of the sample. And we can
measure the whole population!</p>
<p>There are situations where you can use the sample standard deviation
to compute standard error for a z-test, but those should be only when
the sample size is <em>very large</em>. In general you should use a
t-test when the population standard deviation is unknown.</p>
<p>Use this link to download the data for this example: <a
href="GSW_height.csv">link to GSW data</a></p>
<pre class="r"><code># z distribution --------------------

## z test  

df &lt;- read.csv(&quot;GSW_height.csv&quot;)
GSWheight &lt;- df$Height
GSWheight</code></pre>
<pre><code>##  [1] 75 75 81 79 78 84 79 81 81 79 83 79 83 81 77</code></pre>
<pre class="r"><code>mean.gsw &lt;- mean(GSWheight)
sd.gsw &lt;- sd(GSWheight) 
sd.pop &lt;- 4
mean.pop &lt;- 79
n &lt;- length(GSWheight)
s.e. &lt;- sd.pop/sqrt(n)

null.height &lt;- mean.pop   # null: GSW are sampled randomly from the pool of all NBA players. They are not fundamentally different!

z.statistic &lt;- (mean.gsw-null.height)/s.e.
z.statistic</code></pre>
<pre><code>## [1] 0.6454972</code></pre>
<pre class="r"><code>curve(dnorm(x),-3,3)    # we assume that the z statistic is normally distributed- standard normal!
abline(v=z.statistic,col=&quot;green&quot;,lwd=3)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>p &lt;- 1-pnorm(z.statistic)    # is the p value enough evidence to tell you that GSW players are taller than the NBA average??
p       </code></pre>
<pre><code>## [1] 0.2593025</code></pre>
<pre class="r"><code>pnorm(z.statistic)</code></pre>
<pre><code>## [1] 0.7406975</code></pre>
<p><strong>Q</strong> That means that 74% of NBA players are shorter
than the GSW average height, right?</p>
<p>Wrong- but why??</p>
<p><strong>Q</strong> Would you reject the null hypothesis and say that
the GSW team is taller than expected by random chance?</p>
</div>
<div id="in-class-questions-not-to-be-turned-in" class="section level4">
<h4>In-class questions (not to be turned in)</h4>
<ol style="list-style-type: decimal">
<li>For any normal distribution (e.g., the standard normal or z
distribution) what is the probability of obtaining a random number
within 1 standard deviation of the mean?<br />
</li>
<li>For any normal distribution (e.g., the standard normal or z
distribution) what is the probability of obtaining a random number
within 2 standard deviations of the mean?<br />
</li>
<li>What is the 95% quantile of the standard normal distribution (z
distribution)?</li>
</ol>
</div>
</div>
</div>
<div id="t-tests" class="section level2">
<h2>t-tests</h2>
<p>Let’s face it. T-tests are more commonly used than z-tests for data
analysis.</p>
<p>The reason: we usually don’t know the population mean or standard
deviation. All we have is the sample!</p>
<p>To review: we have a continuous response variable and either no
predictor variable (one sample t-test) or a binary predictor variable
(two-sample t-test).</p>
<p>We have already performed simple versions of the t-test (one-sample
t-tests- see above). But t-tests are quite flexible and can be applied
to a wide variety of null-hypothesis testing scenarios:</p>
<div id="one-sample-t-test" class="section level3">
<h3>One-sample t-test</h3>
<p>Quick review: a one-sample t-test tests the consistency of the sample
data with the null hypothesis that the sample was generated from a
population with a specified mean (often zero, but not necessarily). As
we have seen before, the t-statistic in this case is expressed as:</p>
<p><span class="math inline">\(t =
\frac{(\bar{x}-\mu_0)}{s.e.}\)</span></p>
<p>Where <span class="math inline">\(\bar{x}\)</span> is the sample
mean, <span class="math inline">\(s.e.\)</span> is the standard error of
the mean, and <span class="math inline">\(\mu_0\)</span> is the
population mean under the null hypothesis.</p>
<p>The t-statistic can be interpreted as the difference between the
sample mean and the (null) population mean in units of standard
error.</p>
<p>Under the null hypothesis, the sampling distribution of the
t-statistic should follow a t distribution with n-1 degrees of
freedom.</p>
<pre class="r"><code># one sample t-test (paired t-test is a type of one sample t-test)  -------------

sample.data &lt;- rgamma(10,2,.1)
null.mean &lt;- 10

sample.size &lt;- length(sample.data)
sample.mean &lt;- mean(sample.data)
sample.sd &lt;- sd(sample.data)
std.err &lt;- sample.sd/sqrt(sample.size)
t.stat &lt;- (sample.mean-null.mean)/std.err

t.crit &lt;- abs(qt(0.025,sample.size-1))   # for 2-tailed test

p.val &lt;- (1-pt(abs(t.stat),sample.size-1))*2   # 


### alternatively use the t.test function:

t.test(sample.data,mu=null.mean)   # should get the same answer!</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  sample.data
## t = 2.5356, df = 9, p-value = 0.03194
## alternative hypothesis: true mean is not equal to 10
## 95 percent confidence interval:
##  11.18081 30.71919
## sample estimates:
## mean of x 
##     20.95</code></pre>
<div id="paired-t-test" class="section level4">
<h4>Paired t-test</h4>
<p>A common version of the one-sample t-test is the ‘paired t-test’, in
which a measurement is taken on a single individual before and after a
treatment is applied. An example of a paired t-test is the ‘weight loss
drug’ example from the opening of this lecture. In the weight-loss drug’
example, we measured patients before and after taking a drug and
measured the total weight change in each patient. Even though we have
two measurements per patient (repeated measures!) we collapse the data
for each patient into a single number (difference in weight) before
running our t-test.</p>
</div>
</div>
<div id="two-sample-t-test" class="section level3">
<h3>Two-sample t-test</h3>
<p>A two-sample t-test tests the consistency of the sample data with the
null hypothesis that sample A was generated from the same underlying
population as sample B. The t-statistic in this case is expressed
as:</p>
<p><span class="math inline">\(t =
\frac{(\bar{x_A}-\bar{x_B})}{s.e._{pooled}}\)</span></p>
<p>Where <span class="math inline">\(\bar{x_A}\)</span> is the sample
mean for group A, <span class="math inline">\(\bar{x_B}\)</span> is the
sample mean for group B, and <span
class="math inline">\(s.e._{pooled}\)</span> is the pooled standard
error of the mean across both samples.</p>
<p>The t-statistic in the 2-sample case can be interpreted as the
difference between the sample mean from population A and the sample mean
from population B, in units of standard errors.</p>
<p>The formula for the pooled standard deviation depends on whether your
sample size is equal in the two samples and whether you are able to
assume that the standard deviation is the same or similar in the two
underlying populations.</p>
<p>For example, the formula for the pooled standard deviation when we
assume equal standard deviation in the two populations (equal variance
assumption) but we have unequal sample size is:</p>
<p><span class="math inline">\(\sqrt{((N_1-1)*\sigma_1^2 +
(N_2-1)*\sigma_2^2)/(N_{pooled}-2)}\)</span></p>
<p>For the 2-sample t-test, the degrees of freedom is equal to the total
sample size across both samples minus 2.</p>
<p><em>Q</em>: Can you think of why the df is two less than the pooled
sample size?</p>
<pre class="r"><code># two sample t-test -----------------

sample.data.1 &lt;- rnorm(15,55,10)
sample.data.2 &lt;- rnorm(10,45,10)

sample.size.1 &lt;- length(sample.data.1)
sample.size.2 &lt;- length(sample.data.2)
sample.size.pooled &lt;- length(sample.data.1) + length(sample.data.2)

sample.mean1 &lt;- mean(sample.data.1)
sample.mean2 &lt;- mean(sample.data.2)

sample.sd1 &lt;- sd(sample.data.1)
sample.sd2 &lt;- sd(sample.data.2)
sample.sd.pooled &lt;- sqrt(((sample.size.1-1)*sample.sd1^2 + (sample.size.2-1)*sample.sd2^2)/(sample.size.pooled-2))

std.err.pooled &lt;- sample.sd.pooled*sqrt(1/sample.size.1+1/sample.size.2)
t.stat &lt;- (sample.mean1-sample.mean2)/std.err.pooled

t.crit &lt;- abs(qt(0.025,sample.size.pooled-2))   # for 2-tailed test

p.val &lt;- (1-pt(abs(t.stat),sample.size.pooled-2))*2   # 2-tailed test



### alternatively use the t.test function:

t.test(sample.data.1,sample.data.2,var.equal = T)   # should get the same answer!</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  sample.data.1 and sample.data.2
## t = 4.0266, df = 23, p-value = 0.0005266
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##   7.615085 23.706624
## sample estimates:
## mean of x mean of y 
##  54.74747  39.08662</code></pre>
<div id="one-tailed-vs-two-tailed-tests" class="section level4">
<h4>One tailed vs two-tailed tests</h4>
<p>Tailed-ness is a common point of confusion when running t-tests and
z-tests, so it’s worth taking a little time to review.</p>
<p>It all has to do with what we mean by ‘more extreme than the observed
test statistic’ (from the definition of a p-value).</p>
<p>In a one-tailed test in which our (alternative) hypothesis is that
our sample mean is <em>greater than</em> than the null hypothesis, we
mean ‘more positive than the observed test statistic’.</p>
<p>In a one-tailed test in which our (alternative) hypothesis is that
our sample mean is <em>less than</em> than the null hypothesis, we mean
‘more negative than the observed test statistic’.</p>
<p>In a two-tailed test, we mean ‘more positive or more negative than
the absolute value of our observed test statistic’ (absolute value is
more extreme)</p>
<p>Here is an R demo to illustrate the concept:</p>
<p>We have the following data:</p>
<pre class="r"><code># one vs two tailed demo

#my.data &lt;- rnorm(15, 0.5, 1)   # generate sample data
my.data &lt;- c(0.20119786,1.41700898,-0.72426698,0.44006284,0.01487128,-0.19031680,1.75470699,-0.81992816,2.31978530,  2.71442595,-0.31461411,0.52086138,-0.50580117,1.52260888,0.76454698)
samp.mean &lt;- mean(my.data)
samp.sd &lt;- sd(my.data)
samp.n &lt;- length(my.data)
std.err &lt;- samp.sd/sqrt(samp.n)

null.mean &lt;- 0

t.statistic &lt;- (samp.mean-null.mean)/std.err

### Two-tailed
curve(dt(x,samp.n-1),-3,3, main=&quot;Meaning of more extreme (two tailed version)&quot;,
      ylab=&quot;probability density&quot;,xlab=&quot;t statistic&quot;)    # visualize the sampling distribution of the t-statistic
abline(v=t.statistic,lwd=2,col=&quot;blue&quot;)

xs &lt;- seq(abs(t.statistic),10,0.05)                
ys &lt;- dt(xs,samp.n-1)
polygon(x=c(xs,rev(xs)),y=c(ys,rep(0,times=length(ys))),col=&quot;green&quot;,border=NA)
polygon(x=c(-xs,rev(-xs)),y=c(ys,rep(0,times=length(ys))),col=&quot;green&quot;,border=NA)

p.twosided &lt;- pt(-abs(t.statistic),samp.n-1)*2     # two-tailed p-value

text(-2,0.3,paste(&quot;p =&quot;,round(p.twosided,4)))</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>### One-sided (alternative = &#39;greater&#39;)
curve(dt(x,samp.n-1),-3,3, main=&quot;Meaning of more extreme (one tailed version: greater than)&quot;,
      ylab=&quot;probability density&quot;,xlab=&quot;t statistic&quot;)    # visualize the sampling distribution of the t-statistic
abline(v=t.statistic,lwd=2,col=&quot;blue&quot;)

xs &lt;- seq(t.statistic,10,0.05)                
ys &lt;- dt(xs,samp.n-1)
polygon(x=c(xs,rev(xs)),y=c(ys,rep(0,times=length(ys))),col=&quot;green&quot;,border=NA)

p.onesided &lt;- pt(-abs(t.statistic),samp.n-1)     # one-tailed p-value

text(-2,0.3,paste(&quot;p =&quot;,round(p.onesided,4)))</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<p>In the same way, the critical value for a t-statistic is different
for a two-tailed vs a one-tailed test:</p>
<pre class="r"><code>### t-crit in one tailed vs two tailed test

sample.size=7

curve(dt(x,sample.size-1),-8,4, main=&quot;2-tailed vs 1-tailed critical value&quot;,
      xlab=&quot;t-statistic&quot;,ylab=&quot;probability density&quot;)

alpha &lt;- 0.1
t.crit.twosided &lt;- qt(alpha/2,sample.size-1) 

abline(v=c(t.crit.twosided,abs(t.crit.twosided)),col=&quot;red&quot;,lwd=2)


t.crit.twosided &lt;- qt(alpha/2,sample.size-1) 

abline(v=c(t.crit.twosided,abs(t.crit.twosided)),col=&quot;red&quot;,lwd=2)

t.crit.onesided &lt;- qt(alpha,sample.size-1) 

abline(v=abs(t.crit.onesided),col=&quot;green&quot;,lwd=2)
abline(v=t.crit.onesided,col=&quot;blue&quot;,lwd=2)

legend(&quot;topleft&quot;,lwd=c(2,2,2),col=c(&quot;red&quot;,&quot;green&quot;,&quot;blue&quot;),bty=&quot;n&quot;,legend=c(&quot;two-tailed crit value&quot;,&quot;one-tailed crit value (greater than)&quot;,&quot;one-tailed crit value (less than)&quot;))</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>All of these critical values have the same percentage of the sampling
distribution that are more extreme than them (the percentage is alpha,
which is set at 0.1 in this example)– but the definition of
‘extremeness’ differs for the one-tailed and two-tailed cases!</p>
</div>
<div id="welchs-t-test-default-t-test-in-r" class="section level4">
<h4>Welch’s t-test (default t-test in R)</h4>
<p>Welch’s t-test allows for unequal sample sizes AND unequal variance
in the two populations. The Welch’s t-test is like the two-sample t-test
we already ran, but it is more flexible. Like other t-tests, this test
is fairly robust against violations of normality due to the CLT.
<em>This is the DEFAULT version of a t-test in R</em>. If you want a
classical Student’s t-test, you need to tell it to do so (otherwise it
will do the Welch’s test; use the ‘var.equal = TRUE’ argument to run the
classical test)!</p>
<p>I won’t go through the math for the Welch’s t-test (you can look it
up if you’d like) because the basic concepts and interpretation are the
same. The differences lie in the computation of the pooled variance and
the degrees of freedom.</p>
<p>Because the Welch’s t-test is robust to unequal variance
(heteroskedasticity) it can be more robust than the <code>lm()</code>
function, which will assume equal variance among the residuals for the
two groups.</p>
</div>
</div>
</div>
<div id="t-tests-in-r" class="section level2">
<h2>t-tests in R</h2>
<p>Previously, we have mostly avoided using R’s built in ‘t-test’
function. Let’s use it now!</p>
<pre class="r"><code># More t-test examples

# T-tests  ---------------

# Are my data greater than zero? 
Group0 &lt;- c(0.5, -0.03, 4, 2.5, 0.89, 2.2, 1.7, 1.125)
hist(Group0)
t.test(Group0,alternative=&quot;greater&quot;) # This gets at directionality</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  Group0
## t = 3.5487, df = 7, p-value = 0.00468
## alternative hypothesis: true mean is greater than 0
## 95 percent confidence interval:
##  0.7507388       Inf
## sample estimates:
## mean of x 
##  1.610625</code></pre>
<pre class="r"><code>#Are my data different than zero? 
Group0 &lt;- c(0.5, -0.03, 4, 2.5, 0.89, 2.2, 1.7, 1.125)
hist(Group0)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>t.test(Group0) # Okay, that&#39;s to zero. What about if it&#39;s different than 1? </code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  Group0
## t = 3.5487, df = 7, p-value = 0.00936
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  0.5374007 2.6838493
## sample estimates:
## mean of x 
##  1.610625</code></pre>
<pre class="r"><code># are my data different than 1? 
t.test(Group0, mu=1)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  Group0
## t = 1.3454, df = 7, p-value = 0.2205
## alternative hypothesis: true mean is not equal to 1
## 95 percent confidence interval:
##  0.5374007 2.6838493
## sample estimates:
## mean of x 
##  1.610625</code></pre>
<pre class="r"><code># Now let&#39;s test two groups. 
# are the means equal? 
group1 &lt;- c(7,9,6,6,6,11,6,3,8,7)
group2 &lt;- c(11,13,8,6,14,11,13,13,10,11)
t.test(group1, group2, var.equal=T) # Notice how we set equal variance? Look at the output - &quot;Two Sample t-test.&quot;</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  group1 and group2
## t = -3.9513, df = 18, p-value = 0.000936
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -6.27997 -1.92003
## sample estimates:
## mean of x mean of y 
##       6.9      11.0</code></pre>
<pre class="r"><code># is this one-tail or two? 

group1 &lt;- c(7,9,6,6,6,11,6,3,8,7)
group2 &lt;- c(11,13,8,6,14,11,13,13,10,11)
t.test(group1, group2) #  &quot;Welch&#39;s Two Sample t-test&quot;</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  group1 and group2
## t = -3.9513, df = 17.573, p-value = 0.0009743
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -6.283771 -1.916229
## sample estimates:
## mean of x mean of y 
##       6.9      11.0</code></pre>
<pre class="r"><code># WELCH&#39;S TEST IS THE DEFAULT IN R</code></pre>
</div>
<div id="t-test-assumptions" class="section level2">
<h2>t-test assumptions</h2>
<div id="normality-of-the-data" class="section level3">
<h3>Normality of the data</h3>
<p>Like we have mentioned before, the t-test is quite robust to
non-normality of the raw data – this is due to the Central Limit
Theorem!</p>
<p>However, if the data are strongly skewed or otherwise clearly
non-normal you may want to go with a non-parametric alternative (see
below)!</p>
<div id="testing-for-normality" class="section level4">
<h4>Testing for normality!</h4>
<p>Before ‘accepting’ the results of a statistical test, we often want
to run ‘goodness-of-fit’ tests to see if there is any reason to think we
may be violating key assumptions of our statistical analyses. For the
t-test and many other parametric tests, one ‘goodness-of-fit’ test we
might want to run is a test for normality. For linear regression
analyses, we test for normality of the residuals.</p>
<p>For the t-test, we should test for normality of the data (one-sample
t-test) or test separately for each of the two groups (two-sample
t-test).</p>
<p>Here we will discuss two tests- one visual and one quantitative. The
visual test is called a ‘quantile-quantile (Q-Q) plot’ and the
quantitative test is called a Shapiro-Wilk test.</p>
<p>The Q-Q plot compares the quantiles of the observed data (y axis)
with the theoretical quantiles from a normal distribution (x axis). If
the q-q plot is highly non-linear, your data are probably not normally
distributed and you might want to run a non-parametric alternative.</p>
<p>The Shapiro-Wilk test is a way of quantitatively assessing the
linearity of the q-q plot such that you can get a p-value. The p-value
here is interpreted the same as any other p-value. The catch is that the
null hypothesis is that the data are normally distributed. If p&gt;alpha
you fail to reject the null - that is, you can move forward assuming
that the data are normally distributed!</p>
<pre class="r"><code>## testing for normality ------------------

my.data.nonnorm &lt;- rgamma(20,0.1,0.1)    # simulate some non-normal data
hist(my.data.nonnorm,main=&quot;non-normal&quot;)     # visualize the data distribution</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>my.data.norm &lt;- rnorm(20,6,0.9)    # simulate some normal data
hist(my.data.norm,main=&quot;normal&quot;)     # visualize the data distribution</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-10-2.png" width="672" /></p>
<pre class="r"><code># visualize q-q plot

qqnorm(my.data.nonnorm,main=&quot;non-normal&quot;)   # visual test for normality</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-10-3.png" width="672" /></p>
<pre class="r"><code>qqnorm(my.data.norm,main=&quot;normal&quot;)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-10-4.png" width="672" /></p>
<pre class="r"><code># run shapiro-wilk test

shapiro.test(my.data.nonnorm)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  my.data.nonnorm
## W = 0.51579, p-value = 0.0000004431</code></pre>
<pre class="r"><code>shapiro.test(my.data.norm)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  my.data.norm
## W = 0.95002, p-value = 0.3674</code></pre>
</div>
</div>
<div id="outliers" class="section level3">
<h3>Outliers</h3>
<p>The t-test can be highly affected by outliers. If you suspect your
data contain highly influential outliers, you might want to run a
non-parametric alternative, as non-parametric tests are much less
influenced by outliers.</p>
<p>To look for outliers, the first step is to simply visualize a
histogram of the raw data and look for any observations that fall far
from the others.</p>
</div>
<div id="equal-variance" class="section level3">
<h3>Equal variance</h3>
<p>Some versions of the two-sample t-test assume equal variance. I
generally recommend using the Welch’s t-test, which is robust to unequal
variances. But, it’s useful to know how to test for unequal variances.
Bartlett’s test and Levene’s test are both useful for assessing the
equality of variances in two or more groups. These tests are also useful
for ANOVA.</p>
<pre class="r"><code># test for equal variances

# are the variances equal? 
group1 &lt;- c(7,9,6,6,6,11,6,3,8,7)
group2 &lt;- c(11,13,8,6,14,11,13,13,10,11)

bartlett.test(list(group1,group2))   # Bartlett&#39;s test has stong assumption of normality</code></pre>
<pre><code>## 
##  Bartlett test of homogeneity of variances
## 
## data:  list(group1, group2)
## Bartlett&#39;s K-squared = 0.2096, df = 1, p-value = 0.6471</code></pre>
<pre class="r"><code>car::leveneTest(c(group1,group2),rep(c(&quot;group1&quot;,&quot;group2&quot;),each=10))    # Levene&#39;s test is less reliant on normality</code></pre>
<pre><code>## Warning in leveneTest.default(c(group1, group2), rep(c(&quot;group1&quot;, &quot;group2&quot;), :
## rep(c(&quot;group1&quot;, &quot;group2&quot;), each = 10) coerced to factor.</code></pre>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##       Df F value Pr(&gt;F)
## group  1  0.1858 0.6716
##       18</code></pre>
</div>
</div>
<div id="non-parametric-alternatives" class="section level2">
<h2>Non-parametric alternatives</h2>
<p>If your data are highly non-normal or contain obvious outliers, you
probably want to run one of the non-parametric alternatives to the
classical t-test. These tests do not assume normality of the data nor
are they highly affected by outliers!</p>
<div id="wilcoxon-signed-rank-test" class="section level3">
<h3>Wilcoxon signed-rank test</h3>
<p>The non-parametric (distribution-free) alternative to the one-sample
t-test is called the <strong>one sample Wilcoxon signed rank
test</strong>. This analysis first reduces the data to signs- negative 1
if the observation is greater than the null and -1 if the data are less
than the null. Then, like many non-parametric tests, you sort the data
from smallest to largest absolute value. You then multiply the ranks
times the signs and sum across all observations. This test statistic (W)
has a known sampling distribution (approximately anyway!) and therefore
you can compute a p-value. The null hypothesis for the Wilcoxon test
refers to the median of the distribution and not the mean (the null
hypothesis for the t-test refers to the mean). Therefore the two tests
are not strictly comparable!</p>
<pre class="r"><code># Wilcoxon signed rank test

my.data &lt;- rgamma(20,0.1,0.1)-2

hist(my.data)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>wilcox.test(my.data)</code></pre>
<pre><code>## Warning in wilcox.test.default(my.data): cannot compute exact p-value with ties</code></pre>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  my.data
## V = 91, p-value = 0.6142
## alternative hypothesis: true location is not equal to 0</code></pre>
<pre class="r"><code>t.test(my.data)   # t-test for comparison</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  my.data
## t = 0.36876, df = 19, p-value = 0.7164
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -1.739000  2.482826
## sample estimates:
## mean of x 
##  0.371913</code></pre>
</div>
<div id="mann-whitney-u-test" class="section level3">
<h3>Mann-Whitney U test</h3>
<p>The non-parametric alternative to the two-sample t-test is called the
<strong>Mann Whitney U test (or Wilcoxon rank sum test)</strong>. This
analysis first reduces the data to ranks- that is, you first sort the
data from smallest to largest absolute value. You then essentially test
to see if the ranks of one group are higher on average than the ranks of
the other group. This test statistic (U) has a known sampling
distribution (approximately anyway!) under the null hypothesis and
therefore you can compute a p-value. The null hypothesis for the
Mann-Whitney test is that there is a 50% probability that a single
sample from one distribution is larger than a single sample drawn from
the other distribution. The null hypothesis for the t-test is that the
means of two distributions are the same. Therefore the two tests are not
strictly comparable!</p>
<pre class="r"><code># Wilcoxon signed rank test

my.data1 &lt;- rgamma(20,0.1,0.1)-2
my.data2 &lt;- rgamma(20,0.2,0.1)-2

median(my.data1)</code></pre>
<pre><code>## [1] -1.990913</code></pre>
<pre class="r"><code>median(my.data2)</code></pre>
<pre><code>## [1] -1.934051</code></pre>
<pre class="r"><code>wilcox.test(my.data1,my.data2)</code></pre>
<pre><code>## 
##  Wilcoxon rank sum exact test
## 
## data:  my.data1 and my.data2
## W = 138, p-value = 0.0965
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<pre class="r"><code>t.test(my.data1,my.data2)   # t-test for comparison</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  my.data1 and my.data2
## t = -0.82194, df = 36.193, p-value = 0.4165
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -4.182745  1.769844
## sample estimates:
##  mean of x  mean of y 
## -0.5950182  0.6114325</code></pre>
<pre class="r"><code>#### perform rank test from scratch!

allobs &lt;- c(my.data1,my.data2)
inorder &lt;- order(allobs)

rank1 &lt;- inorder[1:20]
rank2 &lt;- inorder[21:40]

t.test(rank1,rank2,var.equal = T)    # perform t-test on the ranks (usually similar to Mann-Whitney test)</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  rank1 and rank2
## t = -0.78051, df = 38, p-value = 0.4399
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -10.421653   4.621653
## sample estimates:
## mean of x mean of y 
##     19.05     21.95</code></pre>
</div>
</div>
<div id="power-analysis-for-t-test" class="section level2">
<h2>Power analysis for t-test</h2>
<p>Okay, before we leave the t-test, let’s run a power analysis!
Remember that a power analysis is a way of assessing whether you are
likely to detect a signal given your null hypothesis is false (i.e.,
there is a signal there to detect!). Power analyses can be a great way
to assess if your sampling design is adequate to detect a signal. If
not, you may want to alter your sampling design by adding more
replicates!</p>
<pre class="r"><code># Power analysis ----------------------

# First we will set the population parameters:

true.mean.A &lt;- 13.5
true.mean.B &lt;- 13.9

true.sd &lt;- 3.4

# Assume a two-tailed test- alternative hypothesis is that the mean of A is different from the mean of B

# Now let&#39;s set the sampling scenario

sampsize.A &lt;- 10
sampsize.B &lt;- 12

# now we will simulate a single &#39;experiment&#39;

samp.A &lt;- rnorm(sampsize.A,true.mean.A,true.sd)
samp.B &lt;- rnorm(sampsize.B,true.mean.B,true.sd)

# and now we can run a test!

this.test &lt;- t.test(samp.A,samp.B,var.equal = T)

# and determine if we rejected our null hypothesis (which we know is not true!)

this.test$p.value &lt; 0.05</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>Okay, but if we really want to compute power we need to do this many
times and see what percent of samples allow us to reject the null!</p>
<pre class="r"><code># full power analysis:   ------------

# First we will set the population parameters:

true.mean.A &lt;- 13.5
true.mean.B &lt;- 13.9

true.sd &lt;- 3.4

# Assume a two-tailed test- alternative hypothesis is that the mean of A is different from the mean of B

# Now let&#39;s set the sampling scenario

sampsize.A &lt;- 10
sampsize.B &lt;- 12

# now we will simulate LOTS of  &#39;experiments&#39;

pvals &lt;- numeric(1000)

for(i in 1:1000){
  samp.A &lt;- rnorm(sampsize.A,true.mean.A,true.sd)
  samp.B &lt;- rnorm(sampsize.B,true.mean.B,true.sd)
  
  # and now we can run a test!
  
  this.test &lt;- t.test(samp.A,samp.B,var.equal = T)
  
  # and determine if we rejected our null hypothesis (which we know is not true!)
  
  pvals[i] &lt;- this.test$p.value
}

 # hist(pvals)

length(which(pvals&lt;0.05))/1000</code></pre>
<pre><code>## [1] 0.059</code></pre>
<p>Of course, if your power is not adequate, you may want to increase
sample size. But what is the minimum sample size that gives you
sufficient power? We can use simulation to answer this question!</p>
<pre class="r"><code># full power analysis WITH SAMPLE SIZE DETERMINATION:   --------

# First we will set the population parameters:

true.mean.A &lt;- 13.5
true.mean.B &lt;- 13.9

true.sd &lt;- 3.4

# Assume a two-tailed test- alternative hypothesis is that the mean of A is different from the mean of B


# now we will simulate LOTS of  &#39;experiments&#39; under different sample sizes

sampsize &lt;- seq(5,400,10)

power &lt;- numeric(length(sampsize))

for(j in 1:length(sampsize)){
  
  pvals &lt;- numeric(1000)
  for(i in 1:1000){
    samp.A &lt;- rnorm(sampsize[j],true.mean.A,true.sd)
    samp.B &lt;- rnorm(sampsize[j],true.mean.B,true.sd)
    
    # and now we can run a test!
    
    this.test &lt;- t.test(samp.A,samp.B,var.equal = T)
    
    # and determine if we rejected our null hypothesis (which we know is not true!)
    
    pvals[i] &lt;- this.test$p.value
  }

 # hist(pvals)

  power[j] &lt;- length(which(pvals&lt;0.05))/1000
}

names(power) &lt;- sampsize

plot(power~sampsize)</code></pre>
<p><img src="LECTURE4_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>So what sample size would you need (in each of the two groups) to
detect a signal in this case?</p>
<p><a href="LECTURE5.html">–go to next lecture–</a></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
