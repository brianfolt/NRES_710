---
title: "Linear Regression (cont.)"
author: "NRES 710"
date: "Last compiled: `r Sys.Date()`"
output: 
  html_document: 
    theme: yeti
    toc: yes
    toc_float: yes
    css: styles.css
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	cache = TRUE
)
```

```{r echo=FALSE}

#  NRES 710, Linear Regression (3)
#     University of Nevada, Reno
#     Assumptions of linear regression   

```

## Review

This week we will continue to explore linear regression by talking about **assumptions of regression** -- and in fact these are important assumptions of many other statistical tests. And then we will discuss how to use linear regression models to make **predictions**.

**Most important thing we have learned so far**: 

- $Y_i = \beta_0 + \beta_1x_i + \epsilon \sim N(0, \sigma)$

There are many assumptions that this regression test makes! But we will focus on what are considered the five most important of these assumptions.

## Assumptions

What are the assumptions of statistical test and how do they influence your results?

The first thing you need to know about assumptions of statistical tests (regression, t-test, ANOVA, other tests we will cover...) is that the tests are **robust to violations of assumptions**.

- Robust means that: if an assumption is violated, it very rarely influences the results we get from the analysis (e.g., slope).
- We'll talk about which of the assumptions influence different parameters...
- But for most parameters, if the assumptions are violated it does not influence the slope.
- Assumption violation may influence the standard deviation, but this is not often reported.
- **Violations cause the p-value to increase.** This means that violations are likely to be conservative. Since we want to avoid committing Type I error (rejecting the null when in fact it is true), then if assumption violation causes p-values to increase then we are less likely to commit Type I error.

A general rule (*axiom*) in statistics is that: **the more assumptions a test makes, the more powerful it is (power = p-values).**

We often use statistical tests that make assumptions. Often, these assumptions are true. And since we make more assumptions, we are more likely to detect a significant effect. **If these assumptions are valid.**

I often don't care too much about assumptions -- because they are often met due to our **study design**, the specific analysis we chose, and **most analyses are robust to violations of assumptions**. But, **reviewers do**. Reviewers will try to find something wrong with your paper. They will try to find something wrong and jam up the process. So it can be useful to carefully document how you examined for violations of assumptions in your analysis. This gets tedious, but is part of the 'statistical ritual' of our field... (Or maybe it shouldn't be. Johnson [1999] made suggestions that our statistical ritual leads to bad practices, and we will read another paper to this effect at the end of the semester.)

But as for me, again, I don't care too much about these assumptions, because **most analyses are robust to violations of assumptions**.

## Five regression assumptions

There are **5 assumptions** to linear regression. I put the equation up on the board again because most of these assumptions are indicated in the equation, either explicitly or implicitly. 

### Continuous Y

- Should be continuous; not a count, like a number. But... if it is a count, that's potentially okay, because regression/ANOVA are robust to violations of assumptions. We don't really need a test for this -- if you collected the data, you should know whether it is continuous or not!

- Note: linear regression also assumes that your X-variable is continuous... but we can relax this assumption, and we will do so next week when we explore t-tests! More on this soon.

- If you Y is not continuous, it will not influence your slope, but it might increase your p-value.

### Error is normally distributed

- Very clearly indicated in the equation! Important and something that a lot of people get wrong. Some folks say that X-variable has to be normally distributed, continuous, etc. -- but nope. Others assume that your Y-variable has to be normally distributed. Nope! This assumption relates to the **error** around the Y-variable. 

For example: consider the data in the left graph. There is a gap in values for the middle-range of X-values. If we examine this as a frequency histogram (middle graph), this is not a normally-distributed y-variable; it is bimodally distributed! This is okay.  When we run this regression, we have not violated any assumptions, because the error is normally distributed around the line.

```{r example-1, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=4}
# Set the seed for reproducibility
set.seed(123)

# Set up the plotting area to display two plots side by side
par(mfrow = c(1, 2))

# Simulate data for the shallow slope
n <- 50
x1 <- rnorm(n, mean = 20, sd = 10)
y1 <- 5 + 4 * x1 + rnorm(n, mean = 0, sd = 8)

# Remove observations in the middle of x to create bimodal y
middle_range <- c(15, 25)
datum1 <- data.frame(x = x1, y = y1)
datum1 <- subset(datum1, x < middle_range[1] | x > middle_range[2])

# Plot the data with the shallow slope
plot(y ~ x, data = datum1,
     ylim=c(0, max(datum1$y)), xlab = "Predictor variable", ylab = "Response variable")

# Add a line to the plot for the shallow slope
results1 <- lm(y ~ x, data = datum1)
abline(results1)

# Plot the density of y
plot(density(datum1$y), col = "skyblue", xlab = "Response variable", ylab = "Density", main = "", lwd = 2)

# Examine normality of residuals
residuals <- resid(results1)
plot(density(residuals), xlab = "Residuals", main = "")
```

Then why are people always asking if the Y-variable is normally distributed...? This is because if you look at the distribution of the y-data and it appears normal, the residuals will almost always be normal when you run the analysis.

But, if you run a histogram and your y-data are not normal, this does not necessarily mean that your error will also not be normal. To really know if the assumption is violated, you need to run the regression and examine whether the residuals are normal (third graph).

If your error is not normally distributed, it's not going to influence your slope -- but it might increase your p-values a little bit. 

### Linear relationship between X and Y

This one is important. This is implicit in our linear model equation.

It assumes that there is a single parameter -- the slope -- describing the relationship between X and Y.

The reason why I think this is important is because so often in ecology/natural resource management I see people obtaining two continuous variables and immediately running a regression model -- without ever considering whether their data have a linear relationship. They don't even think about it.

This is a problem, because in ecology... many processes of interest are non-linear!

Example: mesocosm experiment. We want to understand the effect of crayfish predators on prey fish. Does fish abundance decrease and predatory crayfish increases?

```{r predator-prey-simulation, echo=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=6}
# Set the seed for reproducibility
set.seed(123)

# Simulate non-linear data
n <- 100
time <- seq(1, n)
prey_abundance <- 100 * exp(-0.05 * time) + rnorm(n, mean = 0, sd = 5)
predator_abundance <- 50 * (1 - exp(-0.01 * time)) + rnorm(n, mean = 0, sd = 5)

# Plot the data
plot(predator_abundance, prey_abundance, pch = 16, col = "blue",
     xlab = "Predator abundance", ylab = "Prey abundance")

# Fit a linear model
results <- lm(prey_abundance ~ predator_abundance)
abline(results)
```

This very clearly violates the assumption of linearity! We can fit a better statistical model -- one that does not assume linearity -- which can better help us measure this relationship and explain it to the scientific community! 

So, don't assume there is a linear relationship between X and Y -- examine this, verify, and adjust your model as needed.

If the relationship is not linear, **this will alter your slope**! We are measuring something that is not linear an

### Homoscedasticity

*homo* = same; *scedasticity* = variance, noise, error, etc.

Implicit in our equation: - $\epsilon \sim N(0, \sigma)$

Constant variance: *error does not change not matter what the X and Y values are.*
 
```{r example-3, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=6}
# Set the seed for reproducibility
set.seed(123)

# Simulate data for the shallow slope
n <- 50
x1 <- rnorm(n, mean = 20, sd = 10)
y1 <- 5 + 4 * x1 + rnorm(n, mean = 0, sd = 12)

# Remove observations in the middle of x to create bimodal y
datum1 <- data.frame(x = x1, y = y1)

# Plot the data with the shallow slope
plot(y ~ x, data = datum1,
     ylim=c(0, max(datum1$y)), xlab = "Predictor variable", ylab = "Response variable")

# Add a line to the plot for the shallow slope
results1 <- lm(y ~ x, data = datum1)
abline(results1)
```

We can visualize homoscedasticity by imagining/drawing normally distributed bell curves amidst our data along the regression line...

An example of **heteroscedasticity** is any case where your variance changes. This commonly occurs in ecology when we count animals. For example, when we are electofishing for fish in a river, areas with no fish have little variance; areas with many fish have high variance! It creates this cone-shaped data.

```{r example-4, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=6}
# Set the seed for reproducibility
set.seed(100)

# Simulate data with increasing variance (heteroscedasticity)
n <- 50
x1 <- runif(n, 0, 100)
y1 <- 20 + 25 * x1 + rnorm(n, mean = 0, sd = 10 * x1)

# Create a data frame
datum1 <- abs(data.frame(x = x1, y = y1))

# Fit a linear model
results1 <- lm(y ~ x, data = datum1)

# Predict values and calculate confidence intervals
predictions <- predict(results1, interval = "confidence", level = 0.95)
datum1 <- cbind(datum1, predictions)

# Plot the data
plot(datum1$x, datum1$y, ylim=c(0, max(datum1$y)), xlab = "Predictor variable", ylab = "Response variable", pch = 16)

# Add the regression line
abline(results1, col = "red")
```

We can visualize *heteroscedasticity* by imagining/drawing normally distributed bell curves amidst our data along the regression line, and the bell curves get wider as we increase along X.

*Heteroscedasticity* could also happen in a non-linear way. 

**Q:** Will heteroscedasticity influence your slope? No.

We can account for *heteroscedasticity* in our model by adding a weighting paramter to the error component: $\epsilon \sim N(0, \sigma * y)$ would allow for error to increase with $y$!

### Independent samples

When people say that the x-variable is the 'independent variable', this is what they really mean! Your samples should be independent of eachother and there should be **no autocorrelation**. 

Example: measuring pollution in water samples every 10 m down the middle of a river from a chemical plant. These data will have autocorrelation, because pollution can't change that much from sample to sample.

**Q:** How might you eliminate or minimize autocorrelation? Maybe by increasing distance between samples -- measure every kilometer. This decreases autocorrelation, but also decreases sample size. Tradeoff...

I personally don't worry too much about autocorrelation, because often when you fix it/account for it, nothing changes. If you had a strong slope and p-value with one test, you will likely get a strong slope and p-value with another test.

Consider these autocorrelated data:

```{r example-5, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=6}
# Set the seed for reproducibility
set.seed(100)

# Simulate data with autocorrelation
n <- 100
x <- seq(1, n)
phi <- 0.99  # Autoregressive parameter
epsilon <- rnorm(n, mean = 0, sd = 5)  # Reduced noise term to make the autocorrelation more prominent
y <- numeric(n)
y[1] <- 5 + 2 * x[1] + epsilon[1]
for (i in 2:n) {
  y[i] <- 5 + 2 * x[i] + phi * (y[i-1] - (5 + 2 * x[i-1])) + epsilon[i]
}

# Create a data frame
data <- data.frame(x = x, y = y)

# Plot the data
plot(data$x, data$y, xlab = "Predictor variable", ylab = "Response variable", pch = 16, col = "blue")

# Fit a linear model
model <- lm(y ~ x, data = data)

# Add the regression line to the graph
abline(model, col = "red")
```

Instead of our points bouncing around the line, they tend to follow eachother.

This will not affect our slope, and it probably won't affect the p-value too much (would only increase). So again, linear regression is robust to violation of this assumption.

Two types of autocorrelation issues: spatial and temporal autocorrelation. We will discuss how to deal with this down the road. But again, it's not too big of a concern.

Many things are autocorrelated in nature. Animal movements, for example! Is this a problem?

Nah. This is what animasl do -- they move! Try to get big sample sizes.

## Evaluating whether Assumptions are Met

Statistical tests exist to statistically test for these assumptions. These are p-value generating tests. There are some consequences of this.

- If you have a small sample size, the assumption will never be violated! Because of the relationship between sample size and p-values that we have identified in previous classes.
- Conversely, if you have a really large sample, the assumption will always be violated!

So, for these reasons, I don't like these test and I don't recommend using these tests.

Instead, I look at my data graphically! And I will teach you to do this also. We will visually examine our data to identify whether our data meet these assumptions or not. If it has been violated, we will see this.

```{r, echo=FALSE, message=FALSE}
# Load necessary libraries
library(knitr)
library(kableExtra)

# Create the data frame
data <- data.frame(
  Group = c("Normality", "Linearity", "Homoscedasticity", "No autocorrelation"),
  `X-Y Scatterplot` = c("X", "X", "X", "X"),
  `Residuals Scatterplot` = c("X", "X", "X", "X"),
  `Histogram of Residuals` = c("X", "", "", ""),
  `Autocorrelation Function (ACF)` = c("", "X", "", "X")
)

# Create the table
kable(data, col.names = c("Assumption", "X-Y Scatterplot", "Residuals Scatterplot", "Histogram of Residuals", "Autocorrelation Function (ACF)")) %>%
  kable_styling(full_width = FALSE)

```

Residuals scatterplot: X-variable --> X data; Y-variable --> residual

**Add more details here about different graphing approaches to examining assumptions**

[--go to next lecture--](lecture_7.html)